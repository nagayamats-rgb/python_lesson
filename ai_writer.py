import os
import json
import csv
import re
import time
import logging
from datetime import datetime
from tqdm import tqdm
from dotenv import load_dotenv
from openai import OpenAI, OpenAIError

# ============================================================
# ğŸŒ¸ KOTOHA ENGINE â€” AI Writer v2.1 JSON-Safe Edition
# ============================================================

logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s: %(message)s',
)

# ======== åŸºæœ¬è¨­å®š ========
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    logging.error("âŒ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    exit(1)

client = OpenAI(api_key=OPENAI_API_KEY)

INPUT_DIR = "./output/semantics"
OUTPUT_DIR = "./output/ai_generated"
LOG_DIR = "./logs"
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

# ============================================================
# ğŸ” JSON æŠ½å‡ºãƒ»ä¿®å¾©ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ============================================================

def extract_json_block(text: str) -> str | None:
    """```json ãƒ•ã‚§ãƒ³ã‚¹ã‚„ä½™åˆ†ãªæ–‡ãŒæ··å…¥ã—ã¦ã‚‚ JSON ãƒ–ãƒ­ãƒƒã‚¯ã ã‘æŠœã"""
    if not text:
        return None
    # ```json ãƒ•ã‚§ãƒ³ã‚¹å„ªå…ˆ
    m = re.search(r"```json\s*(\{.*?\})\s*```", text, flags=re.S)
    if m:
        return m.group(1)
    # ãƒ•ã‚§ãƒ³ã‚¹ãŒãªã„å ´åˆ { ... } ã®æœ€å¤–æ®»ã‚’æŠ½å‡º
    start = text.find("{")
    end = text.rfind("}")
    if start != -1 and end != -1 and end > start:
        return text[start:end + 1]
    return None


def sanitize_json_str(s: str) -> str:
    """å£Šã‚ŒãŸJSONã‚’ä¿®å¾©: ã‚¹ãƒãƒ¼ãƒˆã‚¯ã‚©ãƒ¼ãƒˆã€å…¨è§’è¨˜å·ã€æœ«å°¾ã‚«ãƒ³ãƒãªã©"""
    s = s.replace("â€œ", "\"").replace("â€", "\"").replace("â€™", "'")
    s = s.replace("ï¼š", ":").replace("ï¼Œ", ",").replace("ï¼", ".")
    s = re.sub(r"[\x00-\x08\x0B-\x0C\x0E-\x1F]", "", s)
    s = re.sub(r",\s*([\]}])", r"\1", s)
    return s


def parse_json_safely(raw_text: str, save_stub_path: str | None = None) -> dict | None:
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å®‰å…¨ã«JSONã¸ãƒ‘ãƒ¼ã‚¹ã€‚å¤±æ•—æ™‚ã¯None"""
    if save_stub_path:
        with open(save_stub_path, "a", encoding="utf-8") as f:
            f.write("\n\n--- RAW RESPONSE ---\n")
            f.write(raw_text)

    block = extract_json_block(raw_text)
    if not block:
        return None
    try:
        return json.loads(block)
    except json.JSONDecodeError:
        pass

    fixed = sanitize_json_str(block)
    try:
        return json.loads(fixed)
    except json.JSONDecodeError:
        return None

# ============================================================
# ğŸ”® OpenAI å‘¼ã³å‡ºã—
# ============================================================

def ai_generate(prompt: str, max_tokens: int = 600) -> str:
    """OpenAI Chat APIå‘¼ã³å‡ºã— (JSONå³æ ¼ãƒ¢ãƒ¼ãƒ‰ + ãƒªãƒˆãƒ©ã‚¤ä»˜ã)"""
    for attempt in range(3):
        try:
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system",
                        "content": (
                            "ã‚ãªãŸã¯ç†Ÿç·´ã®SEOã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼å…¼ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼ã§ã™ã€‚"
                            "çµ¶å¯¾ã«æœ‰åŠ¹ãªJSONã®ã¿ã‚’è¿”ã—ã€èª¬æ˜ãƒ»å‰ç½®ããƒ»è£…é£¾ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆã¯ç¦æ­¢ã§ã™ã€‚"
                        ),
                    },
                    {"role": "user", "content": prompt},
                ],
                max_tokens=max_tokens,
                temperature=0.5,
            )
            return response.choices[0].message.content or ""
        except OpenAIError as e:
            logging.warning(f"âš ï¸ OpenAIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼({attempt+1}/3): {e}")
            time.sleep(2)
    return ""


def local_fallback(cluster):
    """AIå‘¼ã³å‡ºã—å¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”Ÿæˆ"""
    kw = ", ".join(cluster["keywords"][:3])
    return {
        "catch_copy": f"{kw} â€” é«˜å“è³ªãƒ»é«˜æ©Ÿèƒ½ã®äººæ°—ã‚¢ã‚¯ã‚»ã‚µãƒªã€‚",
        "alt_texts": [f"{kw} ç”¨ã‚¢ã‚¯ã‚»ã‚µãƒª {i}" for i in range(1, 21)],
    }

# ============================================================
# ğŸ§  ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ============================================================

def find_latest_semantics():
    """æœ€æ–°ã® structured_semantics_*.json ã‚’æ¤œå‡º"""
    files = [
        f for f in os.listdir(INPUT_DIR)
        if f.startswith("structured_semantics_") and f.endswith(".json")
    ]
    if not files:
        logging.error("âŒ structured_semantics_*.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None
    files.sort(key=lambda f: os.path.getmtime(os.path.join(INPUT_DIR, f)), reverse=True)
    latest = os.path.join(INPUT_DIR, files[0])
    logging.info(f"ğŸ“„ ä½¿ç”¨ãƒ•ã‚¡ã‚¤ãƒ«: {latest}")
    return latest


def main():
    start_time = time.time()
    logging.info("ğŸŒ¸ KOTOHA ENGINE â€” AI Writer èµ·å‹•")

    input_file = find_latest_semantics()
    if not input_file:
        return

    with open(input_file, "r", encoding="utf-8") as f:
        clusters = json.load(f)

    outputs = []
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    raw_log_path = os.path.join(LOG_DIR, f"ai_writer_raw_{timestamp}.txt")

    for cluster in tqdm(clusters, desc="ğŸª„ ç”Ÿæˆä¸­", unit="cluster"):
        keywords = ", ".join(cluster["keywords"][:5])

        prompt = f"""
ä»¥ä¸‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç¾¤ã‹ã‚‰ã€JSONã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
å‰ç½®ããƒ»èª¬æ˜ãƒ»è£œè¶³ãƒ»ã‚³ãƒ¼ãƒ‰ãƒ•ã‚§ãƒ³ã‚¹ä»¥å¤–ã®æ–‡å­—ã¯ä¸€åˆ‡å‡ºåŠ›ã—ãªã„ã“ã¨ã€‚

è¦ä»¶:
- "catch_copy": æ—¥æœ¬èª / æœ€å¤§60æ–‡å­— / æœ€å°30æ–‡å­— / çµµæ–‡å­—ãƒ»é¡”æ–‡å­—ãªã— / è¨´æ±‚åŠ›é‡è¦–
- "alt_texts": é•·ã•20ã®æ–‡å­—åˆ—é…åˆ— / å„ALTã¯è‡ªç„¶ãªæ—¥æœ¬èªãƒ•ãƒ¬ãƒ¼ã‚º / SEOã¨æ„Ÿæƒ…ãƒãƒ©ãƒ³ã‚¹
- å‡ºåŠ›ã¯æœ‰åŠ¹ãªUTF-8 JSONã§è¿”ã™ã“ã¨ã€‚

ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keywords}

å‡ºåŠ›å½¢å¼ã‚µãƒ³ãƒ—ãƒ«:
{{
  "catch_copy": "ã“ã“ã«60æ–‡å­—ä»¥å†…ã®ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼",
  "alt_texts": ["...", "...", "...", "...", "...", "...", "...", "...", "...", "...",
                "...", "...", "...", "...", "...", "...", "...", "...", "...", "..."]
}}
"""

        result = ai_generate(prompt)
        parsed = parse_json_safely(result, save_stub_path=raw_log_path)

        if not parsed or "catch_copy" not in parsed or "alt_texts" not in parsed:
            logging.warning("âš ï¸ JSONè§£æå¤±æ•—ã€ãƒ†ãƒ³ãƒ—ãƒ¬ç”Ÿæˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")
            parsed = local_fallback(cluster)

        outputs.append({
            "cluster_id": cluster["cluster_id"],
            "keywords": cluster["keywords"],
            **parsed,
        })

    # ===== å‡ºåŠ› =====
    json_out = os.path.join(OUTPUT_DIR, f"ai_generated_{timestamp}.json")
    csv_out = os.path.join(OUTPUT_DIR, f"ai_generated_{timestamp}.csv")

    with open(json_out, "w", encoding="utf-8") as f:
        json.dump(outputs, f, ensure_ascii=False, indent=2)

    with open(csv_out, "w", encoding="utf-8", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["cluster_id", "keywords", "catch_copy", "alt_texts"])
        for o in outputs:
            writer.writerow([o["cluster_id"], ", ".join(o["keywords"]), o["catch_copy"], "; ".join(o["alt_texts"])])

    elapsed = time.time() - start_time
    logging.info(f"âœ… å®Œäº†! AI Writer å®Ÿè¡Œçµæœ: {len(outputs)}ã‚¯ãƒ©ã‚¹ã‚¿ç”Ÿæˆ / {elapsed:.1f}ç§’")
    logging.info(f"ğŸ’¾ JSONå‡ºåŠ›: {json_out}")
    logging.info(f"ğŸ’¾ CSVå‡ºåŠ›: {csv_out}")
    print("\nğŸ‰ KOTOHA ENGINE AI Writer å®Œäº† â€” ç¾ã—ã„ã‚³ãƒ”ãƒ¼ã®èª•ç”Ÿã§ã™ã€‚\n")


if __name__ == "__main__":
    main()
import atlas_autosave_core
