# ===============================================
# ğŸŒ¸ KOTOHA ENGINE â€” AI Writer (Hybrid Edition)
# Hybrid generation: Cluster + Product-level synthesis
# Author: ChatGPT (KOTOHA ENGINE Dev)
# ===============================================

import os
import json
import asyncio
import logging
from datetime import datetime
from dotenv import load_dotenv
from openai import AsyncOpenAI
from tqdm.asyncio import tqdm_asyncio

# -----------------------------------------------
# ãƒ­ã‚°è¨­å®š
# -----------------------------------------------
logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(levelname)s: %(message)s')

# -----------------------------------------------
# è¨­å®šãƒ»ç’°å¢ƒèª­ã¿è¾¼ã¿
# -----------------------------------------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
OUTPUT_DIR = os.path.join(BASE_DIR, "output", "ai_generated")
os.makedirs(OUTPUT_DIR, exist_ok=True)

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    logging.error("âŒ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    exit(1)

client = AsyncOpenAI(api_key=api_key)

# -----------------------------------------------
# æ—¢å­˜æˆæœç‰©ã®èª­è¾¼
# -----------------------------------------------
def load_json(path):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

SEMANTIC_PATH = "./output/semantics/structured_semantics_20251030_224846.json"
VOCAB_PATH = "./output/market_vocab_20251030_201906.json"
CLUSTER_PATH = "./output/lexical_clusters_20251030_223013.json"

structured_semantics = load_json(SEMANTIC_PATH)
market_vocab = load_json(VOCAB_PATH)
lexical_clusters = load_json(CLUSTER_PATH)

logging.info(f"âœ… æˆæœç‰©èª­è¾¼å®Œäº†: semantics={len(structured_semantics)}, vocab={len(market_vocab)}, clusters={len(lexical_clusters)}")

# -----------------------------------------------
# Hybrid Prompt Generator
# -----------------------------------------------
def build_prompt(cluster, products):
    prompt = [
        {
            "role": "system",
            "content": (
                "ã‚ãªãŸã¯å„ªã‚ŒãŸæ—¥æœ¬èªã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼å…¼SEOãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚"
                "å•†å“æƒ…å ±ã¨èªå½™ãƒ‡ãƒ¼ã‚¿ã‚’å‚è€ƒã«ã€è³¼è²·æ„æ¬²ã‚’é«˜ã‚ã‚‹è‡ªç„¶ãªæ—¥æœ¬èªã§"
                "çŸ­ãæ˜ç¢ºãªã‚³ãƒ”ãƒ¼æ–‡ã¨20ä»¶ã®ALTãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n\n"
                "âš™ï¸ åˆ¶ç´„æ¡ä»¶:\n"
                "- ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼ã¯40ã€œ60æ–‡å­—\n"
                "- ALTã¯80ã€œ110æ–‡å­—\n"
                "- ALTã¯1è¡Œ1æ–‡ã€ç”»åƒå†…å®¹ã¸ã®ç›´æ¥è¨€åŠã¯ç¦æ­¢ï¼ˆSEOãƒ¯ãƒ¼ãƒ‰ä¸­å¿ƒï¼‰\n"
                "- ã€ï¼ã€ã®å¤šç”¨ã¯ç¦æ­¢\n"
                "- å„ALTã¯é‡è¤‡ç¦æ­¢ãƒ»è‡ªç„¶ãªè¨€ã„æ›ãˆè¡¨ç¾ã‚’ä½¿ç”¨\n"
                "- å‡ºåŠ›ã¯JSONå½¢å¼ {copy: string, alt: [list of strings]}\n"
            ),
        },
        {
            "role": "user",
            "content": (
                f"ã‚¯ãƒ©ã‚¹ã‚¿ä»£è¡¨èª: {', '.join(cluster.get('keywords', [])[:10])}\n"
                f"é–¢é€£å•†å“åã®ä¾‹: {', '.join(p['name'] for p in products[:3])}\n"
                f"é–¢é€£ãƒˆãƒ”ãƒƒã‚¯: {', '.join(p.get('topics', []) for p in products if 'topics' in p)}"
            ),
        },
    ]
    return prompt

# -----------------------------------------------
# OpenAI å‘¼ã³å‡ºã—ï¼ˆãƒãƒƒãƒå¯¾å¿œï¼‰
# -----------------------------------------------
async def generate_for_cluster(cluster_id, cluster_data, product_list):
    prompt = build_prompt(cluster_data, product_list)
    try:
        response = await client.chat.completions.create(
            model="gpt-4o-mini",
            messages=prompt,
            temperature=0.8,
            max_tokens=800,
            timeout=60,
        )
        content = response.choices[0].message.content.strip()
        try:
            data = json.loads(content)
        except Exception:
            logging.warning("âš ï¸ JSONè§£æå¤±æ•—ã€ãƒ†ãƒ³ãƒ—ãƒ¬ç”Ÿæˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")
            data = {"copy": "è‡ªç„¶ãªé­…åŠ›ã‚’ä¼ãˆã‚‹å•†å“ã§ã™ã€‚", "alt": ["é«˜å“è³ªã§ä¿¡é ¼ã®ã‚¢ã‚¤ãƒ†ãƒ ã§ã™ã€‚"] * 20}
        return {"cluster_id": cluster_id, "output": data}
    except Exception as e:
        logging.error(f"ğŸš« ã‚¯ãƒ©ã‚¹ã‚¿ç”Ÿæˆå¤±æ•—: {e}")
        return {"cluster_id": cluster_id, "output": None}

# -----------------------------------------------
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# -----------------------------------------------
async def main():
    start = datetime.now()
    logging.info("ğŸŒ¸ KOTOHA ENGINE â€” Hybrid AI Writer èµ·å‹•")

    # ä»®: ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã®å•†å“ãƒªã‚¹ãƒˆï¼ˆãƒãƒƒãƒ”ãƒ³ã‚°ã¯æŸ”è»ŸåŒ–å¯èƒ½ï¼‰
    clusters = market_vocab.get("clusters", market_vocab)  # ä¸¡æ–¹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¯¾å¿œ
    products = [{"name": v.get("name", ""), "topics": v.get("keywords", [])} for v in clusters[:700]]

    tasks = []
    for i, cluster in enumerate(lexical_clusters[:50]):
        subset = products[i*14:(i+1)*14]  # å„ã‚¯ãƒ©ã‚¹ã‚¿ã«ç´„14å•†å“å‰²å½“
        tasks.append(generate_for_cluster(i, cluster, subset))

    results = await tqdm_asyncio.gather(*tasks)
    outfile = os.path.join(OUTPUT_DIR, f"ai_generated_hybrid_{datetime.now():%Y%m%d_%H%M%S}.json")
    with open(outfile, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    elapsed = (datetime.now() - start).seconds
    logging.info(f"âœ… Hybrid AI Writer å®Œäº†: {len(results)} ã‚¯ãƒ©ã‚¹ã‚¿ç”Ÿæˆ, å®Ÿè¡Œæ™‚é–“: {elapsed}s")
    logging.info(f"ğŸ’¾ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {outfile}")

# -----------------------------------------------
# å®Ÿè¡Œ
# -----------------------------------------------
if __name__ == "__main__":
    asyncio.run(main())
import atlas_autosave_core
