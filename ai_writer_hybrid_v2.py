import os, json, asyncio
from datetime import datetime
from tqdm.asyncio import tqdm_asyncio
from openai import AsyncOpenAI
from dotenv import load_dotenv
from loguru import logger
import aiofiles

# === åˆæœŸåŒ– ===
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError("âŒ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

client = AsyncOpenAI(api_key=OPENAI_API_KEY)

# === ãƒ«ãƒ¼ãƒˆã¨å‡ºåŠ›è¨­å®š ===
OUTPUT_DIR = "./output/ai_writer"
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs("./output/intermediate", exist_ok=True)

# === ãƒ­ãƒ¼ãƒ‰é–¢æ•° ===
async def load_json(filename):
    async with aiofiles.open(filename, mode="r", encoding="utf-8") as f:
        data = await f.read()
    return json.loads(data)

# === JSONãƒ­ãƒ¼ãƒ€ãƒ¼ï¼ˆå‹è‡ªå‹•åˆ¤å®šä»˜ãï¼‰ ===
async def load_structures():
    semantics = await load_json("./output/semantics/structured_semantics_20251030_224846.json")
    vocab = await load_json("./output/market_vocab_20251030_201906.json")
    clusters = await load_json("./output/lexical_clusters_20251030_223013.json")

    clusters_raw = vocab.get("clusters", vocab)

    # âœ… æ§‹é€ è‡ªå‹•åˆ¤å®šï¼ˆè¾æ›¸ãƒ»ãƒªã‚¹ãƒˆä¸¡å¯¾å¿œï¼‰
    if isinstance(clusters_raw, list):
        clusters_list = []
        for v in clusters_raw:
            if isinstance(v, dict):
                clusters_list.append({
                    "name": v.get("name", ""),
                    "topics": v.get("keywords", [])
                })
            else:
                clusters_list.append({"name": str(v), "topics": []})
    elif isinstance(clusters_raw, dict):
        clusters_list = []
        for k, v in clusters_raw.items():
            if isinstance(v, dict):
                clusters_list.append({
                    "name": k,
                    "topics": v.get("keywords", [])
                })
            else:
                clusters_list.append({"name": k, "topics": []})
    else:
        raise ValueError(f"âŒ clusters ã®æ§‹é€ ãŒä¸æ˜: {type(clusters_raw)}")

    logger.info(f"âœ… æˆæœç‰©èª­è¾¼å®Œäº†: semantics={len(semantics)}, vocab={len(vocab)}, clusters={len(clusters_list)}")
    return semantics, vocab, clusters_list


# === ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ ===
def compose_prompt(name, topics):
    topics_str = "ã€".join(topics[:10])
    prompt = f"""
ã‚ãªãŸã¯ãƒ—ãƒ­ã®ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼å…¼SEOã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®å•†å“ã«å¯¾ã—ã¦ã€è‡ªç„¶ã§è³¼è²·æ„æ¬²ã‚’ããã‚‹ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼ï¼ˆ40ï½60å­—ï¼‰ã‚’1ã¤ã€
ãŠã‚ˆã³SEOã«åŠ¹æœçš„ãªALTãƒ†ã‚­ã‚¹ãƒˆï¼ˆ80ï½110å­—ï¼‰ã‚’20å€‹ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ã€å•†å“åã€‘{name}
ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€‘{topics_str}

å‡ºåŠ›å½¢å¼ã¯æ¬¡ã®JSONå½¢å¼ã§ï¼š
{{
  "catchcopy": "ãƒ»ãƒ»ãƒ»",
  "alts": ["ãƒ»ãƒ»ãƒ»", "ãƒ»ãƒ»ãƒ»", â€¦ (è¨ˆ20ä»¶)]
}}
"""
    return prompt.strip()

# === AIç”Ÿæˆé–¢æ•° ===
async def generate_text(item):
    prompt = compose_prompt(item["name"], item["topics"])
    try:
        response = await client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯å„ªç§€ãªæ—¥æœ¬èªãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.8,
        )
        content = response.choices[0].message.content.strip()
        try:
            data = json.loads(content)
        except json.JSONDecodeError:
            logger.warning("âš ï¸ JSONè§£æå¤±æ•—ã€ãƒ†ãƒ³ãƒ—ãƒ¬ç”Ÿæˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")
            data = {"catchcopy": "ç”Ÿæˆå¤±æ•—", "alts": [f"{item['name']} ã®ç”»åƒ" for _ in range(20)]}
        return {"name": item["name"], "catchcopy": data["catchcopy"], "alts": data["alts"]}
    except Exception as e:
        logger.error(f"ğŸš« OpenAIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
        return {"name": item["name"], "catchcopy": "ã‚¨ãƒ©ãƒ¼", "alts": [f"{item['name']} ã®ç”»åƒ" for _ in range(20)]}

# === ãƒ¡ã‚¤ãƒ³ ===
# === ãƒ¡ã‚¤ãƒ³ ===
async def main():
    semantics, vocab, clusters = await load_structures()
    logger.info("ğŸŒ¸ KOTOHA ENGINE â€” Hybrid AI Writer èµ·å‹•")

    # productsç”Ÿæˆ
    products = clusters[:700]

    # âœ… tqdm_asyncio.gather ã«å¤‰æ›´
    tasks = [generate_text(item) for item in products]
    results = await tqdm_asyncio.gather(*tasks, desc="ğŸª„ ç”Ÿæˆä¸­", total=len(tasks))

    # ä¿å­˜
    output_path = os.path.join(
        OUTPUT_DIR,
        f"hybrid_writer_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    )
    async with aiofiles.open(output_path, "w", encoding="utf-8") as f:
        await f.write(json.dumps(results, ensure_ascii=False, indent=2))

    logger.info(f"âœ… å‡ºåŠ›å®Œäº†: {output_path} ({len(results)}ä»¶)")


if __name__ == "__main__":
    asyncio.run(main())
import atlas_autosave_core
