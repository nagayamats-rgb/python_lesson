import os
import json
import asyncio
import logging
from datetime import datetime
from tqdm import tqdm
from openai import OpenAI
from dotenv import load_dotenv
load_dotenv("/Users/tsuyoshi/Desktop/python_lesson/.env")
# ============================================================
# ğŸŒ¸ KOTOHA ENGINE â€” Hybrid AI Writer v3 (é–‹ç™ºè€…ãƒ¢ãƒ¼ãƒ‰)
# ============================================================

# ---- ãƒ­ã‚°è¨­å®š ----
logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s | %(levelname)-8s | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)

# ---- ç’°å¢ƒå¤‰æ•° ----
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    logging.critical("ğŸš« OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    exit(1)

client = OpenAI(api_key=OPENAI_API_KEY)

# ============================================================
# ğŸ” ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ============================================================

def find_latest_file(base_dir, prefix, ext):
    """æŒ‡å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä»¥ä¸‹ã‹ã‚‰æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ç´¢ï¼ˆå†å¸°å¯¾å¿œï¼‰"""
    logging.debug(f"ğŸ” Searching latest file: {prefix}*{ext} under {base_dir}")
    matched = []
    for root, _, files in os.walk(base_dir):
        for f in files:
            if f.startswith(prefix) and f.endswith(ext):
                matched.append(os.path.join(root, f))
    if not matched:
        logging.warning(f"âš ï¸ No file found for {prefix}")
        return None
    latest = max(matched, key=os.path.getmtime)
    logging.debug(f"âœ… Found latest: {latest}")
    return latest


def safe_load_json(path):
    """å®‰å…¨ãªJSONãƒ­ãƒ¼ãƒ‰"""
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        logging.debug(f"ğŸ“„ Loaded JSON: {path} ({len(data)} entries)")
        return data
    except Exception as e:
        logging.error(f"ğŸš« JSONèª­è¾¼å¤±æ•—: {path} ({e})")
        return []


# ============================================================
# ğŸ“¦ ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
# ============================================================

def load_structures():
    """æ§‹é€ ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰"""
    base_dir = "/Users/tsuyoshi/Desktop/python_lesson"
    logging.info(f"ğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­è¾¼é–‹å§‹: {base_dir}")

    semantics_file = find_latest_file(base_dir, "structured_semantics_", ".json")
    vocab_file = find_latest_file(base_dir, "market_vocab_", ".json")
    cluster_file = find_latest_file(base_dir, "lexical_clusters_", ".json")

    if not all([semantics_file, vocab_file, cluster_file]):
        raise FileNotFoundError("âŒ å¿…é ˆJSONã®ã„ãšã‚Œã‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    semantics = safe_load_json(semantics_file)
    vocab = safe_load_json(vocab_file)
    clusters = safe_load_json(cluster_file)

    logging.info(f"âœ… èª­è¾¼å®Œäº†: semantics={len(semantics)}, vocab={len(vocab)}, clusters={len(clusters)}")
    return semantics, vocab, clusters


# ============================================================
# ğŸ§  AIç”Ÿæˆå‡¦ç†
# ============================================================

async def generate_text(prompt, retries=2):
    """ChatGPTå‘¼ã³å‡ºã—ï¼ˆå†è©¦è¡Œä»˜ãï¼‰"""
    for attempt in range(retries):
        try:
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯é«˜å“è³ªãªæ—¥æœ¬èªã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚"},
                    {"role": "user", "content": prompt},
                ],
                temperature=0.7,
            )
            text = response.choices[0].message.content.strip()
            if text.startswith("{"):
                try:
                    parsed = json.loads(text)
                    return parsed
                except json.JSONDecodeError:
                    logging.warning("âš ï¸ JSONè§£æå¤±æ•—ã€ãƒ†ãƒ³ãƒ—ãƒ¬ç”Ÿæˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")
            return {"copy": text, "alts": []}
        except Exception as e:
            logging.warning(f"âš ï¸ APIã‚¨ãƒ©ãƒ¼: {e} (è©¦è¡Œ {attempt+1}/{retries})")
            await asyncio.sleep(2)
    return {"copy": "ç”Ÿæˆå¤±æ•—", "alts": []}


# ============================================================
# ğŸ§© ã‚¯ãƒ©ã‚¹ã‚¿å˜ä½å‡¦ç†
# ============================================================

async def process_cluster(cluster, idx, total):
    """ã‚¯ãƒ©ã‚¹ã‚¿å˜ä½ã§ç”Ÿæˆ"""
    topic = cluster.get("name", f"å•†å“{idx}")
    keywords = ", ".join(cluster.get("topics", []))[:200]

    prompt = f"""
æ¬¡ã®å•†å“ã®ç‰¹å¾´ã«åŸºã¥ã„ã¦ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼ï¼ˆ40ã€œ60æ–‡å­—ï¼‰ã¨ç”»åƒALTãƒ†ã‚­ã‚¹ãƒˆï¼ˆå„80ã€œ110æ–‡å­—ï¼‰ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ã€å•†å“ã€‘{topic}
ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€‘{keywords}

å‡ºåŠ›ã¯JSONå½¢å¼ã§ï¼š
{{
  "copy": "ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼æ–‡",
  "alts": ["ALTæ–‡1", "ALTæ–‡2", ...20å€‹]
}}
"""
    logging.debug(f"ğŸ§© [{idx}/{total}] Promptæº–å‚™å®Œäº†: {topic}")
    result = await generate_text(prompt)
    if not result.get("alts"):
        logging.warning(f"âš ï¸ ALTæœªç”Ÿæˆ: {topic}")
    return result


# ============================================================
# ğŸš€ ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ============================================================

async def main():
    try:
        semantics, vocab, clusters = load_structures()
    except FileNotFoundError as e:
        logging.critical(f"ğŸš« è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: {e}")
        raise

    logging.info("ğŸŒ¸ KOTOHA ENGINE â€” Hybrid AI Writer (é–‹ç™ºè€…ãƒ¢ãƒ¼ãƒ‰) èµ·å‹•")

    results = []
    total = len(clusters[:700])

    for idx, cluster in enumerate(tqdm(clusters[:700], desc="ğŸª„ ç”Ÿæˆä¸­"), start=1):
        res = await process_cluster(cluster, idx, total)
        results.append(res)

    # çµ±è¨ˆãƒ­ã‚°
    total_alts = sum(len(r.get("alts", [])) for r in results)
    avg_alts = total_alts / len(results) if results else 0
    logging.info(f"ğŸ“Š ALTç”Ÿæˆå¹³å‡æ•°: {avg_alts:.2f}")
    logging.info(f"ğŸ“Š ç·ç”Ÿæˆæ–‡æ•°: {len(results)} ã‚¯ãƒ©ã‚¹ã‚¿ / {total_alts} ALT")

    # å‡ºåŠ›
    out_dir = "./output/ai_writer"
    os.makedirs(out_dir, exist_ok=True)
    out_path = f"{out_dir}/hybrid_writer_full_{datetime.now():%Y%m%d_%H%M}_dev.json"

    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    logging.info(f"ğŸ’¾ å‡ºåŠ›å®Œäº†: {out_path} ({len(results)}ä»¶)")
    logging.info("ğŸ KOTOHA ENGINE Hybrid Writer å®Œäº†")


# ============================================================
# ğŸ”§ ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
# ============================================================

if __name__ == "__main__":
    asyncio.run(main())
import atlas_autosave_core
