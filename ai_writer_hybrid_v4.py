# ============================================
# ğŸŒ¸ KOTOHA ENGINE â€” Hybrid AI Writer v4.1
# å•†å“å˜ä½å®Œå…¨ç”Ÿæˆ + AIæœ€é©åŒ– + é€²æ—ä¿®æ­£ç‰ˆ
# ============================================

import os, json, random, re, time, logging
from datetime import datetime
from tqdm import tqdm
from dotenv import load_dotenv
from openai import OpenAI

# ==========
# è¨­å®š
# ==========
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

OUTPUT_DIR = "./output/ai_writer"
os.makedirs(OUTPUT_DIR, exist_ok=True)

SEMANTICS_PATH = "./output/semantics/structured_semantics_20251030_224846.json"
VOCAB_PATH = "./output/market_vocab_20251030_201906.json"
CLUSTER_PATH = "./output/lexical_clusters_20251030_223013.json"

ALT_COUNT = 20
COPY_RANGE = (40, 60)
ALT_RANGE = (80, 110)

logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)-8s | %(message)s")

# ==========
# é–¢æ•°ç¾¤
# ==========
def load_json(path):
    if not os.path.exists(path):
        raise FileNotFoundError(f"âŒ {path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def clean_text(text):
    text = re.sub(r"\s+", " ", text.strip())
    return text.replace("ï¼", "ã€‚").replace("!", "ã€‚")

def is_valid_length(text, min_len, max_len):
    return min_len <= len(text) <= max_len

def remove_invalid_specs(text, valid_keywords):
    for w in re.findall(r"[A-Za-z0-9]+\w*", text):
        if w not in valid_keywords and re.search(r"\d", w):
            text = text.replace(w, "")
    return text

def decide_ai_usage(vocab_density, category_entropy):
    base = 0.3
    if vocab_density < 0.5:
        base += 0.3
    if category_entropy < 0.4:
        base += 0.1
    return random.random() < min(base, 0.8)

def ai_generate_copy_alt(product_name, keywords, context):
    prompt = f"""
ã‚ãªãŸã¯SEOã«å¼·ã„æ—¥æœ¬èªã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚
å•†å“åã€Œ{product_name}ã€ã«åŸºã¥ã„ã¦ã€ä»¥ä¸‹ã®èªå½™ç¾¤ã‚’æ´»ã‹ã—ã€
è‡ªç„¶ã§é­…åŠ›çš„ãªã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼ï¼ˆ40ï½60å­—ï¼‰ã¨ALTæ–‡ï¼ˆå„80ï½110å­—ï¼‰20æœ¬ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

èªå½™ç¾¤: {', '.join(keywords[:15])}
æ–‡ä½“: èª å®Ÿã§çŸ¥çš„ã€ã‚¦ã‚£ãƒƒãƒˆã«å¯Œã‚€
å‡ºåŠ›å½¢å¼:
{{
  "copy": "ã€œ",
  "alt": ["ã€œ","ã€œ",...20ä»¶]
}}
"""
    try:
        res = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.8,
        )
        data = json.loads(res.choices[0].message.content)
        return data.get("copy", ""), data.get("alt", [])
    except Exception as e:
        logging.warning(f"âš ï¸ AIç”Ÿæˆå¤±æ•—: {e}")
        copy = f"{product_name} â€” é«˜å“è³ªã§ä¿¡é ¼ã®ã‚ã‚‹ä¸€å“ã€‚"
        alt = [f"{product_name} ã®é­…åŠ›ã‚’ä¼ãˆã‚‹é«˜è§£åƒåº¦ç”»åƒ"] * ALT_COUNT
        return copy, alt

# ==========
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ==========
def main():
    start = time.time()
    logging.info("ğŸŒ¸ Hybrid AI Writer v4.1 èµ·å‹• â€” å•†å“å˜ä½å®Œå…¨ç”Ÿæˆãƒ¢ãƒ¼ãƒ‰")

    semantics = load_json(SEMANTICS_PATH)
    vocab = load_json(VOCAB_PATH)
    clusters = load_json(CLUSTER_PATH)

    results = []
    ai_calls, tmpl_uses = 0, 0

    for idx, v in enumerate(tqdm(vocab, desc="ğŸª„ å•†å“ç”Ÿæˆä¸­", total=len(vocab))):
        # æ§‹é€ æ­£è¦åŒ–
        if isinstance(v, dict):
            name = v.get("name") or v.get("å•†å“å") or "ç„¡é¡Œå•†å“"
            keywords = v.get("keywords") or v.get("èªå½™") or []
        elif isinstance(v, list):
            name = v[0] if v else "ç„¡é¡Œå•†å“"
            keywords = v[1:] if len(v) > 1 else []
        else:
            name = str(v)
            keywords = []

        context = clusters[idx % len(clusters)]
        valid_words = [w for w in keywords if len(w) > 1]

        vocab_density = len(valid_words) / 50
        category_entropy = random.random()  # ãƒ€ãƒŸãƒ¼ã€‚å®Ÿéš›ã¯ã‚¯ãƒ©ã‚¹ã‚¿åˆ†æ•£ç‡ãªã©ã§è¨ˆç®—
        use_ai = decide_ai_usage(vocab_density, category_entropy)

        if use_ai:
            copy, alt = ai_generate_copy_alt(name, keywords, context)
            ai_calls += 1
        else:
            tmpl_uses += 1
            copy = clean_text(f"{name} â€” {random.choice(['é«˜æ€§èƒ½', 'æ–°ç™»å ´', 'å¿«é©ãªä½¿ç”¨æ„Ÿ', 'ä¿¡é ¼ã®å“è³ª'])}ã‚’å®Ÿç¾ã€‚")
            alt = [
                clean_text(f"{name} {random.choice(['é«˜è€ä¹…', 'å¤šæ©Ÿèƒ½', 'è»½é‡è¨­è¨ˆ', 'ã‚¹ã‚¿ã‚¤ãƒªãƒƒã‚·ãƒ¥'])}ã§ä½¿ã„ã‚„ã™ã„ãƒ‡ã‚¶ã‚¤ãƒ³ã€‚")
                for _ in range(ALT_COUNT)
            ]

        copy = clean_text(remove_invalid_specs(copy, valid_words))
        alt = [clean_text(remove_invalid_specs(a, valid_words)) for a in alt]

        if not is_valid_length(copy, *COPY_RANGE):
            copy = copy[:COPY_RANGE[1]]
        alt = [a[:ALT_RANGE[1]] if not is_valid_length(a, *ALT_RANGE) else a for a in alt]

        results.append({"product_id": idx + 1, "name": name, "copy": copy, "alt": alt})

    ts = datetime.now().strftime("%Y%m%d_%H%M")
    output_path = f"{OUTPUT_DIR}/hybrid_writer_full_{ts}.json"
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    total = len(results)
    avg_alt = sum(len(r["alt"]) for r in results) / total
    logging.info(f"âœ… å‡ºåŠ›å®Œäº†: {output_path}")
    logging.info(f"ğŸ“Š å•†å“æ•°={total} / Copyé•·å¹³å‡={sum(len(r['copy']) for r in results)//total} / ALTæ•°å¹³å‡={avg_alt}")
    logging.info(f"ğŸ¤– AIç”Ÿæˆ={ai_calls}ä»¶ / ãƒ†ãƒ³ãƒ—ãƒ¬å±•é–‹={tmpl_uses}ä»¶")
    logging.info(f"â± å®Ÿè¡Œæ™‚é–“: {time.time() - start:.1f}s")

# ==========
if __name__ == "__main__":
    main()
import atlas_autosave_core
