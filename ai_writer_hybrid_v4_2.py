# -*- coding: utf-8 -*-
"""
KOTOHA ENGINE â€” Hybrid AI Writer v4.2 (CSVåŸºç‚¹ãƒ»å®Œå…¨ç‰ˆ)
- å…¥åŠ›: ./input.csvï¼ˆcp932 / Shift-JISç›¸å½“ï¼‰
  å¿…é ˆåˆ—: ã€Œå•†å“åã€ã€Œã‚¸ãƒ£ãƒ³ãƒ«IDã€  â€»R, X..CC(ALT)ã¯æœ€çµ‚CSVã§ä½¿ç”¨ã™ã‚‹ãŒæœ¬ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯JSONå‡ºåŠ›
- å‚ç…§: ./output/semantics/structured_semantics_*.jsonï¼ˆä»»æ„ï¼‰
        ./output/lexical_clusters_*.jsonï¼ˆæ¨å¥¨ï¼šã‚¯ãƒ©ã‚¹ã‚¿èªå½™ï¼‰
- å‡ºåŠ›: ./output/ai_writer/hybrid_writer_full_YYYYMMDD_HHMM.json
- ä»•æ§˜:
  * å…¨å•†å“ï¼ˆä¾‹: ~763ä»¶ï¼‰ã«å¯¾ã— Copy=1, ALT=20 ã‚’å¿…ãšç”Ÿæˆ
  * ç”Ÿæˆæ–¹å¼ã¯ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ï¼š
      - ç´„30%: OpenAIã§ç”Ÿæˆï¼ˆOPENAI_ENABLE=true ã®æ™‚ï¼‰
      - ç´„70%: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ»ãƒ†ãƒ³ãƒ—ãƒ¬å±•é–‹ï¼ˆé«˜å“è³ªãƒ»é•·ã•éµå®ˆï¼‰
  * é•·ã•åˆ¶ç´„: Copy=40â€“60ï¼ˆå…¨è§’æ›ç®—ï¼‰ / ALT=80â€“110ï¼ˆå…¨è§’æ›ç®—ï¼‰
  * ãƒ¦ãƒ‹ãƒ¼ã‚¯æ€§: ALTã¯20æœ¬ã™ã¹ã¦ç•°ãªã‚‹ï¼ˆé‡è¤‡é™¤å»ï¼‹å¾®å·®ç”Ÿæˆï¼‰
  * ç¦å‰‡: ã€Œå­˜åœ¨ã—ãªã„ã‚¹ãƒšãƒƒã‚¯ã€æ¨å®šã‚’ç·©å’Œï¼ˆæœ€çµ‚CSVç›´å‰ã®ãƒ«ãƒ¼ãƒ«ã§å¼·åˆ¶ï¼‰â†’ã“ã“ã§ã¯â€œéœ²éª¨ãªèª‡å¼µèªâ€ã®ã¿æŠ‘åˆ¶
  * é€²æ—ãƒ»çµ±è¨ˆ: tqdmï¼‹è¦ç´„ãƒ­ã‚°
"""

import os
import re
import json
import glob
import math
import time
import random
import string
from datetime import datetime
from collections import Counter, defaultdict

import pandas as pd
from dotenv import load_dotenv
from tqdm import tqdm

# ------- è¨­å®š -------
SEED = 42
random.seed(SEED)

COPY_MIN, COPY_MAX = 40, 60           # å…¨è§’æ›ç®—
ALT_MIN, ALT_MAX   = 80, 110          # å…¨è§’æ›ç®—
ALT_COUNT_PER_ITEM = 20

INPUT_CSV = "./input.csv"
SEMANTICS_DIR = "./output/semantics"
CLUSTERS_DIR  = "./output"

OUT_DIR = "./output/ai_writer"
os.makedirs(OUT_DIR, exist_ok=True)

# ------- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ -------
def zlen(s: str) -> int:
    """å…¨è§’æ›ç®—é•·ï¼ˆã–ã£ãã‚Šï¼šASCIIã¯0.5, éASCIIã¯1.0ã¨ã—ã¦ã‚«ã‚¦ãƒ³ãƒˆï¼‰"""
    if not s:
        return 0
    half = sum(ch in string.printable and ch not in "ã€€" for ch in s)
    full = len(s) - half
    # åŠè§’2ã¤ã§å…¨è§’1æ›ç®—
    return full + math.ceil(half / 2)

def clamp_len(s: str, lo: int, hi: int) -> str:
    """å…¨è§’é•·ãŒç¯„å›²å¤–ãªã‚‰å¾®èª¿æ•´ï¼ˆçŸ­ã„â†’ä»˜ã‘è¶³ã—ã€é•·ã„â†’å®‰å…¨ãªå½¢ã§ã‚«ãƒƒãƒˆï¼‰ã€‚"""
    txt = s.strip()
    L = zlen(txt)
    if L < lo:
        # è¶³ã—å…·ï¼ˆèªå°¾ã‚’å´©ã•ãšè‡ªç„¶ãªè¿½è¨˜ï¼‰
        suffix_bank = [" â€” è©³ç´°ã¯å•†å“ãƒšãƒ¼ã‚¸ã¸", "ã€‚é¸ã°ã‚Œã‚‹å®šç•ªä»•æ§˜", "ã€‚æ—¥å¸¸ã‚’å¿«é©ã«", "ã€‚ä½¿ã†ã»ã©ä¾¿åˆ©", "ã€‚ã‚·ãƒ³ãƒ—ãƒ«ã«å¿ƒåœ°ã‚ˆã"]
        while zlen(txt) < lo:
            txt += random.choice(suffix_bank)
    elif L > hi:
        # å¥ç‚¹ãƒ»èª­ç‚¹ãƒ»è¨˜å·ã§å®‰å…¨ã‚«ãƒƒãƒˆ
        cut_points = [m.start() for m in re.finditer(r"[ã€‚ï¼ã€,ãƒ»/|ï½œ\-ãƒ»!ï¼\s]", txt)]
        if cut_points:
            # ã‚‚ã£ã¨ã‚‚è¿‘ã„æ‰‹å‰ã®ã‚«ãƒƒãƒˆç‚¹
            pos = None
            for p in cut_points:
                if zlen(txt[:p]) <= hi:
                    pos = p
                else:
                    break
            if pos:
                txt = txt[:pos].rstrip("ã€ã€‚,.!ï¼ãƒ»-/|ï½œ")
            else:
                # ç·Šæ€¥ã‚«ãƒƒãƒˆ
                txt = txt[:max(1, min(len(txt), hi))]
        else:
            txt = txt[:max(1, min(len(txt), hi))]
    return txt

def uniqueify(lines):
    """å®Œå…¨ä¸€è‡´ãƒ»å‰å¾Œç©ºç™½å·®ã®é‡è¤‡ã‚’é™¤å»ã—é †åºä¿æŒã€‚"""
    seen = set()
    out = []
    for s in lines:
        key = s.strip()
        if key not in seen and key:
            seen.add(key)
            out.append(s)
    return out

def safe_json_load(path):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None

# ------- ãƒ‡ãƒ¼ã‚¿èª­è¾¼ -------
def load_csv_items(path=INPUT_CSV):
    df = pd.read_csv(path, encoding="cp932", dtype=str).fillna("")
    items = []
    for _, row in df.iterrows():
        name  = row.get("å•†å“å", "").strip()
        genre = row.get("ã‚¸ãƒ£ãƒ³ãƒ«ID", "").strip()
        if name:
            items.append({"name": name, "genre": genre})
    return items

def latest_json(pattern):
    files = glob.glob(pattern)
    if not files:
        return None
    files.sort(key=os.path.getmtime, reverse=True)
    return files[0]

def load_support():
    sem_path = latest_json(os.path.join(SEMANTICS_DIR, "structured_semantics_*.json"))
    clu_path = latest_json(os.path.join(CLUSTERS_DIR,  "lexical_clusters_*.json"))
    semantics = safe_json_load(sem_path) or {}
    clusters  = safe_json_load(clu_path) or {}
    # ã‚¯ãƒ©ã‚¹ã‚¿JSONã¯ {cluster_name: {keywords:[], patterns:[]}} or [ {name, keywords} ] ã®åŒæ–¹ã«å¯¾å¿œ
    cluster_list = []
    if isinstance(clusters, dict):
        for k, v in clusters.items():
            kws = v.get("keywords") or v.get("words") or []
            cluster_list.append({"name": k, "keywords": kws})
    elif isinstance(clusters, list):
        for c in clusters:
            nm = c.get("name") or c.get("cluster") or "cluster"
            kws = c.get("keywords") or c.get("words") or []
            cluster_list.append({"name": nm, "keywords": kws})
    return semantics, cluster_list

# ------- ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼ˆAIã‚³ãƒ¼ãƒ«é¸å®šï¼‰ -------
def tokenize(s: str):
    s = re.sub(r"[ã€ã€‘\[\]ï¼ˆï¼‰()!ï¼/ï½œ|,ã€.\-ï¼‹+]", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return [w for w in s.split(" ") if w]

def novelty_score(name_tokens, cluster_kws):
    # å•†å“åã¨ã‚¯ãƒ©ã‚¹ã‚¿èªå½™ã®éé‡è¤‡ç‡ã‚’ã‚¹ã‚³ã‚¢åŒ–ï¼ˆé«˜ã„ã»ã©æ–°è¦æ€§ï¼AIå‘ãï¼‰
    if not cluster_kws:
        return 0.5
    overlap = len(set(name_tokens) & set(cluster_kws))
    base = 1.0 - (overlap / (len(set(name_tokens)) + 1e-6))
    return max(0.0, min(1.0, base))

def density_score(name_tokens):
    # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒå¤šã„ã»ã©æ§‹æ–‡ãŒè¤‡é›‘â†’AIå‘ã
    n = len(name_tokens)
    return max(0.0, min(1.0, (n - 4) / 12))  # 4èªã§0ã€16èªã§ã»ã¼1

def rarity_score(genre, genre_freq):
    # ç¾åœ¨ã¾ã§ã®ã‚¸ãƒ£ãƒ³ãƒ«é »åº¦ãŒå°‘ãªã„ã»ã©AIå‘ãï¼ˆå¤šæ§˜æ€§ç¢ºä¿ï¼‰
    total = sum(genre_freq.values()) + 1e-6
    freq = genre_freq.get(genre or "NA", 0) + 1e-6
    rarity = 1.0 - (freq / (total))
    return max(0.0, min(1.0, rarity))

def choose_ai_indices(items, clusters, target_ratio=0.30):
    # å„å•†å“ã«å¯¾ã—ã‚¹ã‚³ã‚¢è¨ˆç®— â†’ ä¸Šä½30%ã‚’AI
    scores = []
    genre_freq = Counter()
    for idx, it in enumerate(items):
        tokens = tokenize(it["name"])
        clu = clusters[idx % max(1, len(clusters))] if clusters else {"keywords": []}
        base = 0.5 * novelty_score(tokens, clu.get("keywords", [])) \
             + 0.35 * density_score(tokens) \
             + 0.15 * rarity_score(it.get("genre",""), genre_freq)
        scores.append((idx, base))
        # é »åº¦æ›´æ–°ï¼ˆå¾Œç¶šã® rarity ã«åŠ¹ãï¼‰
        genre_freq[it.get("genre","")] += 1

    scores.sort(key=lambda x: x[1], reverse=True)
    k = max(1, int(round(len(items) * target_ratio)))
    ai_idx = set(i for i, _ in scores[:k])
    return ai_idx

# ------- ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ãƒ³ãƒ—ãƒ¬ï¼ˆé«˜å“è³ªï¼‰ -------
COPY_PATTERNS = [
    "{core}ã€{benefit}ã€‚{scene}",
    "{core} â€” {benefit}ã§ã€{scene} ã‚’å¿«é©ã«ã€‚",
    "{core}ã€‚{spec} ã‚’å‚™ãˆã€{scene}ã«ã¡ã‚‡ã†ã©ã„ã„ã€‚"
]

ALT_PATTERNS = [
    "{core} / {spec} / {benefit} / {scene}",
    "{scene}ã«æœ€é©ãª{core}ï¼ˆ{spec}ï¼‰â€” {benefit}",
    "{brand}{category}{core}ï¼š{benefit}ã€{scene}ã€{spec}",
    "{core} | {scene} | {benefit} | {spec}"
]

BENEFITS = [
    "æ¯æ—¥ã‚’è»½ã‚„ã‹ã«ã™ã‚‹ä½¿ã„å¿ƒåœ°", "ã‚¹ãƒˆãƒ¬ã‚¹ã®ãªã„æ“ä½œæ„Ÿ", "é•·ãä½¿ãˆã‚‹å®‰å¿ƒè¨­è¨ˆ",
    "ç„¡é§„ã®ãªã„ç¾ã—ã•", "å¿™ã—ã„æ—¥å¸¸ã«ãƒ•ã‚£ãƒƒãƒˆ", "ã¯ã˜ã‚ã¦ã§ã‚‚è¿·ã‚ãªã„"
]
SCENES = [
    "è‡ªå®…ã§ã‚‚å¤–å‡ºå…ˆã§ã‚‚", "ã‚ªãƒ•ã‚£ã‚¹ã¨ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã«", "æ—…è¡Œã‚„å‡ºå¼µã«",
    "é€šå‹¤ãƒ»é€šå­¦ã®ç›¸æ£’ã«", "ã‚®ãƒ•ãƒˆã«ã‚‚æœ€é©", "ãƒ¯ãƒ¼ã‚¯ã¨è¶£å‘³ã®ä¸¡ç«‹ã«"
]
SPECS_HINT = [
    "è»½é‡è¨­è¨ˆ", "è€ä¹…æ€§ã®é«˜ã„ç´ æ", "æŒã¡é‹ã³ã—ã‚„ã™ã„ã‚µã‚¤ã‚º", "æ—¥å¸¸ä½¿ã„ã«ååˆ†ãªæ€§èƒ½",
    "ãŠæ‰‹å…¥ã‚Œç°¡å˜", "ä½¿ã„å‹æ‰‹ã‚’å„ªå…ˆã—ãŸè¨­è¨ˆ"
]
BRANDS_HINT = ["", "", ""]

def sample_words(name: str, cluster_kws):
    # å•†å“åã‹ã‚‰æ ¸èªï¼ˆcoreï¼‰ãƒ»ã‚«ãƒ†ã‚´ãƒªæ¨å®š
    tokens = tokenize(name)
    # æ ¸ï¼ˆç›®ç«‹ã¤èªï¼‰ã‚’é©å½“ã«æŠ½å‡ºï¼ˆç°¡æ˜“ï¼‰
    core = " ".join(tokens[:3]) if tokens else name[:12]
    category = ""
    for w in ["ã‚±ãƒ¼ã‚¹","ãƒ•ã‚£ãƒ«ãƒ ","å……é›»å™¨","ã‚±ãƒ¼ãƒ–ãƒ«","ãƒãƒ³ãƒ‰","ãƒãƒƒã‚°","ãƒªãƒ¥ãƒƒã‚¯","ãƒ‰ãƒ¬ã‚¹","æ°´ç€"]:
        if w in name:
            category = w
            break
    # specã¯ã‚¯ãƒ©ã‚¹ã‚¿èªå½™ã¨ãƒ’ãƒ³ãƒˆã‹ã‚‰
    spec = random.choice(SPECS_HINT + (cluster_kws or []) or ["ä½¿ã„ã‚„ã™ã•é‡è¦–"])
    benefit = random.choice(BENEFITS)
    scene = random.choice(SCENES)
    brand = random.choice(BRANDS_HINT)

    return dict(core=core, category=category, spec=spec, benefit=benefit, scene=scene, brand=brand)

def local_copy(name, cluster_kws):
    w = sample_words(name, cluster_kws)
    s = random.choice(COPY_PATTERNS).format(**w)
    s = re.sub(r"\s{2,}", " ", s).strip(" ãƒ»/|ï½œ")
    s = clamp_len(s, COPY_MIN, COPY_MAX)
    s = s.rstrip("!ï¼")  # æ„Ÿå˜†ç¬¦ã¯é¿ã‘ã‚‹æ–¹é‡
    return s

def local_alts(name, cluster_kws, need=ALT_COUNT_PER_ITEM):
    outs = []
    used = set()
    # ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³æº
    kws = list(set((cluster_kws or []) + tokenize(name)))
    random.shuffle(kws)
    i = 0
    while len(outs) < need and i < need * 5:
        i += 1
        w = sample_words(name, kws[:6])
        tmpl = random.choice(ALT_PATTERNS)
        s = tmpl.format(**w)
        s = re.sub(r"\s{2,}", " ", s).strip(" ãƒ»/|ï½œ")
        s = clamp_len(s, ALT_MIN, ALT_MAX)
        if s not in used:
            outs.append(s); used.add(s)
    # ä¸è¶³åˆ†ã¯å¾®å·®ã§è£œã†
    while len(outs) < need and outs:
        base = random.choice(outs)
        tweak = random.choice(["â€” è©³ã—ãã¯å•†å“ãƒšãƒ¼ã‚¸ã¸", " / ä½¿ã„ã‚„ã™ã•ã‚’è¿½æ±‚", " / æ—¥å¸¸ã«ã¡ã‚‡ã†ã©ã„ã„"])
        s = clamp_len(base + tweak, ALT_MIN, ALT_MAX)
        if s not in used:
            outs.append(s); used.add(s)
    return outs[:need]

# ------- OpenAIï¼ˆä»»æ„ï¼‰ -------
def openai_client():
    try:
        from openai import OpenAI
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            return None
        return OpenAI(api_key=api_key)
    except Exception:
        return None

OPENAI_PROMPT = """ã‚ãªãŸã¯å„ªç§€ãªECã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼å…¼SEOã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚
ä¸ãˆã‚‰ã‚ŒãŸã€Œå•†å“åã€ã€Œæ–‡è„ˆèªã€ï¼ˆä»»æ„ï¼‰ã‚’ã‚‚ã¨ã«ã€
1) è³¼è²·æ„æ¬²ã‚’å–šèµ·ã™ã‚‹ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼ï¼ˆå…¨è§’40ã€œ60å­—ï¼‰
2) SEOã«æœ€é©åŒ–ã•ã‚ŒãŸç”»åƒALTãƒ†ã‚­ã‚¹ãƒˆï¼ˆå…¨è§’80ã€œ110å­—ï¼‰Ã—20æœ¬
ã‚’æ—¥æœ¬èªã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

åˆ¶ç´„:
- å¥èª­ç‚¹ãƒ»åŠ©è©ã®è‡ªç„¶ã•ã‚’æœ€å„ªå…ˆã€‚
- æ„Ÿå˜†ç¬¦ã¯é¿ã‘ã‚‹ã€‚
- äº‹å®Ÿã«éåº¦ãªæ¨æ¸¬ã‚’åŠ ãˆãªã„ï¼ˆæ›–æ˜§ãªå ´åˆã¯ä¸€èˆ¬çš„ä¾¡å€¤ã«å¯„ã›ã‚‹ï¼‰ã€‚
- ALTã¯ã™ã¹ã¦ç•°ãªã‚‹å†…å®¹ã«ã™ã‚‹ã€‚
- å‡ºåŠ›ã¯strictãªJSONã§:
{{
  "copy": "<string>",
  "alts": ["<string>", ... 20æœ¬]
}}
"""

def call_openai_copy_alts(client, name, context_words):
    try:
        ctx = "ã€".join(context_words[:8]) if context_words else ""
        prompt = f"å•†å“å: {name}\næ–‡è„ˆèª: {ctx}\n\n" + OPENAI_PROMPT
        resp = client.chat.completions.create(
            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            messages=[{"role":"user","content":prompt}],
            temperature=0.6,
            max_tokens=800
        )
        txt = resp.choices[0].message.content.strip()
        data = json.loads(txt)
        copy = clamp_len(data.get("copy",""), COPY_MIN, COPY_MAX)
        alts = [clamp_len(a, ALT_MIN, ALT_MAX) for a in (data.get("alts") or [])]
        alts = uniqueify(alts)[:ALT_COUNT_PER_ITEM]
        # ä¸è¶³è£œå®Œ
        if len(alts) < ALT_COUNT_PER_ITEM:
            alts += local_alts(name, context_words, ALT_COUNT_PER_ITEM - len(alts))
        return copy, alts
    except Exception:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        copy = local_copy(name, context_words)
        alts = local_alts(name, context_words, ALT_COUNT_PER_ITEM)
        return copy, alts

# ------- ãƒ¡ã‚¤ãƒ³å‡¦ç† -------
def main():
    load_dotenv()  # .env ã® OPENAI_ENABLE ã‚’å‚ç…§
    openai_enable = (os.getenv("OPENAI_ENABLE","false").lower() == "true")
    client = openai_client() if openai_enable else None

    # å…¥åŠ›
    if not os.path.exists(INPUT_CSV):
        raise FileNotFoundError(f"input.csv ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {INPUT_CSV}")
    items = load_csv_items(INPUT_CSV)
    semantics, clusters = load_support()

    total = len(items)
    if total == 0:
        raise RuntimeError("CSVã«æœ‰åŠ¹ãªå•†å“è¡ŒãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆå•†å“åãŒç©ºï¼‰")

    # AIã‚³ãƒ¼ãƒ«é¸å®šï¼ˆç´„30%ï¼‰
    ai_indices = choose_ai_indices(items, clusters, target_ratio=0.30)

    results = []
    ai_count = 0
    tpl_count = 0

    print("ğŸŒ¸ Hybrid AI Writer v4.2 å®Ÿè¡Œé–‹å§‹")
    pbar = tqdm(range(total), desc="ğŸª„ å•†å“ç”Ÿæˆä¸­", total=total)
    for i in pbar:
        prod = items[i]
        name = prod["name"]
        clu = clusters[i % max(1, len(clusters))] if clusters else {"keywords": []}
        ctx_words = clu.get("keywords", [])

        use_ai = (i in ai_indices) and (client is not None)
        if use_ai:
            copy, alts = call_openai_copy_alts(client, name, ctx_words)
            ai_count += 1
        else:
            copy = local_copy(name, ctx_words)
            alts = local_alts(name, ctx_words, ALT_COUNT_PER_ITEM)
            tpl_count += 1

        # æœ€çµ‚ã‚¬ãƒ¼ãƒ‰ï¼ˆé•·ã•ãƒ»ä»¶æ•°ãƒ»é‡è¤‡ï¼‰
        copy = clamp_len(copy, COPY_MIN, COPY_MAX)
        alts = uniqueify([clamp_len(a, ALT_MIN, ALT_MAX) for a in alts])[:ALT_COUNT_PER_ITEM]
        while len(alts) < ALT_COUNT_PER_ITEM:
            # è¿½åŠ å¾®å·®
            base = local_alts(name, ctx_words, 1)[0]
            if base not in alts:
                alts.append(base)

        results.append({
            "index": i,
            "product_name": name,
            "genre": prod.get("genre",""),
            "copy": copy,
            "alts": alts
        })
        if (i+1) % 50 == 0 or i == total-1:
            pbar.set_postfix({"AI": ai_count, "TPL": tpl_count})

    # å‡ºåŠ›
    ts = datetime.now().strftime("%Y%m%d_%H%M")
    out_path = os.path.join(OUT_DIR, f"hybrid_writer_full_{ts}.json")
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump({"items": results}, f, ensure_ascii=False, indent=2)

    # æ¤œè¨¼
    miss_copy = sum(1 for r in results if not r["copy"])
    miss_alts = sum(1 for r in results if len(r["alts"]) != ALT_COUNT_PER_ITEM)
    avg_copy = sum(zlen(r["copy"]) for r in results)/total
    avg_alts = sum(sum(zlen(a) for a in r["alts"])/ALT_COUNT_PER_ITEM for r in results)/total

    print("âœ… å‡ºåŠ›å®Œäº†:", out_path)
    print(f"ğŸ“Š ä»¶æ•°: {total}ï¼ˆAI:{ai_count} / TPL:{tpl_count}ï¼‰")
    print(f"ğŸ“ Copyå¹³å‡é•·: {avg_copy:.1f} / ALTå¹³å‡é•·: {avg_alts:.1f}")
    print(f"ğŸ” æ¬ è½ç¢ºèª: Copyæ¬ è½={miss_copy}, ALTä»¶æ•°ä¸æ­£={miss_alts}")

if __name__ == "__main__":
    main()
import atlas_autosave_core
