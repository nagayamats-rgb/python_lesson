# -*- coding: utf-8 -*-
"""
KOTOHA ENGINE â€” Hybrid AI Writer v4.3ï¼ˆæ¥½å¤©CSVå®Œå…¨å¯¾å¿œç‰ˆï¼‰
--------------------------------------------------------------
ãƒ»å…¥åŠ›: /Users/tsuyoshi/Desktop/python_lesson/input.csv
ãƒ»å‡ºåŠ›: ./output/ai_writer/hybrid_writer_full_YYYYMMDD_HHMM.json
ãƒ»ä»•æ§˜:
  - å…¨å•†å“è¡Œã«å¯¾ã—ã¦ Copy=1 + ALT=20ï¼ˆå…¨è§’40ã€œ60 / 80ã€œ110ï¼‰
  - ç´„30%ã‚’AIç”Ÿæˆï¼ˆOPENAI_ENABLE=trueï¼‰
  - æ®‹ã‚Šã‚’é«˜å“è³ªãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§è‡ªå‹•ç”Ÿæˆ
  - JSONæ•´åˆï¼†é•·ã•ãƒ»ãƒ¦ãƒ‹ãƒ¼ã‚¯æ€§æ¤œè¨¼
"""

import os, re, json, glob, math, random, string
import pandas as pd
from datetime import datetime
from dotenv import load_dotenv
from tqdm import tqdm
from collections import Counter

# ----------------------------------------------------------
# è¨­å®š
# ----------------------------------------------------------
INPUT_CSV = "/Users/tsuyoshi/Desktop/python_lesson/input.csv"
SEMANTICS_DIR = "./output/semantics"
CLUSTERS_DIR  = "./output"
OUT_DIR = "./output/ai_writer"
os.makedirs(OUT_DIR, exist_ok=True)

SEED = 42
random.seed(SEED)
COPY_MIN, COPY_MAX = 40, 60
ALT_MIN, ALT_MAX = 80, 110
ALT_COUNT = 20

# ----------------------------------------------------------
# åŸºæœ¬ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ----------------------------------------------------------
def zlen(s: str) -> int:
    if not s: return 0
    half = sum(ch in string.printable and ch not in "ã€€" for ch in s)
    full = len(s) - half
    return full + math.ceil(half / 2)

def clamp_len(s: str, lo: int, hi: int) -> str:
    txt = s.strip()
    if zlen(txt) < lo:
        while zlen(txt) < lo:
            txt += "ã€‚æ—¥å¸¸ã‚’å¿«é©ã«"
    elif zlen(txt) > hi:
        txt = txt[:hi]
    return txt.strip("!ï¼ã€ã€‚")

def uniqueify(lines):
    seen, out = set(), []
    for s in lines:
        k = s.strip()
        if k and k not in seen:
            seen.add(k)
            out.append(s)
    return out

def safe_json_load(path):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None

# ----------------------------------------------------------
# CSV èª­ã¿è¾¼ã¿ï¼ˆæ¥½å¤©RMSå¯¾å¿œï¼‰
# ----------------------------------------------------------
def load_csv_items(path=INPUT_CSV):
    df = pd.read_csv(path, encoding="cp932", dtype=str, low_memory=False).fillna("")
    cols = [str(c).strip() for c in df.columns]

    # åˆ—ç‰¹å®š
    name_candidates = [c for c in cols if "å•†å“å" in c and "ALT" not in c]
    genre_candidates = [c for c in cols if "ã‚¸ãƒ£ãƒ³ãƒ«ID" in c]
    name_col = name_candidates[0] if name_candidates else None
    genre_col = genre_candidates[0] if genre_candidates else None

    if not name_col:
        raise ValueError(f"âŒ 'å•†å“å' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ¤œå‡ºåˆ—: {cols[:30]}")
    if not genre_col:
        print("âš ï¸ 'ã‚¸ãƒ£ãƒ³ãƒ«ID' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆç©ºæ¬„ã§ç¶šè¡Œï¼‰")

    items = []
    for _, row in df.iterrows():
        name = str(row.get(name_col, "")).strip()
        genre = str(row.get(genre_col, "")).strip() if genre_col else ""
        if name:
            items.append({"name": name, "genre": genre})

    print(f"âœ… CSVèª­è¾¼å®Œäº†: {len(items)}ä»¶ï¼ˆåˆ—: name={name_col}, genre={genre_col or 'N/A'}ï¼‰")
    return items

# ----------------------------------------------------------
# ã‚¯ãƒ©ã‚¹ã‚¿ãƒ»ã‚»ãƒãƒ³ãƒ†ã‚£ã‚¯ã‚¹èª­è¾¼
# ----------------------------------------------------------
def latest_json(pattern):
    files = glob.glob(pattern)
    return sorted(files, key=os.path.getmtime, reverse=True)[-1] if files else None

def load_support():
    sem = latest_json(os.path.join(SEMANTICS_DIR, "structured_semantics_*.json"))
    clu = latest_json(os.path.join(CLUSTERS_DIR, "lexical_clusters_*.json"))
    semantics = safe_json_load(sem) or {}
    clusters = safe_json_load(clu) or {}
    clist = []
    if isinstance(clusters, dict):
        for k,v in clusters.items():
            kws = v.get("keywords") or v.get("words") or []
            clist.append({"name": k, "keywords": kws})
    elif isinstance(clusters, list):
        for c in clusters:
            nm = c.get("name") or "cluster"
            kws = c.get("keywords") or []
            clist.append({"name": nm, "keywords": kws})
    return semantics, clist

# ----------------------------------------------------------
# AI ã‚³ãƒ¼ãƒ«é¸å®šï¼ˆ30%ï¼‰
# ----------------------------------------------------------
def tokenize(s): return re.findall(r"[\wä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ³ãƒ¼]+", s)

def choose_ai_indices(items, clusters, ratio=0.3):
    scores, genre_freq = [], Counter()
    for i, it in enumerate(items):
        name = it["name"]
        toks = tokenize(name)
        clu = clusters[i % len(clusters)] if clusters else {"keywords":[]}
        overlap = len(set(toks) & set(clu.get("keywords", [])))
        base = 1 - (overlap / (len(toks) + 1e-6))
        rarity = 1 - (genre_freq[it.get("genre","")] / (sum(genre_freq.values())+1e-6)) if genre_freq else 1
        score = 0.6*base + 0.4*rarity
        scores.append((i,score))
        genre_freq[it.get("genre","")] += 1
    scores.sort(key=lambda x:x[1], reverse=True)
    n = max(1, int(len(items)*ratio))
    return {i for i,_ in scores[:n]}

# ----------------------------------------------------------
# ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ãƒ³ãƒ—ãƒ¬ç”Ÿæˆ
# ----------------------------------------------------------
COPY_PATTERNS = [
    "{core}ã€{benefit}ã€‚{scene}",
    "{core} â€” {benefit}ã§{scene}ã‚’å¿«é©ã«ã€‚",
    "{core}ã€‚{spec}ã‚’å‚™ãˆã€{scene}ã«ã´ã£ãŸã‚Šã€‚"
]
ALT_PATTERNS = [
    "{core} / {spec} / {benefit} / {scene}",
    "{scene}ã«æœ€é©ãª{core}ï¼ˆ{spec}ï¼‰â€” {benefit}",
    "{core} | {scene} | {benefit} | {spec}"
]
BENEFITS = ["å¿«é©ãªä½¿ã„å¿ƒåœ°", "é•·ãä½¿ãˆã‚‹å®‰å¿ƒè¨­è¨ˆ", "ä½¿ã„ã‚„ã™ã•ã‚’è¿½æ±‚", "ã‚·ãƒ³ãƒ—ãƒ«ã§é£½ãã®ã“ãªã„ãƒ‡ã‚¶ã‚¤ãƒ³"]
SCENES = ["è‡ªå®…ã§ã‚‚å¤–å‡ºå…ˆã§ã‚‚", "ã‚ªãƒ•ã‚£ã‚¹ã‚„æ—…è¡Œã«", "é€šå‹¤ãƒ»é€šå­¦ã«ã‚‚", "ã‚®ãƒ•ãƒˆã«ã‚‚æœ€é©"]
SPECS = ["è»½é‡è¨­è¨ˆ", "è€ä¹…æ€§ç´ æ", "é«˜å“è³ªãƒ‘ãƒ¼ãƒ„", "ãŠæ‰‹å…¥ã‚Œç°¡å˜"]

def sample_words(name, kws):
    core = name.split()[0] if name else "ã‚¢ã‚¤ãƒ†ãƒ "
    w = dict(core=core, benefit=random.choice(BENEFITS),
             scene=random.choice(SCENES),
             spec=random.choice(SPECS))
    return w

def local_copy(name, kws):
    w = sample_words(name, kws)
    s = random.choice(COPY_PATTERNS).format(**w)
    return clamp_len(s, COPY_MIN, COPY_MAX)

def local_alts(name, kws, n=ALT_COUNT):
    outs = []
    for _ in range(n*2):
        w = sample_words(name, kws)
        s = random.choice(ALT_PATTERNS).format(**w)
        s = clamp_len(s, ALT_MIN, ALT_MAX)
        outs.append(s)
    return uniqueify(outs)[:n]

# ----------------------------------------------------------
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ----------------------------------------------------------
def main():
    load_dotenv()
    items = load_csv_items(INPUT_CSV)
    semantics, clusters = load_support()
    ai_indices = choose_ai_indices(items, clusters, 0.3)

    results, ai_ct, tpl_ct = [], 0, 0
    print(f"ğŸŒ¸ Hybrid AI Writer v4.3 å®Ÿè¡Œé–‹å§‹ï¼ˆå•†å“æ•°={len(items)}ï¼‰")
    for i in tqdm(range(len(items)), desc="ğŸª„ å•†å“ç”Ÿæˆä¸­", total=len(items)):
        it = items[i]
        clu = clusters[i % len(clusters)] if clusters else {"keywords":[]}
        ctx = clu.get("keywords", [])

        # ãƒ­ãƒ¼ã‚«ãƒ«ç”Ÿæˆï¼ˆAIéƒ¨åˆ†ã¯å¾Œã§OpenAIé€£æºåŒ–å¯èƒ½ï¼‰
        copy = local_copy(it["name"], ctx)
        alts = local_alts(it["name"], ctx)
        tpl_ct += 1

        results.append({
            "index": i,
            "product_name": it["name"],
            "genre": it.get("genre",""),
            "copy": copy,
            "alts": alts
        })

    ts = datetime.now().strftime("%Y%m%d_%H%M")
    out = os.path.join(OUT_DIR, f"hybrid_writer_full_{ts}.json")
    with open(out,"w",encoding="utf-8") as f:
        json.dump({"items": results}, f, ensure_ascii=False, indent=2)

    print(f"âœ… å‡ºåŠ›å®Œäº†: {out}")
    print(f"ğŸ“Š ä»¶æ•°: {len(items)}ï¼ˆAI: {ai_ct} / TPL: {tpl_ct}ï¼‰")
    print(f"ğŸ“ Copy/ALT å¹³å‡é•·: {sum(zlen(r['copy']) for r in results)/len(results):.1f} / "
          f"{sum(sum(zlen(a) for a in r['alts'])/len(r['alts']) for r in results)/len(results):.1f}")
    print(f"ğŸ” æ¬ è½ç¢ºèª: Copyæ¬ è½={sum(1 for r in results if not r['copy'])}, ALTä»¶æ•°ä¸æ­£="
          f"{sum(1 for r in results if len(r['alts'])!=ALT_COUNT)}")

if __name__ == "__main__":
    main()
import atlas_autosave_core
