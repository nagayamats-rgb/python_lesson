# -*- coding: utf-8 -*-
"""
alt_writer_v5_semantic_router.py
================================
æ¥½å¤©ALTå°‚ç”¨ï¼ˆ20æœ¬/å•†å“ï¼‰ â€” Semantic Router Readyï¼ˆâ€œè¦/ã‹ã‚“ãªã‚â€å†…è”µï¼‰

å…¥åŠ›:
  /Users/tsuyoshi/Desktop/python_lesson/sauce/rakuten.csv
    - UTF-8ã€ãƒ˜ãƒƒãƒ€ã«ã€Œå•†å“åã€å¿…é ˆ

çŸ¥è¦‹ãƒ•ã‚©ãƒ«ãƒ€:
  /Users/tsuyoshi/Desktop/python_lesson/output/semantics
    - lexical_clusters_*.json
    - market_vocab_*.json
    - structured_semantics_*.json
    - styled_persona_*.json
    - normalized_*.json
    - template_composer*.json
  â€»å­˜åœ¨ã—ãªã„/å½¢å¼ä¸æƒã„OKï¼ˆè‡ªå‹•å¸åï¼†ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰

å‡ºåŠ›:
  /Users/tsuyoshi/Desktop/python_lesson/output/ai_writer/alt_text_ai_raw_router_v5.csv
  /Users/tsuyoshi/Desktop/python_lesson/output/ai_writer/alt_text_refined_router_v5.csv
  /Users/tsuyoshi/Desktop/python_lesson/output/ai_writer/alt_text_diff_router_v5.csv

ä»•æ§˜ãƒã‚¤ãƒ©ã‚¤ãƒˆ:
- AIç”Ÿç”£: ã¾ãš1ã€œ2æ–‡ã®è‡ªç„¶æ–‡ã§ 100ã€œ130å­— / è¡Œ ã‚’ç›®æ¨™ã«20è¡Œç”Ÿæˆï¼ˆå¥ç‚¹çµ‚æ­¢ï¼‰
- ãƒ­ãƒ¼ã‚«ãƒ«æ•´å½¢: 80ã€œ110å­—ã«è‡ªç„¶ã‚«ãƒƒãƒˆã€ç¦å‰‡é©ç”¨ã€é‡è¤‡é™¤å»ã€ç®‡æ¡æ›¸ãå‰¥ãŒã—
- ç©ºè¡Œ/æ¥µçŸ­è¡Œ: ãã®å ´ã§è£œå®Œï¼ˆâ€œè‡ªç„¶ãªæ¥½å¤©ALTãƒ†ãƒ³ãƒ—ãƒ¬æœ€å°æ§‹æ–‡â€ã§è‡ªå‹•åŸ‹ã‚ï¼‰
- â€œè¦ / ã‹ã‚“ãªã‚â€: ç”Ÿæˆã®ãƒ–ãƒ¬ã‚’æŠ‘ãˆã‚‹ä¸­æ ¸è¦ç¯„ï¼ˆå£èª¿ãƒ»æ§‹æ–‡ãƒ»ç¦å‰‡ï¼‰ã‚’å¸¸æ™‚æ³¨å…¥
- Semantic Router: å•†å“åâ‡„çŸ¥è¦‹èªå½™ã®é¡ä¼¼åº¦ã§â€œãã®å•†å“ã«åŠ¹ãèªå½™/æ§‹æ–‡â€ã‚’æŠ½å‡ºæŠ•å…¥
- OpenAI: .env ã® OPENAI_MODEL / OPENAI_MODE / OPENAI_TEMPERATURE / OPENAI_MAX_TOKENS ã‚’å°Šé‡
  * æœªè¨­å®šæ™‚ã¯ model="gpt-5.1-mini" ãŒã‚ã‚Œã°ä½¿ç”¨ã€ãªã‘ã‚Œã° "gpt-4o" ã¸è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
  * response_format={"type":"text"} / max_completion_tokens=env or 1000 / æ¸©åº¦ã¯ env or 1.0

æ³¨æ„:
- ALTã¯æ¥½å¤©å°‚ç”¨ã€‚Yahooå°‚ç”¨ã®è²©ä¿ƒèª/è£…é£¾èªã¯é¿ã‘ã‚‹ã€‚
- â€œç”»åƒãƒ»å†™çœŸãƒ»å½“åº—ãƒ»ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ»ã‚¯ãƒªãƒƒã‚¯ã€œâ€ç­‰ã®ãƒ¡ã‚¿èªã¯ç¦æ­¢ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«å´ã§å¼·åˆ¶é™¤å»ï¼‰
"""

import os
import re
import csv
import glob
import json
import time
import math
from collections import defaultdict, Counter
from pathlib import Path
from dotenv import load_dotenv

# é€²æ—ãƒãƒ¼
try:
    from tqdm.auto import tqdm
except Exception:
    def tqdm(x, **k): return x

# OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
try:
    from openai import OpenAI
except Exception:
    raise SystemExit("openai SDK ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚`pip install openai python-dotenv` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 0) å›ºå®šãƒ‘ã‚¹ï¼ˆè¦ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE = Path("/Users/tsuyoshi/Desktop/python_lesson")
INPUT_CSV = BASE / "sauce" / "rakuten.csv"
SEMANTICS_DIR = BASE / "output" / "semantics"
OUT_DIR = BASE / "output" / "ai_writer"

RAW_PATH  = OUT_DIR / "alt_text_ai_raw_router_v5.csv"
REF_PATH  = OUT_DIR / "alt_text_refined_router_v5.csv"
DIFF_PATH = OUT_DIR / "alt_text_diff_router_v5.csv"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1) å®šæ•°ï¼ˆç¦å‰‡ãƒ»æ­£è¦åŒ–ãƒ»ç›®æ¨™é•·ãªã©ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ç”»åƒæå†™/ãƒ¡ã‚¿/æ¯”è¼ƒè¡¨ç¾ ç¦æ­¢èªï¼ˆâ€œè¦/ã‹ã‚“ãªã‚â€è¦ç¯„ï¼‰
FORBIDDEN_GLOBAL = [
    "ç”»åƒ", "å†™çœŸ", "è¦‹ãŸç›®", "ä¸Šã®ç”»åƒ", "ä¸‹ã®å†™çœŸ",
    "å½“åº—", "å½“ç¤¾", "ã‚·ãƒ§ãƒƒãƒ—", "ãƒ¬ãƒ“ãƒ¥ãƒ¼", "ãƒ©ãƒ³ã‚­ãƒ³ã‚°",
    "ãƒªãƒ³ã‚¯", "ã“ã¡ã‚‰", "ã‚¯ãƒªãƒƒã‚¯", "ã‚«ãƒ¼ãƒˆ", "è³¼å…¥ã¯ã“ã¡ã‚‰",
    "ç«¶åˆ", "å„ªä½æ€§", "æ¥­ç•Œæœ€é«˜", "æœ€å®‰", "No.1", "ãƒŠãƒ³ãƒãƒ¼ãƒ¯ãƒ³", "å£²ä¸ŠNo1",
    "è¿”é‡‘ä¿è¨¼", "é€æ–™ç„¡æ–™ï¼ˆç¢ºç´„ï¼‰",
]

# ç®‡æ¡æ›¸ããƒ»åˆ—æŒ™ã®é ­ã‚’å‰¥ãŒã™
LEADING_ENUM_RE = re.compile(r"^\s*[\dâ‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©\-ãƒ»\*\u2022]\s*[\.ï¼ã€]?\s*")
MULTI_COMMA_RE  = re.compile(r"ã€{3,}")
WS_RE           = re.compile(r"\s+")

# AIå‡ºåŠ›â†’ãƒ­ãƒ¼ã‚«ãƒ«æ•´å½¢ã®ç›®æ¨™ãƒ¬ãƒ³ã‚¸
RAW_MIN, RAW_MAX     = 100, 130
FINAL_MIN, FINAL_MAX =  80, 110

# â€œè¦ / ã‹ã‚“ãªã‚â€ â€” ã‚³ã‚¢è¦ç¯„ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¸¸é§ï¼‰
KANNAME_COVENANT = (
    "ã€è¦/ã‹ã‚“ãªã‚ã€‘\n"
    "ãƒ»ALTã¯â€œç”»åƒã®èª¬æ˜â€ã§ã¯ãªãã€å•†å“ã®ç‰¹å¾´ãƒ»ç”¨é€”ãƒ»å¯¾è±¡ãƒ»ä¾¿ç›Šã‚’è‡ªç„¶æ–‡ã§è¦ç´„ã™ã‚‹ã“ã¨ã€‚\n"
    "ãƒ»æ¥½å¤©ã‚µã‚¤ãƒˆå†…SEOã‚’æ„è­˜ã—ã€å¯¾å¿œæ©Ÿç¨®/å‹ç•ªãƒ»æ©Ÿèƒ½ãƒ»ç´ æãƒ»è¦æ ¼ãƒ»ä½¿ç”¨ã‚·ãƒ¼ãƒ³ãƒ»ãƒ¦ãƒ¼ã‚¶ãƒ¼åƒã‚’ç„¡ç†ãªãç¹”ã‚Šè¾¼ã‚€ã€‚\n"
    "ãƒ»å¥èª­ç‚¹ã®éå‰°ã‚„ç®‡æ¡æ›¸ãã¯é¿ã‘ã€1ã€œ2æ–‡ã®æµã‚Œã‚‹æ—¥æœ¬èªã«ã™ã‚‹ã€‚\n"
    "ãƒ»ç¦æ­¢èªï¼ˆç”»åƒ/å†™çœŸ/å½“åº—/ãƒ©ãƒ³ã‚­ãƒ³ã‚°/ã‚¯ãƒªãƒƒã‚¯ ç­‰ï¼‰ã¯ä½¿ã‚ãªã„ã€‚ç«¶åˆæ¯”è¼ƒã‚‚é¿ã‘ã‚‹ã€‚\n"
    "ãƒ»å®£ä¼èª¿ã®éåº¦ãªèª‡å¼µã¯é¿ã‘ã€å…·ä½“ã‚’é‡è¦–ã™ã‚‹ã€‚"
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2) ç’°å¢ƒ & OpenAI åˆæœŸåŒ–ï¼ˆ.env å›ºå®š/ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def init_env_and_client():
    load_dotenv(override=True)
    api_key = (os.getenv("OPENAI_API_KEY") or "").strip()
    if not api_key:
        raise SystemExit("OPENAI_API_KEY ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚.env ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    # ãƒ¢ãƒ‡ãƒ«/ãƒ¢ãƒ¼ãƒ‰/æ¸©åº¦/ãƒˆãƒ¼ã‚¯ãƒ³: .env ã‚’å°Šé‡ã—ã¤ã¤å®‰å…¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    model = (os.getenv("OPENAI_MODEL") or "").strip() or "gpt-5.1-mini"
    if model.lower() in {"gpt-5-nano", "gpt-5.1-nano"}:
        # nanoã¯å‡ºåŠ›ãŒçŸ­æ–‡åŒ–ã—ãŒã¡ â†’ miniç³»ã‚’æ¨å¥¨
        model = "gpt-5.1-mini"
    # ä¸‡ä¸€gpt-5ç³»ãŒæœªé–‹é€šãªã‚‰ 4o ã«åˆ‡æ›¿ï¼ˆå®Ÿè¡Œæ™‚ã‚¨ãƒ©ãƒ¼ã‚’é¿ã‘ã‚‹ï¼‰
    fallback_model = os.getenv("OPENAI_FALLBACK_MODEL", "gpt-4o").strip()
    mode = (os.getenv("OPENAI_MODE") or "chat").strip()
    temperature = float(os.getenv("OPENAI_TEMPERATURE") or "1.0")
    max_tokens  = int(os.getenv("OPENAI_MAX_TOKENS") or "1000")

    client = OpenAI(api_key=api_key)
    return client, model, fallback_model, mode, temperature, max_tokens

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3) å…¥åŠ›ï¼ˆå•†å“åï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_products(path: Path):
    if not path.exists():
        raise SystemExit(f"å…¥åŠ›CSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path}")
    products = []
    with path.open("r", encoding="utf-8", newline="") as f:
        reader = csv.DictReader(f)
        if "å•†å“å" not in reader.fieldnames:
            raise SystemExit("å…¥åŠ›CSVã«ã€å•†å“åã€ãƒ˜ãƒƒãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        for r in reader:
            nm = (r.get("å•†å“å") or "").strip()
            if nm:
                products.append(nm)
    # é †åºã‚’ä¿ã£ãŸé‡è¤‡é™¤å»
    seen, uniq = set(), []
    for nm in products:
        if nm not in seen:
            uniq.append(nm); seen.add(nm)
    return uniq

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4) çŸ¥è¦‹ã®èª­è¾¼ & Semantic Router
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def safe_load_json(p: Path):
    try:
        with p.open("r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None

def list_semantic_files():
    if not SEMANTICS_DIR.is_dir():
        return {}
    def latest(pattern):
        files = list(SEMANTICS_DIR.glob(pattern))
        return sorted(files, key=lambda x: x.stat().st_mtime, reverse=True)

    return {
        "lexical": latest("lexical_clusters_*.json"),
        "market": latest("market_vocab_*.json"),
        "semantic": latest("structured_semantics_*.json"),
        "persona": latest("styled_persona_*.json"),
        "normalized": latest("normalized_*.json"),
        "template": latest("template_composer*.json"),
    }

def flatten_terms(data):
    out = []
    if isinstance(data, dict):
        for v in data.values():
            out.extend(flatten_terms(v))
    elif isinstance(data, list):
        for v in data:
            out.extend(flatten_terms(v))
    elif isinstance(data, str):
        out.append(data)
    return out

def tokenize(s: str):
    # ç°¡æ˜“ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼šå…¨è§’â†’åŠè§’ã®ä¸€éƒ¨ã€éæ–‡å­—é™¤å»ã€ç©ºç™½split
    s2 = re.sub(r"[^\w\dã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥\-ï¼‹+/\.ï¼…%ãœmmcmCMxX ]+", " ", s)
    s2 = WS_RE.sub(" ", s2).strip()
    return [t for t in s2.split(" ") if t]

def jaccard(a: set, b: set):
    if not a or not b: return 0.0
    return len(a & b) / len(a | b)

def semantic_router(product: str, all_buckets: dict, top_k=24):
    """
    å•†å“åã®ãƒˆãƒ¼ã‚¯ãƒ³é›†åˆã¨çŸ¥è¦‹èªå½™ã®é›†åˆã® Jaccard é¡ä¼¼ã§ç²—ãã‚¹ã‚³ã‚¢ â†’ ä¸Šä½æŠ½å‡º
    â€»ç²¾ç·»ã§ãªãã¦OKã€‚å®‰å®šãƒ»é«˜é€Ÿãƒ»å†ç¾æ€§é‡è¦–ã€‚
    """
    p_tokens = set(tokenize(product))
    scored = []
    # ãã‚Œãã‚Œã®ãƒã‚±ãƒ„ã‹ã‚‰èªå½™ã‚’åé›†
    for bucket_name, terms in all_buckets.items():
        for t in terms:
            t_tokens = set(tokenize(t))
            score = jaccard(p_tokens, t_tokens)
            if score > 0:
                scored.append((t, score))
    # ä¸Šä½æŠ½å‡ºï¼ˆé‡è¤‡èªã¯ã‚¹ã‚³ã‚¢æœ€å¤§ã®ã¿ï¼‰
    score_map = {}
    for t, sc in scored:
        if t not in score_map or sc > score_map[t]:
            score_map[t] = sc
    top_terms = [t for t, _ in sorted(score_map.items(), key=lambda x: x[1], reverse=True)[:top_k]]
    return top_terms

def load_knowledge_for_router():
    files = list_semantic_files()
    buckets = defaultdict(list)
    forb_local = set()

    # lexical clusters
    for p in files.get("lexical", []):
        data = safe_load_json(p)
        if not data: continue
        if isinstance(data, dict) and "clusters" in data:
            for c in (data.get("clusters") or []):
                terms = c.get("terms") or []
                buckets["lexical"].extend([t for t in terms if isinstance(t, str)])
        else:
            buckets["lexical"].extend([t for t in flatten_terms(data)])

    # market vocab
    for p in files.get("market", []):
        data = safe_load_json(p)
        if not data: continue
        if isinstance(data, list):
            for v in data:
                if isinstance(v, dict) and isinstance(v.get("vocabulary"), str):
                    buckets["market"].append(v["vocabulary"])
                elif isinstance(v, str):
                    buckets["market"].append(v)
        elif isinstance(data, dict):
            arr = data.get("vocabulary") or data.get("vocab") or []
            if isinstance(arr, list):
                buckets["market"].extend([x for x in arr if isinstance(x, str)])

    # structured semantics
    for p in files.get("semantic", []):
        data = safe_load_json(p)
        if not data: continue
        # æƒ³å®š: {"concepts":[...], "scenes":[...], "targets":[...], "use_cases":[...], "features":[...], "benefits":[...]}
        if isinstance(data, dict):
            for k in ("concepts","scenes","targets","use_cases","features","benefits","semantics"):
                arr = data.get(k) or []
                buckets["semantic"].extend([x for x in arr if isinstance(x, str)])
        else:
            buckets["semantic"].extend([t for t in flatten_terms(data)])

    # personaï¼ˆå£èª¿/ãƒ¬ã‚¸ã‚¹ã‚¿ãƒ¼ï¼‰
    for p in files.get("persona", []):
        data = safe_load_json(p)
        if not data: continue
        if isinstance(data, dict):
            tone = data.get("tone") or {}
            if isinstance(tone, dict):
                for v in tone.values():
                    if isinstance(v, str): buckets["persona"].append(v)
        buckets["persona"].extend([t for t in flatten_terms(data)])

    # normalizedï¼ˆç¦å‰‡ï¼‰
    for p in files.get("normalized", []):
        data = safe_load_json(p)
        if not data: continue
        if isinstance(data, dict):
            fw = data.get("forbidden_words") or []
            for w in fw:
                if isinstance(w, str): forb_local.add(w)

    # template composerï¼ˆéª¨å­/å‹ãƒ’ãƒ³ãƒˆï¼‰
    for p in files.get("template", []):
        data = safe_load_json(p)
        if not data: continue
        if isinstance(data, dict):
            hints = data.get("hints") or data.get("templates") or []
            buckets["template"].extend([h for h in hints if isinstance(h, str)])
        else:
            buckets["template"].extend([t for t in flatten_terms(data)])

    return buckets, sorted(set(list(FORBIDDEN_GLOBAL) + list(forb_local)))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5) ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SYSTEM_PROMPT = (
    "ã‚ãªãŸã¯ECã‚µã‚¤ãƒˆï¼ˆæ¥½å¤©ï¼‰ã®ALTãƒ†ã‚­ã‚¹ãƒˆå°‚é–€ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚\n"
    + KANNAME_COVENANT +
    "\nä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã‚’å³å®ˆã—ã¦20è¡Œã®è‡ªç„¶æ–‡ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„:\n"
    f"ãƒ»å„è¡Œã¯1ã€œ2æ–‡ã€å…¨è§’ãŠã‚ˆã{RAW_MIN}ã€œ{RAW_MAX}æ–‡å­—ã€‚\n"
    "ãƒ»å¿…ãšå¥ç‚¹ã€Œã€‚ã€ã§çµ‚ãˆã‚‹ã€‚ç®‡æ¡æ›¸ãã‚„ç•ªå·ï¼ˆ1. ãƒ» - ãªã©ï¼‰ã‚„ãƒ©ãƒ™ãƒ«ã¯ä»˜ã‘ãªã„ã€‚\n"
    "ãƒ»å‡ºåŠ›ã¯ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ï¼ˆJSON/è¨˜å·/æ ãªã—ï¼‰ã€‚\n"
)

def build_user_prompt(product: str, top_terms: list, forbidden_words: list):
    # ãƒ«ãƒ¼ã‚¿ãŒæ¸¡ã™â€œã“ã®å•†å“ã«åŠ¹ãèªå½™/éª¨å­â€
    router_hint = "ã€".join(top_terms[:30]) if top_terms else ""
    forbid_txt  = "ã€".join(sorted(set(forbidden_words)))
    # æ§‹æˆãƒ’ãƒ³ãƒˆï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ã§ã¯ãªãè‡ªç„¶ã«ï¼‰
    structure = "å•†å“ã‚¹ãƒšãƒƒã‚¯â†’ã‚³ã‚¢ã‚³ãƒ³ãƒ”ã‚¿ãƒ³ã‚¹â†’ã©ã‚“ãªäººâ†’åˆ©ç”¨ã‚·ãƒ¼ãƒ³â†’ä¾¿ç›Šï¼ˆè‡ªç„¶ãªæ—¥æœ¬èªã€è©°ã‚è¾¼ã¿ã™ããªã„ï¼‰"
    return (
        f"å•†å“å: {product}\n"
        f"çŸ¥è¦‹ãƒ’ãƒ³ãƒˆ: {router_hint}\n"
        f"æ§‹æˆãƒ’ãƒ³ãƒˆ: {structure}\n"
        f"ç¦æ­¢èª: {forbid_txt}\n"
        "å‡ºåŠ›: 20è¡Œã®è‡ªç„¶æ–‡ï¼ˆå„è¡Œ1ã€œ2æ–‡ï¼‰ã€‚å¥ç‚¹ã§çµ‚ãˆã‚‹ã€‚"
    )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6) OpenAI å‘¼ã³å‡ºã—ï¼ˆå …ç‰¢ãƒ»ãƒãƒƒã‚¯ã‚ªãƒ•ä»˜ãï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def call_openai_lines(client, model, fallback_model, mode, temperature, max_tokens, system_prompt, user_prompt, retry=4, wait=6):
    last_err = None
    use_model = model
    for attempt in range(retry):
        try:
            res = client.chat.completions.create(
                model=use_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user",   "content": user_prompt},
                ],
                response_format={"type": "text"},
                max_completion_tokens=max_tokens,
                temperature=temperature,
            )
            content = (res.choices[0].message.content or "").strip()
            if not content:
                raise RuntimeError("Empty content from OpenAI.")
            return content
        except Exception as e:
            last_err = e
            # 429 / ä¸€éƒ¨ã®ä¸æ˜ã‚¨ãƒ©ãƒ¼ã¯å¾…æ©Ÿâ†’ãƒªãƒˆãƒ©ã‚¤
            err_msg = str(e)
            if "insufficient_quota" in err_msg or "429" in err_msg:
                time.sleep(wait * (attempt + 1))
            elif "model_not_found" in err_msg or "does not exist" in err_msg:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                use_model = fallback_model
                time.sleep(2)
            else:
                time.sleep(wait)
    raise RuntimeError(f"OpenAIå¿œç­”å–å¾—ã«å¤±æ•—: {last_err}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 7) ãƒ­ãƒ¼ã‚«ãƒ«æ•´å½¢ï¼ˆè‡ªç„¶ã‚«ãƒƒãƒˆ/ç¦å‰‡/é‡è¤‡/è£œå®Œï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def soft_clip_sentence(text: str, min_len=FINAL_MIN, max_len=FINAL_MAX) -> str:
    t = (text or "").strip()
    if not t: return ""
    # åˆ—æŒ™é ­é™¤å»
    t = LEADING_ENUM_RE.sub("", t).strip("ãƒ»-â€”â—ã€€")
    # ä½™åˆ†ã‚¹ãƒšãƒ¼ã‚¹åœ§ç¸®ãƒ»èª­ç‚¹é€£ç¶šèª¿æ•´
    t = WS_RE.sub(" ", t)
    t = MULTI_COMMA_RE.sub("ã€ã€", t)
    # å¥ç‚¹çµ‚æ­¢
    if not t.endswith("ã€‚"):
        t += "ã€‚"
    # é•·ã™ãã‚‹ â†’ 120ã¾ã§è¨±å®¹ã€è¿‘ã„å¥ç‚¹ã¾ã§å‰è©°ã‚
    if len(t) > 120:
        cut = t[:120]
        p = cut.rfind("ã€‚")
        t = cut if p == -1 else cut[:p+1]
    # ç¦æ­¢èªã®å®Œå…¨é™¤å»ï¼ˆç—•è·¡ãŒå‡ºãªã„ã‚ˆã†æ–‡å­—åˆ—ç½®æ›ï¼‰
    for ng in FORBIDDEN_GLOBAL:
        if ng and ng in t:
            t = t.replace(ng, "")
    return t.strip()

def refine_lines(raw_lines):
    # å¥ç‚¹çµ‚æ­¢ & è‡ªç„¶ã‚«ãƒƒãƒˆ
    norm = []
    for ln in raw_lines:
        if not ln: continue
        ln = soft_clip_sentence(ln)
        if len(ln) < 15:
            continue
        norm.append(ln)

    # é‡è¤‡é™¤å»ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰
    uniq = []
    seen = set()
    for ln in norm:
        if ln not in seen:
            uniq.append(ln); seen.add(ln)

    # 20æœ¬ã«èª¿æ•´
    def fill_line(seed: str) -> str:
        # èªå°¾ã‚†ã‚‹å¤‰ï¼ˆä½“è¨€æ­¢ã‚æ··åœ¨ã‚’è¨±ã™ï¼‰
        s = seed
        s = s.replace("ã—ã¾ã™ã€‚", "ã§ã™ã€‚")
        s = s.replace("ã§ãã¾ã™ã€‚", "ã—ã‚„ã™ã„ã§ã™ã€‚")
        if not s.endswith("ã€‚"):
            s += "ã€‚"
        return soft_clip_sentence(s)

    i = 0
    while len(uniq) < 20 and uniq:
        uniq.append(fill_line(uniq[i % len(uniq)]))
        i += 1

    return uniq[:20]

def sanitize_model_bullets(text: str):
    """
    OpenAIãŒåˆ—æŒ™ã—ã¦ããŸå ´åˆã«å‚™ãˆã€è¡Œã”ã¨ã«æ•´å½¢ã—ã‚„ã™ã„å½¢ã¸ã€‚
    """
    lines = [LEADING_ENUM_RE.sub("", ln).strip("ãƒ»-â€”â—ã€€") for ln in text.split("\n") if ln.strip()]
    # å…ˆé ­60æœ¬ã¾ã§ä¿æŒï¼ˆéå‰°ç”Ÿæˆå¯¾ç­–ï¼‰
    return lines[:60]

def minimal_fallback(product: str):
    # ç©ºè¡Œè£œå®Œãƒ†ãƒ³ãƒ—ãƒ¬ï¼ˆæœ€å°æ§‹æ–‡ï¼‰
    return f"{product} ã®ä½¿ã„å‹æ‰‹ã‚’é«˜ã‚ã€æ—¥å¸¸ã®ä¸ä¾¿ã‚’è§£æ¶ˆã™ã‚‹è¨­è¨ˆã§ã™ã€‚"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 8) æ›¸ãå‡ºã—
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ensure_outdir():
    OUT_DIR.mkdir(parents=True, exist_ok=True)

def write_raw(products, all_raw):
    with RAW_PATH.open("w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["å•†å“å"] + [f"ALT_raw_{i+1}" for i in range(20)])
        for p, lines in zip(products, all_raw):
            row = [p] + (lines[:20] + [""] * max(0, 20 - len(lines)))
            w.writerow(row)

def write_refined(products, all_refined):
    with REF_PATH.open("w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["å•†å“å"] + [f"ALT_{i+1}" for i in range(20)])
        for p, lines in zip(products, all_refined):
            row = [p] + (lines[:20] + [""] * max(0, 20 - len(lines)))
            w.writerow(row)

def write_diff(products, all_raw, all_refined):
    with DIFF_PATH.open("w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        header = ["å•†å“å"] + [f"ALT_raw_{i+1}" for i in range(20)] + [f"ALT_refined_{i+1}" for i in range(20)]
        w.writerow(header)
        for p, r, ref in zip(products, all_raw, all_refined):
            r_line   = (r[:20]   + [""] * max(0, 20 - len(r)))
            ref_line = (ref[:20] + [""] * max(0, 20 - len(ref)))
            w.writerow([p] + r_line + ref_line)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 9) ãƒ¡ã‚¤ãƒ³
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    print("ğŸŒ¸ ALTãƒ©ã‚¤ã‚¿ãƒ¼ v5.0ï¼ˆSemantic Router Ready + â€œè¦/ã‹ã‚“ãªã‚â€ï¼‰")
    client, model, fallback_model, mode, temperature, max_tokens = init_env_and_client()
    ensure_outdir()

    products = load_products(INPUT_CSV)
    print(f"âœ… å¯¾è±¡å•†å“: {len(products)}ä»¶")

    # çŸ¥è¦‹ã®å¸åï¼ˆæŸ”è»Ÿï¼‰â†’ Semantic Router ã§å•†å“ã”ã¨ã®ãƒˆãƒƒãƒ—èªå½™æŠ½å‡º
    buckets, forbidden_all = load_knowledge_for_router()

    all_raw, all_refined = [], []

    for product in tqdm(products, desc="ğŸ§  ç”Ÿæˆä¸­", total=len(products)):
        try:
            top_terms = semantic_router(product, buckets, top_k=28)
            user_prompt = build_user_prompt(product, top_terms, forbidden_all)
            content = call_openai_lines(
                client, model, fallback_model, mode, temperature, max_tokens,
                SYSTEM_PROMPT, user_prompt, retry=4, wait=6
            )
            raw_lines = sanitize_model_bullets(content)

            # ç©º/çŸ­æ–‡ã‚’æœ€å°æ§‹æ–‡ã§è£œå¡«ï¼ˆ20è¡Œç¢ºä¿ã®ãŸã‚ã®å®‰å…¨å¼ï¼‰
            if len(raw_lines) < 20:
                raw_lines += [minimal_fallback(product) for _ in range(20 - len(raw_lines))]

        except Exception:
            # OpenAIå…¨æ»…æ™‚ã‚‚æ¬ ç•ªãªã—ã«é€²ã‚ã‚‹
            raw_lines = [minimal_fallback(product)] * 20

        refined = refine_lines(raw_lines)

        # è¡Œé ­/è¡Œæœ«ã®ã‚´ãƒŸé™¤å»ãƒ»ä½“è¨€æ­¢ã‚æ··åœ¨è¨±å®¹
        refined = [ln.strip(" ãƒ»-â€”â—") for ln in refined]

        all_raw.append(raw_lines[:20])
        all_refined.append(refined[:20])

        # è»½ã„ã‚¹ãƒ­ãƒƒãƒˆãƒªãƒ³ã‚°ï¼ˆ429ç·©å’Œï¼‰
        time.sleep(0.2)

    # æ›¸ãå‡ºã—
    write_raw(products, all_raw)
    write_refined(products, all_refined)
    write_diff(products, all_raw, all_refined)

    # ã–ã£ãã‚Šçµ±è¨ˆ
    def avg_len(blocks):
        lens = [len(x) for lines in blocks for x in lines if x]
        return (sum(lens) / max(1, len(lens)))

    print("âœ… å‡ºåŠ›å®Œäº†:")
    print(f"   - AIç”Ÿå‡ºåŠ› : {RAW_PATH}")
    print(f"   - æ•´å½¢å¾Œ   : {REF_PATH}")
    print(f"   - å·®åˆ†æ¯”è¼ƒ : {DIFF_PATH}")
    print(f"ğŸ“ æ–‡å­—æ•°(å¹³å‡): raw={avg_len(all_raw):.1f} / refined={avg_len(all_refined):.1f}")
    print("ğŸ”’ ä»•æ§˜ãƒ¡ãƒ¢:")
    print("   - â€œè¦/ã‹ã‚“ãªã‚â€å¸¸é§ã€ç¦å‰‡å¼·åŒ–ã€è‡ªç„¶æ–‡1ã€œ2æ–‡ã€å¥ç‚¹çµ‚æ­¢ã€æ¥½å¤©ALTç‰¹åŒ–")
    print(f"   - AIç›®æ¨™ {RAW_MIN}ã€œ{RAW_MAX}å­— â†’ ãƒ­ãƒ¼ã‚«ãƒ«æ•´å½¢ {FINAL_MIN}ã€œ{FINAL_MAX}å­—")

if __name__ == "__main__":
    main()
import atlas_autosave_core
