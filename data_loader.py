"""
ğŸŒ¸ KOTOHA ENGINE v1.2 - data_loader.py
-----------------------------------------
äººã¨ AI ã®å£æ ¹ã‚’ãªãã™ç¬¬ä¸€æ­©ã€‚
æœ¬ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€ã‚ãªãŸã®CSVæ§‹é€ ã«å®Œå…¨å¯¾å¿œã—ãŸãƒ‡ãƒ¼ã‚¿å…¥åŠ›å±¤ã§ã™ã€‚

- .env ã®å®‰å…¨èª­è¾¼ï¼ˆAPIã‚­ãƒ¼æ¤œè¨¼ï¼‰
- CSV (Shift_JIS) ã®å®‰å…¨èª­è¾¼ã¨ã‚«ãƒ©ãƒãƒªå‡¦ç†
- å¯¾è±¡åˆ—: å•†å“å / ã‚¸ãƒ£ãƒ³ãƒ«ID / ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼ / å•†å“ç”»åƒåï¼ˆALTï¼‰1ã€œ20
- æ›¸ãæˆ»ã—: ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼ï¼‹ALTç¾¤ã®ã¿ä¸Šæ›¸ãä¿å­˜ï¼ˆä»–åˆ—ã¯éç ´å£Šï¼‰
"""

import os
import pandas as pd
from dotenv import load_dotenv
import logging

# -----------------------------------
# ğŸŒ¸ ãƒ­ã‚¬ãƒ¼è¨­å®š
# -----------------------------------
logger = logging.getLogger("KOTOHA_ENGINE")
if not logger.handlers:
    handler = logging.StreamHandler()
    fmt = logging.Formatter("[%(asctime)s] %(levelname)s: %(message)s")
    handler.setFormatter(fmt)
    logger.addHandler(handler)
logger.setLevel(logging.INFO)

# -----------------------------------
# âœ… å¯¾è±¡åˆ—ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆã‚ãªãŸã®CSVæ§‹é€ ã«å®Œå…¨æº–æ‹ ï¼‰
# -----------------------------------
TARGET_COLUMNS = {
    "name_col": "å•†å“å",
    "genre_col": "ã‚¸ãƒ£ãƒ³ãƒ«ID",
    "copy_col": "ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼",
    "alt_cols": [f"å•†å“ç”»åƒåï¼ˆALTï¼‰{i}" for i in range(1, 21)]
}

# -----------------------------------
# âœ… .env èª­ã¿è¾¼ã¿
# -----------------------------------
def load_env_config(env_path: str = ".env.txt") -> dict:
    if not os.path.exists(env_path):
        logger.warning(f"âš ï¸ .env ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {env_path}")
        return {}
    load_dotenv(env_path)
    keys = [
        "RAKUTEN_API_BASE_URL", "RAKUTEN_APP_ID",
        "YAHOO_API_BASE_URL", "YAHOO_APP_ID",
        "OPENAI_API_BASE_URL", "OPENAI_API_KEY"
    ]
    cfg = {}
    missing = []
    for k in keys:
        v = os.getenv(k)
        if v and v.strip():
            cfg[k] = v.strip()
        else:
            missing.append(k)
    if missing:
        logger.warning(f"âš ï¸ æœªè¨­å®šã‚­ãƒ¼: {', '.join(missing)}")
    else:
        logger.info("âœ… .env èª­ã¿è¾¼ã¿æˆåŠŸ")
    return cfg

# -----------------------------------
# âœ… CSV èª­ã¿è¾¼ã¿ï¼ˆShift JIS / ã‚«ãƒ©ãƒãƒªå¯¾å¿œï¼‰
# -----------------------------------
def load_product_core_columns(path: str = "input.csv", encoding: str = "cp932") -> pd.DataFrame:
    """å•†å“åãƒ»ã‚¸ãƒ£ãƒ³ãƒ«IDãƒ»ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼ãƒ»ALTç¾¤ã ã‘ã‚’å®‰å…¨ã«èª­ã¿è¾¼ã‚€"""
    if not os.path.exists(path):
        raise FileNotFoundError(f"âŒ CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path}")

    try:
        df = pd.read_csv(path, encoding=encoding, dtype=str)
    except UnicodeDecodeError:
        logger.warning("âš ï¸ cp932ã§å¤±æ•—ã€utf-8-sigã§å†è©¦è¡Œã—ã¾ã™")
        df = pd.read_csv(path, encoding="utf-8-sig", dtype=str)

    all_cols = [TARGET_COLUMNS["name_col"], TARGET_COLUMNS["genre_col"], TARGET_COLUMNS["copy_col"]] + TARGET_COLUMNS["alt_cols"]

    # å­˜åœ¨ã—ãªã„åˆ—ã¯ç©ºã§è£œå®Œ
    for col in all_cols:
        if col not in df.columns:
            logger.warning(f"âš ï¸ åˆ— '{col}' ãŒCSVã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚ç©ºåˆ—ã‚’ç”Ÿæˆã—ã¾ã™ã€‚")
            df[col] = ""

    df = df[all_cols].fillna("")

    # ã‚«ãƒ©ãƒãƒªå‡¦ç†
    last_name = None
    for i, name in enumerate(df[TARGET_COLUMNS["name_col"]]):
        if not str(name).strip() and last_name:
            df.at[i, TARGET_COLUMNS["name_col"]] = last_name
        elif str(name).strip():
            last_name = str(name).strip()

    logger.info(f"âœ… CSVèª­è¾¼å®Œäº†: shape={df.shape}")
    return df

# -----------------------------------
# âœ… ã‚¸ãƒ£ãƒ³ãƒ«é¡æ¨
# -----------------------------------
def infer_genre_map(df: pd.DataFrame) -> dict:
    mapping = {}
    for _, row in df.iterrows():
        name = str(row[TARGET_COLUMNS["name_col"]]).strip()
        gid = str(row[TARGET_COLUMNS["genre_col"]]).strip()
        if name and gid and name not in mapping:
            mapping[name] = gid
    logger.info(f"ğŸ§­ ã‚¸ãƒ£ãƒ³ãƒ«IDé¡æ¨ãƒãƒƒãƒ—ç”Ÿæˆ: {len(mapping)} ä»¶")
    return mapping

# -----------------------------------
# âœ… æ›¸ãæˆ»ã—å‡¦ç†
# -----------------------------------
def save_generated_fields(df, path="input.csv", encoding="cp932"):
    """ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼ã¨ALTåˆ—ã®ã¿ä¸Šæ›¸ãä¿å­˜ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆï¼‰"""
    backup = path.replace(".csv", "_backup.csv")
    if not os.path.exists(path):
        raise FileNotFoundError(f"âŒ ä¿å­˜å…ˆCSVãŒå­˜åœ¨ã—ã¾ã›ã‚“: {path}")

    original = pd.read_csv(path, encoding=encoding, dtype=str)
    for col in [TARGET_COLUMNS["copy_col"]] + TARGET_COLUMNS["alt_cols"]:
        if col in df.columns and col in original.columns:
            original[col] = df[col]
        else:
            logger.warning(f"âš ï¸ åˆ— '{col}' ã¯ä¸Šæ›¸ãå¯¾è±¡ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")

    original.to_csv(backup, encoding=encoding, index=False, errors="ignore")
    original.to_csv(path, encoding=encoding, index=False, errors="ignore")
    logger.info(f"ğŸ’¾ CSVæ›´æ–°å®Œäº†ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆæ¸ˆã¿ï¼‰: {path}")

# -----------------------------------
# âœ… å®Ÿè¡Œãƒ†ã‚¹ãƒˆ
# -----------------------------------
if __name__ == "__main__":
    logger.info("ğŸŒ¸ KOTOHA ENGINE èµ·å‹• â€” äººã¨AIã®å£æ ¹ã‚’ãªãã™ç¬¬ä¸€æ­©")
    cfg = load_env_config()
    df = load_product_core_columns("input.csv")
    genre_map = infer_genre_map(df)
    df.to_csv("structured_preview.csv", encoding="utf-8-sig", index=False)
    logger.info("ğŸ’¾ structured_preview.csv ã‚’å‡ºåŠ›ã—ã¾ã—ãŸã€‚")
import atlas_autosave_core
