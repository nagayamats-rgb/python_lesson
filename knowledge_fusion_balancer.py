# -*- coding: utf-8 -*-
"""
knowledge_fusion_balancer.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ç›®çš„ï¼š
  /output/semantics é…ä¸‹ã®å„çŸ¥è¦‹JSONã‚’èª­ã¿è¾¼ã¿ã€
  èªå½™é‡ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚ŠãªãŒã‚‰ã€Œæ§‹é€ çš„è‡ªç„¶æ–‡ã€ã‚’ç”Ÿæˆã™ã‚‹ã€‚

å‡ºåŠ›ï¼š
  ./output/semantics/knowledge_fused_structured.json

ç‰¹å¾´ï¼š
  - lexical, market, semantics, template, persona ã®5ã‚«ãƒ†ã‚´ãƒªã‚’çµ±åˆ
  - å„ã‚«ãƒ†ã‚´ãƒªã«é‡ã¿ã‚’ä»˜ã‘ã¦è‡ªç„¶æ–‡æ§‹æˆ
  - forbidden_words ã‚‚é›†ç´„
  - v5ç³»ãƒ©ã‚¤ã‚¿ãƒ¼ã§ç›´æ¥ä½¿ç”¨å¯èƒ½ï¼ˆknowledge_text, forbidden_wordsï¼‰
"""

import os, json, glob, random

BASE_DIR = os.path.dirname(__file__)
SEM_DIR = os.path.join(BASE_DIR, "output", "semantics")
OUT_PATH = os.path.join(SEM_DIR, "knowledge_fused_structured.json")

# =====================
# é‡ã¿ä»˜ã‘ï¼ˆè‡ªç„¶æ–‡å¿—å‘ï¼‰
# =====================
WEIGHTS = {
    "lexical": 0.20,
    "market": 0.20,
    "semantic": 0.25,
    "template": 0.25,
    "persona": 0.10,
}

# =====================
# ãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡ºã¨åˆ†é¡
# =====================
def detect_jsons():
    files = glob.glob(os.path.join(SEM_DIR, "*.json"))
    mapping = {"lexical": [], "market": [], "semantic": [], "template": [], "persona": [], "normalized": []}
    for f in files:
        name = os.path.basename(f).lower()
        if "lexical" in name: mapping["lexical"].append(f)
        elif "market" in name: mapping["market"].append(f)
        elif "semantic" in name: mapping["semantic"].append(f)
        elif "template" in name: mapping["template"].append(f)
        elif "persona" in name: mapping["persona"].append(f)
        elif "normal" in name: mapping["normalized"].append(f)
    return mapping

# =====================
# JSON èª­è¾¼ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =====================
def load_json_safe(path):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None

def flatten_terms(data):
    """æ§‹é€ ãŒã©ã‚“ãªå½¢ã§ã‚‚å˜èªåˆ—ã‚’ã‚†ã‚‹ãæŠ½å‡º"""
    if isinstance(data, dict):
        vals = []
        for v in data.values():
            vals.extend(flatten_terms(v))
        return vals
    elif isinstance(data, list):
        vals = []
        for x in data:
            vals.extend(flatten_terms(x))
        return vals
    elif isinstance(data, str):
        return [data.strip()]
    else:
        return []

# =====================
# è‡ªç„¶æ–‡ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç¾¤
# =====================
TEMPLATES = [
    "ã“ã®ã‚«ãƒ†ã‚´ãƒªã§ã¯{0}ã‚„{1}ãŒä¸­å¿ƒã§ã€{2}ãªã©ã«å¯¾å¿œã—ã¾ã™ã€‚",
    "{0}ã‚’æ­è¼‰ã—ã€{1}ã‚·ãƒ¼ãƒ³ã§æ´»èºã™ã‚‹è¨­è¨ˆã§ã™ã€‚",
    "{0}ã«é©ã—ãŸæ§‹é€ ã§ã€{1}å‘ã‘ã«é–‹ç™ºã•ã‚Œã¦ã„ã¾ã™ã€‚",
    "æ—¥å¸¸åˆ©ç”¨ã«åŠ ãˆã€{0}ã‚„{1}ãªã©å¤šæ§˜ãªå ´é¢ã§æ´»ç”¨ã§ãã¾ã™ã€‚",
    "å…¨ä½“ã¨ã—ã¦{0}ã¨{1}ã‚’ä¸¡ç«‹ã—ã€{2}ãªå°è±¡ã‚’ä¸ãˆã¾ã™ã€‚",
    "{0}ã‚„{1}ã‚’å‚™ãˆãŸå®Ÿç”¨çš„ãªãƒ‡ã‚¶ã‚¤ãƒ³ãŒç‰¹å¾´ã§ã™ã€‚",
    "é«˜ã„{0}ã¨{1}ã‚’å…¼ã­å‚™ãˆã€{2}ã«ã‚‚å¯¾å¿œã™ã‚‹æ§‹æˆã§ã™ã€‚",
    "{0}ã‚’æ„è­˜ã—ãŸè¨­è¨ˆã§ã€{1}ã‚„{2}ã§ã‚‚å¿«é©ã«ä½¿ç”¨ã§ãã¾ã™ã€‚"
]

# =====================
# è‡ªç„¶æ–‡ç”Ÿæˆã‚³ã‚¢
# =====================
def build_sentence(terms):
    """èªå½™ç¾¤ã‹ã‚‰è‡ªç„¶æ–‡ã‚’ç”Ÿæˆ"""
    if not terms:
        return ""
    # ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦è‡ªç„¶æ§‹æˆ
    t = random.sample(terms, min(len(terms), 5))
    t += ["å¤šæ©Ÿèƒ½", "ãƒ‡ã‚¶ã‚¤ãƒ³æ€§", "å®Ÿç”¨æ€§", "å¿«é©ã•"]
    temp = random.choice(TEMPLATES)
    try:
        return temp.format(*t[:temp.count("{")])
    except Exception:
        return "ã€".join(t[:5]) + "ã®ç‰¹å¾´ã‚’å‚™ãˆã¦ã„ã¾ã™ã€‚"

# =====================
# çŸ¥è¦‹çµ±åˆãƒ­ã‚¸ãƒƒã‚¯
# =====================
def fuse_knowledge(mapping):
    combined = []
    for key, files in mapping.items():
        if key == "normalized":
            continue
        all_terms = []
        for f in files:
            data = load_json_safe(f)
            if data:
                all_terms += flatten_terms(data)
        if not all_terms:
            continue

        weight = WEIGHTS.get(key, 0.1)
        n_sent = max(1, int(weight * 10))  # é‡ã¿ã«å¿œã˜ã¦æ–‡æ•°
        for _ in range(n_sent):
            sentence = build_sentence(all_terms)
            if sentence and sentence not in combined:
                combined.append(sentence)

    return combined

# =====================
# ç¦å‰‡èªæŠ½å‡º
# =====================
def collect_forbidden(mapping):
    words = []
    for f in mapping.get("normalized", []):
        data = load_json_safe(f)
        if isinstance(data, dict):
            words += data.get("forbidden_words", [])
    return sorted(list(set(words)))

# =====================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# =====================
def main():
    print("ğŸ§© KOTOHAçŸ¥è¦‹ãƒãƒ©ãƒ³ã‚µãƒ¼èµ·å‹•ä¸­â€¦")
    mapping = detect_jsons()
    knowledge_lines = fuse_knowledge(mapping)
    forbidden_words = collect_forbidden(mapping)

    result = {
        "knowledge_text": knowledge_lines,
        "forbidden_words": forbidden_words
    }

    os.makedirs(SEM_DIR, exist_ok=True)
    with open(OUT_PATH, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"âœ… å‡ºåŠ›å®Œäº†: {OUT_PATH}")
    print(f"ğŸ“˜ çŸ¥è¦‹æ–‡æ•°: {len(knowledge_lines)} / ç¦å‰‡èªæ•°: {len(forbidden_words)}")

if __name__ == "__main__":
    main()
import atlas_autosave_core
