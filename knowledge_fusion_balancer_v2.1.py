# -*- coding: utf-8 -*-
"""
KOTOHAçŸ¥è¦‹ãƒãƒ©ãƒ³ã‚µãƒ¼ v2.1
---------------------------------
ç›®çš„:
  /output/semantics/ é…ä¸‹ã®æ—¢å­˜ JSON ç¾¤ã‚’èåˆã—ã€
  ALTãƒ»ã‚³ãƒ”ãƒ¼ç”Ÿæˆç”¨ã®ã€Œæ§‹é€ åŒ–ï¼‹è‡ªç„¶æ–‡ã€çŸ¥è¦‹ã‚’ç”Ÿæˆã™ã‚‹ã€‚

æ”¹è‰¯ç‚¹:
  âœ… feature/scenes/targets/benefits ãŒç©ºã®å ´åˆã¯å•†å“ã‚¿ã‚¤ãƒˆãƒ«ãªã©ã‹ã‚‰è‡ªå‹•æŠ½å‡º
  âœ… lexicalãƒ»market èªç¾¤ã‚’è‡ªå‹•æ•´å½¢ã—æ–‡ç´ æã‚’è£œå®Œ
  âœ… template_composer.json ãŒç©ºãªã‚‰è‡ªå‹•ç”Ÿæˆï¼ˆæ–‡æ§‹é€ ãƒ‘ã‚¿ãƒ¼ãƒ³10ç¨®ï¼‰
  âœ… è‡ªç„¶æ–‡ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’åˆ©ç”¨ã—ã¦8ã€œ14æ–‡ã®çŸ¥è¦‹æ–‡ã‚’æ§‹ç¯‰
  âœ… ç¦å‰‡èªãƒªã‚¹ãƒˆã‚‚çµ±åˆã—ã¦ JSON å‡ºåŠ›

å‡ºåŠ›:
  ./output/semantics/knowledge_fused_structured_v2_1.json
"""

import os
import re
import json
import random
import glob
from collections import defaultdict
from datetime import datetime

BASE_DIR = "/Users/tsuyoshi/Desktop/python_lesson"
SEM_DIR = os.path.join(BASE_DIR, "output/semantics")
OUT_PATH = os.path.join(SEM_DIR, "knowledge_fused_structured_v2_1.json")

# ======================================
# Utility
# ======================================

def safe_load(path):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None


def normalize_word(w: str):
    """åŠè§’â†’å…¨è§’ã€è¨˜å·é™¤å»ãªã©è»½æ•´å½¢"""
    if not isinstance(w, str):
        return ""
    w = re.sub(r"[\sã€€]+", "", w)
    w = re.sub(r"[!ï¼?ï¼Ÿãƒ»:ï¼š;ï¼›]", "", w)
    return w.strip()


def end_with_maru(s: str) -> str:
    s = s.strip()
    return s if s.endswith("ã€‚") else s + "ã€‚"


# ======================================
# 1. å…¥åŠ›èª­è¾¼
# ======================================
def collect_semantic_files():
    files = glob.glob(os.path.join(SEM_DIR, "*.json"))
    print(f"ğŸ” èª­ã¿è¾¼ã¿å¯¾è±¡JSON: {len(files)}ä»¶")
    return files


# ======================================
# 2. æ–‡ç´ ææŠ½å‡ºï¼‹è£œå®Œ
# ======================================
def extract_semantic_payload(files):
    payload = defaultdict(list)
    forbidden = set()

    for path in files:
        name = os.path.basename(path).lower()
        data = safe_load(path)
        if not data:
            continue

        try:
            if "lexical" in name:
                # èªå½™ã‚¯ãƒ©ã‚¹ã‚¿
                arr = data.get("clusters") or data if isinstance(data, list) else []
                for c in arr:
                    terms = c.get("terms") if isinstance(c, dict) else None
                    if isinstance(terms, list):
                        payload["lexical"].extend(normalize_word(t) for t in terms)
            elif "market" in name:
                # å¸‚å ´èªå½™
                vocab = []
                if isinstance(data, list):
                    for v in data:
                        if isinstance(v, dict) and "vocabulary" in v:
                            vocab.append(v["vocabulary"])
                        elif isinstance(v, str):
                            vocab.append(v)
                elif isinstance(data, dict):
                    vocab.extend(data.get("vocabulary", []))
                payload["market"].extend(normalize_word(v) for v in vocab)
            elif "structured_semantics" in name or "semantic" in name:
                # æ§‹é€ èªç¾¤
                for key in ["features", "scenes", "targets", "benefits"]:
                    vals = data.get(key) or []
                    payload[key].extend(normalize_word(v) for v in vals)
            elif "persona" in name:
                tone = data.get("tone") or {}
                for v in tone.values() if isinstance(tone, dict) else []:
                    if isinstance(v, str):
                        payload["tone"].append(v)
            elif "normalized" in name or "forbid" in name:
                fw = data.get("forbidden_words") or []
                forbidden.update(fw)
            elif "template" in name:
                hints = data.get("hints") or []
                payload["templates"].extend(hints)
        except Exception:
            continue

    # ====== æ¬ æè£œå®Œ ======
    # features/scenes/targets/benefits ãŒç©ºãªã‚‰ lexical / market ã‹ã‚‰æ“¬ä¼¼ç”Ÿæˆ
    for key in ["features", "scenes", "targets", "benefits"]:
        if not payload.get(key):
            seeds = random.sample(payload.get("lexical", []) + payload.get("market", []), 
                                  k=min(10, len(payload.get("lexical", []))))
            payload[key] = list({normalize_word(s) for s in seeds if s})

    # templates ãŒç©ºãªã‚‰ãƒ‡ãƒ•ã‚©ç”Ÿæˆ
    if not payload.get("templates"):
        payload["templates"] = [
            "ç‰¹å¾´â†’ç”¨é€”â†’åˆ©ä¾¿æ€§",
            "å¯¾è±¡â†’ç‰¹å¾´â†’ãƒ™ãƒãƒ•ã‚£ãƒƒãƒˆ",
            "ç´ æâ†’æ©Ÿèƒ½â†’å¿«é©æ€§",
            "åˆ©ç”¨ã‚·ãƒ¼ãƒ³â†’ç‰¹å¾´â†’æº€è¶³æ„Ÿ",
            "ãƒ‡ã‚¶ã‚¤ãƒ³â†’ä½¿ç”¨æ„Ÿâ†’è€ä¹…æ€§",
            "æ€§èƒ½â†’æ“ä½œæ€§â†’åˆ©ä¾¿æ€§",
            "ä¾¡æ ¼â†’æ©Ÿèƒ½â†’æº€è¶³åº¦",
            "æ§‹é€ â†’å¿«é©æ€§â†’å®‰å¿ƒæ„Ÿ",
            "ä»•æ§˜â†’æºå¸¯æ€§â†’å¿«é©æ€§",
            "ç’°å¢ƒâ†’ç´ æâ†’ä½¿ã„ã‚„ã™ã•"
        ]

    return payload, list(forbidden)


# ======================================
# 3. è‡ªç„¶æ–‡æ§‹ç¯‰
# ======================================
TEMPLATES_SENTENCE = [
    "{feature}ã¯{target}ã«æœ€é©ã§ã€{scene}ã§{benefit}",
    "{target}ãŒæ±‚ã‚ã‚‹{feature}ã‚’å®Ÿç¾ã—ã€{scene}ã‚’ã‚ˆã‚Šå¿«é©ã«",
    "{scene}ã§æ´»èºã™ã‚‹{feature}ãŒã€{target}ã«{benefit}",
    "é«˜å“è³ªãª{feature}ã§ã€{target}ã®{scene}ã‚’ã‚µãƒãƒ¼ãƒˆ",
    "{scene}ã§ã‚‚æ´»èºã™ã‚‹{feature}ãŒ{benefit}",
    "{feature}ã«ã‚ˆã‚Šã€{target}ãŒ{scene}ã§å¿«é©ã«éã”ã›ã¾ã™",
    "è€ä¹…æ€§ã®ã‚ã‚‹{feature}ã§ã€{scene}ã‚„{target}ã«ã‚‚å®‰å¿ƒ",
    "æ—¥å¸¸ã®{scene}ã«æº¶ã‘è¾¼ã‚€{feature}ã§{benefit}",
    "{target}ãŒé¸ã¶{feature}ã€{scene}ã§ã‚‚ä¾¿åˆ©",
    "{scene}ã§ã‚‚{benefit}ã‚’æ„Ÿã˜ã‚‹{feature}ãŒç‰¹é•·"
]


def build_sentence(feature, scene, target, benefit):
    tpl = random.choice(TEMPLATES_SENTENCE)
    s = tpl.format(
        feature=feature or "é«˜æ€§èƒ½è¨­è¨ˆ",
        scene=scene or "æ—¥å¸¸åˆ©ç”¨",
        target=target or "å¹…åºƒã„ãƒ¦ãƒ¼ã‚¶ãƒ¼",
        benefit=benefit or "å¿«é©æ€§ã‚’æä¾›"
    )
    return end_with_maru(s)


def to_natural_sentences(payload, aim_min=8, aim_max=14):
    feats = payload.get("features", [])
    scns = payload.get("scenes", [])
    tgs = payload.get("targets", [])
    bens = payload.get("benefits", [])

    sents = []
    for _ in range(random.randint(aim_min, aim_max)):
        f = random.choice(feats) if feats else ""
        s = random.choice(scns) if scns else ""
        t = random.choice(tgs) if tgs else ""
        b = random.choice(bens) if bens else ""
        sents.append(build_sentence(f, s, t, b))

    return sents


# ======================================
# 4. æ›¸ãå‡ºã—
# ======================================
def main():
    print("ğŸ§© KOTOHAçŸ¥è¦‹ãƒãƒ©ãƒ³ã‚µãƒ¼ v2.1 èµ·å‹•ä¸­â€¦")
    os.makedirs(SEM_DIR, exist_ok=True)

    files = collect_semantic_files()
    payload, forbidden = extract_semantic_payload(files)
    sentences = to_natural_sentences(payload)

    out = {
        "version": "2.1",
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "knowledge_sentences": sentences,
        "structured_counts": {k: len(v) for k, v in payload.items()},
        "forbidden_words": forbidden,
    }

    with open(OUT_PATH, "w", encoding="utf-8") as f:
        json.dump(out, f, ensure_ascii=False, indent=2)

    print(f"âœ… å‡ºåŠ›å®Œäº†: {OUT_PATH}")
    print(f"ğŸ“˜ çŸ¥è¦‹æ–‡æ•°: {len(sentences)}")
    print(f"ğŸ“Š æ§‹é€ ã‚«ã‚¦ãƒ³ãƒˆ: {out['structured_counts']}")

if __name__ == "__main__":
    main()
import atlas_autosave_core
