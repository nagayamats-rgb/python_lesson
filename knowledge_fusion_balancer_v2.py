# -*- coding: utf-8 -*-
"""
KOTOHA çŸ¥è¦‹ãƒãƒ©ãƒ³ã‚µãƒ¼ v2ï¼ˆè‡ªç„¶æ–‡æ§‹é€ ç‰ˆï¼‰
- ç›®çš„: /output/semantics é…ä¸‹ã® JSON ç¾¤ã‚’â€œè‡ªç„¶æ–‡ã®çŸ¥è¦‹â€ã«å†æ§‹æˆã—ã€
        ALT/ã‚³ãƒ”ãƒ¼ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ç›´æ¥æŠ•å…¥ã§ãã‚‹å½¢ã¸æ­£è¦åŒ–ã™ã‚‹ã€‚
- å…¥åŠ›: ./output/semantics/
    lexical_clusters_*.json       â†’ ç”¨èªãƒ»å½¢å®¹èªãƒ»ç‰¹å¾´
    market_vocab_*.json           â†’ å¸‚å ´èªå½™ï¼ˆæ©Ÿèƒ½/ç”¨é€”/å¯¾è±¡ãªã©ï¼‰
    structured_semantics_*.json   â†’ scenes/targets/features/benefits ãªã©ã®æ§‹é€ èªå½™
    styled_persona_*.json         â†’ tone/style ç­‰
    template_composer.json        â†’ æ§‹æˆãƒ’ãƒ³ãƒˆ
    normalized_*.json             â†’ ç¦å‰‡èªï¼ˆforbidden_wordsï¼‰
- å‡ºåŠ›:
    ./output/semantics/knowledge_fused_structured_v2.json

è¨­è¨ˆãƒ¡ãƒ¢:
- å…¥ã£ã¦ãã‚‹ JSON ã®å½¢ã¯è¾æ›¸ã ã£ãŸã‚Šé…åˆ—ã ã£ãŸã‚Šãƒãƒ©ãƒãƒ©ãªã®ã§ã€ã™ã¹ã¦å®‰å…¨ã«å¸åã€‚
- ã‚¿ã‚°åˆ—ã‚’ãã®ã¾ã¾ç¹‹ãŒãšã€çŸ­ã„è‡ªç„¶æ–‡ã¸å¤‰æ›ï¼ˆã€œã«é©ã—ãŸã€ã€œã‚’å‚™ãˆãŸã€ã€œã‚’æƒ³å®šã—ãŸ ãªã©ï¼‰ã€‚
- æ–‡ã¯ 8ã€œ14 æ–‡ã‚’ç›®å®‰ã«ç”Ÿæˆï¼ˆé•·æ–‡ ALT/ã‚³ãƒ”ãƒ¼ã®ç¨®ã¨ã—ã¦ã¡ã‚‡ã†ã©è‰¯ã„é‡ï¼‰ã€‚
- å¥ç‚¹ã€Œã€‚ã€ã§å¿…ãšçµ‚æ­¢ã€‚é‡è¤‡ã‚’é™¤å»ã€‚ç¦å‰‡èªã¯æœ€å¾Œã«å†æ¤œé–²ã—ã¦é™¤å»/ç½®æ›ã€‚
"""

import os
import re
import json
import glob
import random
from typing import Any, Dict, List, Iterable

BASE_DIR = os.path.abspath(os.path.dirname(__file__))
SEMANTICS_DIR = os.path.join(BASE_DIR, "output", "semantics")
OUT_PATH = os.path.join(SEMANTICS_DIR, "knowledge_fused_structured_v2.json")

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# åŸºæœ¬ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
def safe_load_json(path: str) -> Any:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None

def ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)

def uniq(seq: Iterable[str]) -> List[str]:
    seen, out = set(), []
    for x in seq:
        if not isinstance(x, str):
            continue
        x2 = x.strip()
        if not x2 or x2 in seen:
            continue
        out.append(x2)
        seen.add(x2)
    return out

def normalize_term(t: str) -> str:
    t = (t or "").strip()
    # éœ²éª¨ãªãƒ©ãƒ™ãƒ«/è¨˜å·ã‚’æƒé™¤
    t = re.sub(r"^(?:[-*ãƒ»â—\dâ‘ -â‘©]\s*[\.ï¼ã€]?\s*)", "", t)
    # é€£ç¶šç©ºç™½
    t = re.sub(r"\s+", " ", t)
    return t

def end_with_maru(s: str) -> str:
    s = s.strip()
    if not s.endswith("ã€‚"):
        s += "ã€‚"
    return s

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# æ—¢å­˜ JSON ç¾¤ã‚’å¸å
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
def load_semantic_inputs() -> Dict[str, Any]:
    files = glob.glob(os.path.join(SEMANTICS_DIR, "*.json"))
    payload = {
        "lexical": [],
        "market": [],
        "structured": {},
        "persona": {},
        "templates": [],
        "forbidden": [],
    }

    for p in files:
        name = os.path.basename(p).lower()
        data = safe_load_json(p)
        if data is None:
            continue

        try:
            # lexical_clusters_* ä¾‹: {"clusters":[{"terms":[...]}, ...]} / [{"terms":[...]}] / ["èª1","èª2"]
            if "lexical" in name and "cluster" in name:
                if isinstance(data, dict):
                    arr = data.get("clusters") or data.get("lexical") or []
                elif isinstance(data, list):
                    arr = data
                else:
                    arr = []
                for c in arr:
                    if isinstance(c, dict) and "terms" in c and isinstance(c["terms"], list):
                        payload["lexical"].extend([normalize_term(t) for t in c["terms"] if isinstance(t, str)])
                    elif isinstance(c, str):
                        payload["lexical"].append(normalize_term(c))

            # market_vocab_* ä¾‹: [{"vocabulary":"MagSafe"}, ...] / ["MagSafe", "PD"]
            elif "market" in name and "vocab" in name:
                if isinstance(data, list):
                    for v in data:
                        if isinstance(v, dict) and "vocabulary" in v:
                            payload["market"].append(normalize_term(v.get("vocabulary", "")))
                        elif isinstance(v, str):
                            payload["market"].append(normalize_term(v))
                elif isinstance(data, dict):
                    vocab = data.get("vocabulary") or data.get("vocab") or []
                    if isinstance(vocab, list):
                        payload["market"].extend([normalize_term(x) for x in vocab if isinstance(x, str)])

            # structured_semantics_* ä¾‹: {"scenes":[...], "targets":[...], "features":[...], "benefits":[...]}
            elif "structured_semantics" in name or ("structured" in name and "semantic" in name):
                if isinstance(data, dict):
                    for k in ["scenes", "targets", "features", "benefits", "concepts", "use_cases"]:
                        arr = data.get(k) or []
                        if isinstance(arr, list):
                            payload["structured"].setdefault(k, [])
                            payload["structured"][k].extend([normalize_term(x) for x in arr if isinstance(x, str)])
                elif isinstance(data, list):
                    # æƒ³å®šå¤–ã ãŒã€æ–‡å­—åˆ—ãƒªã‚¹ãƒˆãªã‚‰ features æ‰±ã„ã§å¸å
                    payload["structured"].setdefault("features", [])
                    payload["structured"]["features"].extend([normalize_term(x) for x in data if isinstance(x, str)])

            # styled_persona_* ä¾‹: {"tone":{"style":"ã€œ","register":"ã€œ"}} / {"tone":["ä¸Šå“","çŸ¥çš„"]} / ["ã€œ"]
            elif "styled_persona" in name or "persona" in name:
                tone = {}
                if isinstance(data, dict):
                    t = data.get("tone")
                    if isinstance(t, dict):
                        for k, v in t.items():
                            if isinstance(v, str):
                                tone[k] = normalize_term(v)
                    elif isinstance(t, list):
                        tone["hints"] = uniq([normalize_term(x) for x in t if isinstance(x, str)])
                elif isinstance(data, list):
                    tone["hints"] = uniq([normalize_term(x) for x in data if isinstance(x, str)])
                if tone:
                    payload["persona"] = tone

            # template_composer.json ä¾‹: {"hints":[...]} / {"templates":[...]} / ["ã€œ"]
            elif "template_composer" in name or "template" in name:
                if isinstance(data, dict):
                    arr = data.get("hints") or data.get("templates") or []
                    if isinstance(arr, list):
                        payload["templates"].extend([normalize_term(x) for x in arr if isinstance(x, str)])
                elif isinstance(data, list):
                    payload["templates"].extend([normalize_term(x) for x in data if isinstance(x, str)])

            # normalized_* ä¾‹: {"forbidden_words":[...]} / ["ç”»åƒ","å†™çœŸ",...]
            elif "normalized" in name or "forbid" in name:
                if isinstance(data, dict):
                    fw = data.get("forbidden_words") or data.get("forbidden") or []
                    if isinstance(fw, list):
                        payload["forbidden"].extend([normalize_term(x) for x in fw if isinstance(x, str)])
                elif isinstance(data, list):
                    payload["forbidden"].extend([normalize_term(x) for x in data if isinstance(x, str)])

        except Exception:
            # å£Šã‚ŒãŸå½¢å¼ã¯é»™ã£ã¦ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå …ç‰¢å„ªå…ˆï¼‰
            pass

    # ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–
    payload["lexical"]   = uniq(payload["lexical"])
    payload["market"]    = uniq(payload["market"])
    for k, v in list(payload["structured"].items()):
        payload["structured"][k] = uniq(v)
    payload["templates"] = uniq(payload["templates"])
    payload["forbidden"] = uniq(payload["forbidden"])
    return payload

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# è‡ªç„¶æ–‡ã¸ã®å†™åƒï¼ˆçŸ­æ–‡ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ï¼‰
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
CONNECTORS = [
    "ã€", "ã§", "ãªãŒã‚‰", "ã ã‹ã‚‰", "ã ã‹ã‚‰ã“ã", "ã ã‹ã‚‰ã¨è¨€ã£ã¦", "ãã—ã¦", "ã•ã‚‰ã«"
]

PATTERNS_FEATURE = [
    "{feat}ã‚’å‚™ãˆã€{benefit}ã‚’å®Ÿç¾ã—ã¾ã™",
    "{feat}ã®è¨­è¨ˆã§ã€{scene}ã§ã‚‚å¿«é©ã«ä½¿ãˆã¾ã™",
    "æ—¥å¸¸ã®{scene}ã§å½¹ç«‹ã¤{feat}ãŒé­…åŠ›ã§ã™",
    "{feat}ã«ã‚ˆã‚Šã€{target}ã®{benefit}ã«è²¢çŒ®ã—ã¾ã™",
]

PATTERNS_SCENE = [
    "{scene}ã«æœ€é©ã§ã€{feature}ãŒ{benefit}ã‚’å¾ŒæŠ¼ã—ã—ã¾ã™",
    "{scene}ã‚’æƒ³å®šã—ãŸè¨­è¨ˆã§ã€{target}ã®ä½¿ã„å‹æ‰‹ã‚’é«˜ã‚ã¾ã™",
]

PATTERNS_TARGET = [
    "{target}ã«å‘ã‘ã¦ä½œã‚‰ã‚Œã€{feature}ã§{benefit}ã‚’ã‚‚ãŸã‚‰ã—ã¾ã™",
    "{target}ã®æ—¥å¸¸ã«å¯„ã‚Šæ·»ã„ã€{scene}ã§ã‚‚æ‰±ã„ã‚„ã™ã„é…æ…®ãŒã‚ã‚Šã¾ã™",
]

PATTERNS_GENERIC = [
    "{feature}ã«é…æ…®ã—ãŸè¨­è¨ˆã§ã€{benefit}ã‚’ç‹™ãˆã¾ã™",
    "ä½¿ã„å‹æ‰‹ã‚’é‡è¦–ã—ã€{scene}ã§ã‚‚æ‰±ã„ã‚„ã™ãä»•ä¸Šã’ã¦ã„ã¾ã™",
]

def pick(xs: List[str], n: int) -> List[str]:
    xs = [x for x in xs if isinstance(x, str) and x]
    if not xs:
        return []
    if len(xs) <= n:
        return xs
    return random.sample(xs, n)

def join_terms(terms: List[str], limit: int = 3) -> str:
    terms = uniq([t for t in terms if t])
    if not terms:
        return ""
    # 3èªãã‚‰ã„ã¾ã§ç´ ç›´ã«èª­ç‚¹æ¥ç¶š
    return "ã€".join(terms[:limit])

def build_sentence(feature: str = "", scene: str = "", target: str = "", benefit: str = "") -> str:
    """
    ä¸ãˆã‚‰ã‚ŒãŸ semantic ã‚¹ãƒ­ãƒƒãƒˆã‹ã‚‰è‡ªç„¶æ–‡ã‚’1ã¤ç”Ÿæˆã€‚
    ã‚¹ãƒ­ãƒƒãƒˆãŒç©ºã§ã‚‚ç ´ç¶»ã—ãªã„ãƒ†ãƒ³ãƒ—ãƒ¬é¸æŠã€‚
    """
    feature = normalize_term(feature)
    scene   = normalize_term(scene)
    target  = normalize_term(target)
    benefit = normalize_term(benefit)

    # ã‚¹ãƒ­ãƒƒãƒˆã«å¿œã˜ã¦ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
    if feature and scene and benefit:
        tpl = random.choice(PATTERNS_FEATURE + PATTERNS_SCENE)
    elif feature and target and benefit:
        tpl = random.choice(PATTERNS_FEATURE + PATTERNS_TARGET)
    elif scene and target:
        tpl = random.choice(PATTERNS_SCENE + PATTERNS_TARGET)
    else:
        tpl = random.choice(PATTERNS_GENERIC)

    # â†ã“ã“ã‚’ä¿®æ­£ï¼š 'feat' ã§ã¯ãªã 'feature' ã«çµ±ä¸€
    s = tpl.format(
        feature=feature or "åˆ©ä¾¿æ€§",
        scene=scene or "æ—¥å¸¸åˆ©ç”¨",
        target=target or "å¹…åºƒã„ãƒ¦ãƒ¼ã‚¶ãƒ¼",
        benefit=benefit or "å¿«é©æ€§ã®å‘ä¸Š",
    )

    # æ¥ç¶šè©ã§è»½ãè±Šã‹ã•ã‚’å‡ºã™ï¼ˆå¿…è¦ãªã¨ãã ã‘ï¼‰
    if random.random() < 0.35 and feature and scene:
        s = f"{feature}{random.choice(CONNECTORS)}{s}"

    return end_with_maru(s)

def to_natural_sentences(payload: Dict[str, Any], aim_min=8, aim_max=14) -> List[str]:
    # ç´ æã®å–ã‚Šå‡ºã—
    feats   = payload.get("structured", {}).get("features", []) or payload.get("lexical", [])
    scenes  = payload.get("structured", {}).get("scenes",   [])
    targets = payload.get("structured", {}).get("targets",  [])
    bens    = payload.get("structured", {}).get("benefits", [])
    market  = payload.get("market", [])

    # â€œèªã®ç¾…åˆ—â€ã§ãªãã€â€œæ–‡â€ã¨ã—ã¦ã®ç´ æã‚’å¢—ã‚„ã™ãŸã‚ã€å°‘ã—ã ã‘æ··ãœã‚‹
    # ä½¿ã„ã™ãã‚‹ã¨ä¸è‡ªç„¶ã«ãªã‚‹ã®ã§ã€ãã‚Œãã‚Œä¸Šé™ã‚’çµã‚‹
    feats_use   = pick(feats or market,   10)
    scenes_use  = pick(scenes or market,   8)
    targets_use = pick(targets or market,  6)
    bens_use    = pick(bens or feats,      8)

    # æ–‡ã‚’çµ„ã‚€
    sentences = []
    iter_max = max(aim_max * 2, 30)  # ç”Ÿæˆä½™è£•
    i = 0
    while len(sentences) < aim_max and i < iter_max:
        i += 1
        f = feats_use[i % len(feats_use)] if feats_use else ""
        sc = scenes_use[i % len(scenes_use)] if scenes_use else ""
        tg = targets_use[i % len(targets_use)] if targets_use else ""
        bn = bens_use[i % len(bens_use)] if bens_use else ""
        s  = build_sentence(f, sc, tg, bn)
        sentences.append(s)

    # ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–
    sentences = uniq(sentences)

    # æ–‡å­—æ•°ã®è»½ã„æ•´å½¢ï¼ˆ60ã€œ120å­—ç›®å®‰ï¼‰
    def soft_len(s: str) -> str:
        s = s.strip()
        # æœ«å°¾èª¿æ•´
        s = end_with_maru(s)
        # çŸ­ã™ãã‚‹å ´åˆã¯ã€featureã‚„marketã‚’1èªã ã‘è¶³ã—ã¦å»¶ã°ã™
        if len(s) < 50 and feats:
            s = re.sub(r"ã€‚$", f"ã€{random.choice(feats)}ã‚’æ„è­˜ã—ãŸè¨­è¨ˆã§ã™ã€‚", s)
        # é•·éãã‚‹å ´åˆã¯ã€å¥ç‚¹ã§è‡ªç„¶ã‚«ãƒƒãƒˆ
        if len(s) > 130:
            cut = s[:130]
            p = cut.rfind("ã€‚")
            s = (cut[:p+1] if p != -1 else cut)
        return s

    sentences = [soft_len(s) for s in sentences]

    # ç›®æ¨™æ•°ã«å¯„ã›ã‚‹ï¼ˆä¸è¶³åˆ†ã¯è»½ã„ãƒ‘ãƒ©ãƒ•ãƒ¬ãƒ¼ã‚ºï¼‰
    def paraphrase(s: str) -> str:
        s2 = s.replace("å®Ÿç¾ã—ã¾ã™ã€‚", "å¶ãˆã¾ã™ã€‚")
        s2 = s2.replace("ä½¿ãˆã¾ã™ã€‚", "ã—ã‚„ã™ã„ã§ã™ã€‚")
        s2 = s2.replace("ä»•ä¸Šã’ã¦ã„ã¾ã™ã€‚", "ä»•ä¸Šã’ã§ã™ã€‚")
        if s2 == s:
            s2 = s[:-1] + "ã®ãŒç‰¹é•·ã§ã™ã€‚"
        return end_with_maru(s2)

    j = 0
    while len(sentences) < aim_min and sentences:
        sentences.append(paraphrase(sentences[j % len(sentences)]))
        j += 1

    # ä»•ä¸Šã’ï¼šé‡è¤‡é™¤å» & ç›®æ¨™ä¸Šé™ã«ã‚¯ãƒªãƒƒãƒ—
    return sentences[:aim_max]

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# ç¦å‰‡é©ç”¨
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
DEFAULT_FORBIDDEN = [
    "ç”»åƒ", "å†™çœŸ", "è¦‹ãŸç›®", "ä¸Šã®ç”»åƒ", "ä¸‹ã®å†™çœŸ",
    "å½“åº—", "å½“ç¤¾", "ãƒ¬ãƒ“ãƒ¥ãƒ¼", "ãƒ©ãƒ³ã‚­ãƒ³ã‚°",
    "ã‚¯ãƒªãƒƒã‚¯", "ã“ã¡ã‚‰", "ãƒªãƒ³ã‚¯", "è³¼å…¥ã¯ã“ã¡ã‚‰",
    "ç«¶åˆ", "å„ªä½æ€§", "æ¥­ç•Œæœ€é«˜", "æœ€å®‰", "No.1", "ãƒŠãƒ³ãƒãƒ¼ãƒ¯ãƒ³", "å£²ä¸ŠNo1",
]

def apply_forbidden(sentences: List[str], words: List[str]) -> List[str]:
    ngs = uniq((words or []) + DEFAULT_FORBIDDEN)
    out = []
    for s in sentences:
        t = s
        for ng in ngs:
            if ng and ng in t:
                t = t.replace(ng, "")
        t = re.sub(r"\s+", " ", t).strip()
        t = end_with_maru(t)
        out.append(t)
    return out

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# ãƒ¡ã‚¤ãƒ³
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
def main():
    random.seed(42)  # å†ç¾æ€§ç¢ºä¿ï¼ˆå¿…è¦ã«å¿œã˜ã¦å¤–ã™ï¼‰
    ensure_dir(SEMANTICS_DIR)

    payload = load_semantic_inputs()
    sentences = to_natural_sentences(payload, aim_min=8, aim_max=14)
    sentences = apply_forbidden(sentences, payload.get("forbidden", []))

    result = {
        "knowledge_text": sentences,
        "forbidden_words": uniq((payload.get("forbidden") or []) + DEFAULT_FORBIDDEN),
        "meta": {
            "source_files_count": len(glob.glob(os.path.join(SEMANTICS_DIR, "*.json"))),
            "lexical_count": len(payload.get("lexical", [])),
            "market_count": len(payload.get("market", [])),
            "structured_counts": {k: len(v) for k, v in payload.get("structured", {}).items()},
            "templates_count": len(payload.get("templates", [])),
            "persona_keys": list(payload.get("persona", {}).keys()),
        }
    }

    with open(OUT_PATH, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"âœ… å‡ºåŠ›å®Œäº†: {OUT_PATH}")
    print(f"ğŸ“˜ çŸ¥è¦‹æ–‡æ•°: {len(sentences)} / ç¦å‰‡èªæ•°: {len(result['forbidden_words'])}")

if __name__ == "__main__":
    main()
import atlas_autosave_core
