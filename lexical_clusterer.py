# ---------- åˆæœŸè¨­å®š ----------
import os
import json
import time
import logging
import pandas as pd
from datetime import datetime
from dotenv import load_dotenv
from openai import OpenAI
from sklearn.cluster import KMeans
from sklearn.preprocessing import normalize
import numpy as np
from pathlib import Path

# ============================================================
# ğŸŒ¸ KOTOHA ENGINE â€” lexical_clusterer.py
# ============================================================

# ---------- .env èª­ã¿è¾¼ã¿ ----------
env_path = Path("/Users/nagayamasoma/Desktop/python_lesson/.env.txt")  # â† çµ¶å¯¾ãƒ‘ã‚¹æŒ‡å®šï¼
if not env_path.exists():
    logging.error(f"âŒ .env ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {env_path}")
else:
    load_dotenv(dotenv_path=env_path)
    logging.info(f"âœ… .env èª­ã¿è¾¼ã¿æˆåŠŸ: {env_path}")

# ---------- OpenAIåˆæœŸåŒ– ----------
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    logging.error("ğŸš« OPENAI_API_KEY ãŒèª­ã¿è¾¼ã¾ã‚Œã¾ã›ã‚“ã€‚")
else:
    client = OpenAI(api_key=api_key)
    logging.info("âœ… OpenAI APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¾ã—ãŸã€‚")


# ---------- ãƒ•ã‚¡ã‚¤ãƒ«æ¢ç´¢ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ----------
def find_latest_file(directory="./output", prefix="market_vocab_", ext=".json"):
    """æœ€æ–°ã®å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡ºï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯ãƒ•ã‚©ãƒ«ãƒ€è‡ªå‹•ç”Ÿæˆï¼‰"""
    # --- å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèª ---
    if not os.path.exists(directory):
        logging.warning(f"âš ï¸ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚ä½œæˆã—ã¾ã™: {directory}")
        os.makedirs(directory, exist_ok=True)
        return None  # æ–°è¦ä½œæˆæ™‚ã¯ã¾ã ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„ãŸã‚ None ã‚’è¿”ã™

    # --- ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ ---
    files = [f for f in os.listdir(directory) if f.startswith(prefix) and f.endswith(ext)]
    if not files:
        logging.error(f"âŒ {prefix}*.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None

    # --- æœ€çµ‚æ›´æ–°æ—¥æ™‚ã§ã‚½ãƒ¼ãƒˆ ---
    files.sort(key=lambda x: os.path.getmtime(os.path.join(directory, x)), reverse=True)
    latest = os.path.join(directory, files[0])
    logging.info(f"âœ… æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º: {latest}")
    return latest


# ---------- Embeddingãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ----------
def get_embedding(text):
    """OpenAI Embedding API å‘¼ã³å‡ºã—"""
    try:
        response = client.embeddings.create(model="text-embedding-3-small", input=text)
        return response.data[0].embedding
    except Exception as e:
        logging.error(f"ğŸš« Embedding å¤±æ•—: {e}")
        return None

# ---------- ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ ----------
def cluster_phrases(phrases, n_clusters=7):
    """AIè£œåŠ©ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°"""
    valid_phrases = [p for p in phrases if p and isinstance(p, str)]
    logging.info(f"ğŸ“Š ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å¯¾è±¡ãƒ•ãƒ¬ãƒ¼ã‚ºæ•°: {len(valid_phrases)}")

    embeddings = []
    for p in valid_phrases:
        emb = get_embedding(p)
        if emb:
            embeddings.append(emb)
        time.sleep(0.1)  # APIãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆå¯¾ç­–

    if not embeddings:
        logging.error("ğŸš« æœ‰åŠ¹ãªEmbeddingãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
        return {}

    # æ­£è¦åŒ–
    X = normalize(np.array(embeddings))
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X)

    clusters = {i: [] for i in range(n_clusters)}
    for phrase, label in zip(valid_phrases, labels):
        clusters[label].append(phrase)
    return clusters

# ---------- ã‚¯ãƒ©ã‚¹ã‚¿å‘½å ----------
CLUSTER_NAMES = ["SPEC", "FEATURE", "USAGE", "USABILITY", "EMOTION", "QUALITY", "MARKET"]

def assign_cluster_names(clusters):
    """å˜ç´”ãªé †åºãƒãƒƒãƒ”ãƒ³ã‚°ã§ã‚¯ãƒ©ã‚¹ã‚¿åã‚’ä»˜ä¸"""
    named = {}
    for i, (key, values) in enumerate(clusters.items()):
        name = CLUSTER_NAMES[i] if i < len(CLUSTER_NAMES) else f"CLUSTER_{i}"
        named[name] = values
    return named

# ---------- ãƒ¡ã‚¤ãƒ³å‡¦ç† ----------
def main():
    logging.info("ğŸŒ¸ KOTOHA ENGINE â€” lexical_clusterer èµ·å‹•")

    # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«æ±ºå®š
    input_file = find_latest_file() or "market_vocab_20251030_201906.json"
    if not os.path.exists(input_file):
        logging.error(f"âŒ å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_file}")
        return

    # å‡ºåŠ›ãƒ‘ã‚¹æº–å‚™
    Path("./output").mkdir(exist_ok=True)
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    json_out = f"./output/lexical_clusters_{ts}.json"
    csv_out = f"./output/lexical_summary_{ts}.csv"

    # å…¥åŠ›èª­ã¿è¾¼ã¿
    with open(input_file, "r", encoding="utf-8") as f:
        data = json.load(f)

    # èªå½™æŠ½å‡º
    vocab = list({w for group in data.values() for w in group if isinstance(w, str)})
    logging.info(f"ğŸ“¦ èªå½™æ•°ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯ï¼‰: {len(vocab)}")

    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
    clusters = cluster_phrases(vocab, n_clusters=len(CLUSTER_NAMES))
    named_clusters = assign_cluster_names(clusters)

    # ä»£è¡¨èªæŠ½å‡º
    summary = []
    for name, words in named_clusters.items():
        rep = words[0] if words else ""
        summary.append({
            "ã‚¯ãƒ©ã‚¹ã‚¿": name,
            "ä»£è¡¨èª": rep,
            "ç™»éŒ²èªæ•°": len(words)
        })

    # å‡ºåŠ›
    with open(json_out, "w", encoding="utf-8") as f:
        json.dump(named_clusters, f, ensure_ascii=False, indent=2)
    pd.DataFrame(summary).to_csv(csv_out, index=False, encoding="utf-8-sig")

    logging.info(f"ğŸ’¾ ã‚¯ãƒ©ã‚¹ã‚¿å‡ºåŠ›: {json_out}")
    logging.info(f"ğŸ’¾ ã‚µãƒãƒªå‡ºåŠ›: {csv_out}")
    logging.info("âœ… lexical_clusterer å®Œäº†")

# ---------- å®Ÿè¡Œ ----------
if __name__ == "__main__":
    main()
import atlas_autosave_core
