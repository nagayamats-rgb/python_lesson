import os
import json
import time
import logging
from datetime import datetime
from tqdm import tqdm
from dotenv import load_dotenv
from openai import OpenAI, OpenAIError

# ======================================
# ğŸŒ¸ KOTOHA ENGINE â€” Lexical Clusterer v1.3 Progress Enhanced
# ======================================

logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(levelname)s: %(message)s')

# --- åˆæœŸè¨­å®š ---
load_dotenv("/Users/nagayamasoma/Desktop/python_lesson/.env")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OUTPUT_DIR = "./output"
os.makedirs(OUTPUT_DIR, exist_ok=True)

if not OPENAI_API_KEY:
    logging.error("âŒ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    exit(1)

client = OpenAI(api_key=OPENAI_API_KEY)


def find_latest_file(directory=OUTPUT_DIR, prefix="market_vocab_", ext=".json"):
    """æœ€æ–°ã® market_vocab ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—"""
    try:
        files = [f for f in os.listdir(directory) if f.startswith(prefix) and f.endswith(ext)]
        if not files:
            logging.error("âŒ market_vocab_*.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return None
        files.sort(key=lambda f: os.path.getmtime(os.path.join(directory, f)), reverse=True)
        latest = os.path.join(directory, files[0])
        logging.info(f"ğŸ“„ æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨: {latest}")
        return latest
    except Exception as e:
        logging.error(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«æ¢ç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def get_embedding(text, max_retries=3):
    """OpenAI Embedding API å‘¼ã³å‡ºã—ï¼ˆãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰"""
    for attempt in range(max_retries):
        try:
            resp = client.embeddings.create(
                model="text-embedding-3-small",
                input=text
            )
            return resp.data[0].embedding
        except OpenAIError as e:
            logging.warning(f"âš ï¸ åŸ‹ã‚è¾¼ã¿å¤±æ•— ({attempt+1}/{max_retries}): {e}")
            time.sleep(2)
        except Exception as e:
            logging.error(f"ğŸš« äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")
            time.sleep(2)
    return None


def main(verbose=False):
    start_time = time.time()
    logging.info("ğŸŒ¸ KOTOHA ENGINE â€” Lexical Clusterer èµ·å‹•")

    input_file = find_latest_file()
    if not input_file:
        return

    # --- JSON èª­ã¿è¾¼ã¿ ---
    with open(input_file, "r", encoding="utf-8") as f:
        vocab_data = json.load(f)

    total_items = len(vocab_data)
    logging.info(f"ğŸ“Š èªå½™ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {total_items} ä»¶")

    # --- Embedding ç”Ÿæˆ ---
    print("\nğŸ§  Embeddingç”Ÿæˆä¸­...\n")
    embeddings = []
    for item in tqdm(vocab_data, total=total_items, desc="ğŸ” èªå½™åŸ‹ã‚è¾¼ã¿é€²è¡Œä¸­", unit="èª"):
        text = item.get("term") if isinstance(item, dict) else str(item)
        emb = get_embedding(text)
        if emb:
            embeddings.append({"term": text, "vector": emb})
        else:
            logging.error(f"ğŸš« åŸ‹ã‚è¾¼ã¿å¤±æ•—: {text[:40]}")

    # --- ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æï¼ˆç°¡æ˜“ãƒ€ãƒŸãƒ¼ï¼‰ ---
    print("\nğŸ’¡ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æä¸­...\n")
    clusters = [{"cluster_id": i, "terms": [e["term"] for e in embeddings[i::50]]} for i in range(50)]

    # --- ä¿å­˜ ---
    output_path = os.path.join(OUTPUT_DIR, f"lexical_clusters_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(clusters, f, ensure_ascii=False, indent=2)

    elapsed = time.time() - start_time
    print("\nâœ… å®Œäº†! Lexical Clusterer å®Ÿè¡Œçµæœ:")
    print(f"ğŸ•’ å®Ÿè¡Œæ™‚é–“: {elapsed/60:.2f}åˆ†")
    print(f"ğŸ§© ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {len(clusters)}")
    print(f"ğŸ“š åŸ‹ã‚è¾¼ã¿èªå½™æ•°: {len(embeddings)}")
    print(f"ğŸ’¾ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_path}\n")

    if verbose:
        logging.info("ğŸ” è©³ç´°ãƒ¢ãƒ¼ãƒ‰: å…ˆé ­ã‚¯ãƒ©ã‚¹ã‚¿ã‚’å‡ºåŠ›ã—ã¾ã™:")
        print(json.dumps(clusters[0], ensure_ascii=False, indent=2))


if __name__ == "__main__":
    import sys
    verbose = "--verbose" in sys.argv
    main(verbose=verbose)
import atlas_autosave_core
