import json, os, re
import pandas as pd
from datetime import datetime

def load_json(path):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def extract_field(text, keywords):
    """ç°¡æ˜“çš„ãªã‚¿ã‚°æŠ½å‡º: å„æ§‹æˆè¦ç´ (spec, competenceç­‰)ã‚’æ¤œå‡º"""
    if not text: return ""
    text = re.sub(r"[ã€Œã€ã€ã€]", "", text)
    for kw in keywords:
        if kw in text:
            return text
    return ""

def classify_sentence(sentence):
    """æ–‡ã‚’æ§‹é€ çš„ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡"""
    sentence = sentence.strip()
    spec_kw = ["ä»•æ§˜", "ç´ æ", "æ©Ÿèƒ½", "æ€§èƒ½", "ã‚¿ã‚¤ãƒ—", "ãƒ¢ãƒ‡ãƒ«", "ã‚µã‚¤ã‚º"]
    competence_kw = ["ç‰¹é•·", "ç‰¹å¾´", "å¼·ã¿", "é­…åŠ›", "ã“ã ã‚ã‚Š", "è¨­è¨ˆ", "å·¥å¤«"]
    user_kw = ["ã‚ãªãŸ", "æ–¹", "äºº", "å­ä¾›", "å¥³æ€§", "ç”·æ€§", "å­¦ç”Ÿ", "ãƒ“ã‚¸ãƒã‚¹"]
    scene_kw = ["è‡ªå®…", "å¤–å‡º", "æ—…è¡Œ", "ã‚ªãƒ•ã‚£ã‚¹", "é€šå‹¤", "é€šå­¦", "é‹å‹•"]
    benefit_kw = ["ä¾¿åˆ©", "è§£æ±º", "å¿«é©", "ä½¿ã„ã‚„ã™ã„", "å½¹ç«‹ã¤", "å®‰å¿ƒ", "æº€è¶³"]

    if extract_field(sentence, spec_kw): return "spec"
    if extract_field(sentence, competence_kw): return "competence"
    if extract_field(sentence, user_kw): return "user"
    if extract_field(sentence, scene_kw): return "scene"
    if extract_field(sentence, benefit_kw): return "benefit"
    return "misc"

def fill_missing(tags):
    """æ¬ è½è¦ç´ ã‚’è£œå®Œ"""
    defaults = {
        "spec": "é«˜å“è³ªãªè¨­è¨ˆ",
        "competence": "ç´°éƒ¨ã¾ã§ä¸å¯§ã«ä½œã‚‰ã‚ŒãŸæ§‹é€ ",
        "user": "å¹…åºƒã„å¹´ä»£ã®æ–¹",
        "scene": "æ¯æ—¥ã®ç”Ÿæ´»ã‚·ãƒ¼ãƒ³",
        "benefit": "å¿«é©ã§ä¾¿åˆ©ã«ä½¿ãˆã‚‹"
    }
    for k, v in defaults.items():
        if not tags.get(k):
            tags[k] = v
    return tags

def reorder_and_refine(tags):
    """æŒ‡å®šé †åºã§è‡ªç„¶ãªæ–‡æ§‹æˆã«å†æ§‹æˆ"""
    order = ["spec", "competence", "user", "scene", "benefit"]
    structured = "ã€‚".join([tags[k] for k in order if k in tags])
    return re.sub(r"ã€‚ã€‚+", "ã€‚", structured.strip("ã€‚") + "ã€‚")

def adjust_copy_length(copy_text, platform):
    """åª’ä½“åˆ¥æ–‡å­—æ•°èª¿æ•´"""
    copy_text = copy_text.strip()
    if platform == "rakuten":
        max_len, ideal = 87, (60, 80)
    elif platform == "yahoo":
        max_len, ideal = 30, (25, 30)
    else:
        return copy_text

    # æ–‡å­—æ•°èª¿æ•´
    if len(copy_text) > max_len:
        copy_text = copy_text[:max_len]
    elif len(copy_text) < ideal[0]:
        copy_text += "ã€‚ä½¿ã„ã‚„ã™ã•ã‚‚é­…åŠ›ã§ã™"

    return copy_text

def refine_text(copy_raw, alts_raw):
    """æ§‹æ–‡åˆ†é¡â†’è£œå®Œâ†’æ•´å½¢â†’åª’ä½“åˆ¥ã‚³ãƒ”ãƒ¼ä½œæˆ"""
    sentences = re.split(r"[ã€‚ï¼ï¼Ÿ]", copy_raw)
    tags = {cat: "" for cat in ["spec", "competence", "user", "scene", "benefit"]}
    for s in sentences:
        cat = classify_sentence(s)
        if cat in tags and not tags[cat]:
            tags[cat] = s

    tags = fill_missing(tags)
    refined_core = reorder_and_refine(tags)

    rakuten_copy = adjust_copy_length(refined_core, "rakuten")
    yahoo_copy = adjust_copy_length(refined_core, "yahoo")

    refined_alts = []
    for a in alts_raw[:20]:
        alt_text = reorder_and_refine(fill_missing(tags))
        if len(alt_text) > 110:
            alt_text = alt_text[:110]
        refined_alts.append(alt_text)

    return rakuten_copy, yahoo_copy, refined_alts

def main():
    input_dir = "./output/ai_writer"
    output_dir = "./output/refined"
    os.makedirs(output_dir, exist_ok=True)

    latest_file = sorted([f for f in os.listdir(input_dir) if f.endswith(".json")])[-1]
    data = load_json(os.path.join(input_dir, latest_file))

    results = []
    for item in data:
        name = item.get("å•†å“å") or item.get("product_name")
        copy_raw = item.get("copy") or item.get("ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼") or ""
        alts_raw = item.get("alts") or [""]
        rakuten_copy, yahoo_copy, refined_alts = refine_text(copy_raw, alts_raw)

        results.append({
            "å•†å“å": name,
            "æ¥½å¤©ç”¨ã‚³ãƒ”ãƒ¼": rakuten_copy,
            "Yahooç”¨ã‚³ãƒ”ãƒ¼": yahoo_copy,
            "ALTæ•°": len(refined_alts),
            "ALTä¾‹": refined_alts[0] if refined_alts else ""
        })

    now = datetime.now().strftime("%Y%m%d_%H%M")
    json_path = os.path.join(output_dir, f"refined_copy_alt_{now}.json")
    csv_path = os.path.join(output_dir, f"refined_copy_alt_{now}.csv")

    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    pd.DataFrame(results).to_csv(csv_path, index=False, encoding="utf-8-sig")

    print(f"âœ… å‡ºåŠ›å®Œäº†: {json_path}")
    print(f"âœ… CSVå‡ºåŠ›: {csv_path}")

if __name__ == "__main__":
    print("ğŸŒ¸ KOTOHA Local Refiner v1.0 èµ·å‹•")
    main()
import atlas_autosave_core
