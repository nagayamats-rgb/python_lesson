"""
ğŸŒ¸ KOTOHA ENGINE v1.2 - market_enricher.py
--------------------------------------------
ç›®çš„:
- query_merger.py ã®å‡ºåŠ›ï¼ˆmergedã‚¯ã‚¨ãƒªï¼‰ã‚’ä½¿ã£ã¦å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
- æ¥½å¤© & Yahoo! å•†å“æ¤œç´¢APIã‚’åˆ©ç”¨ã—ã¦èªå½™ãƒ»å…±èµ·èªã‚’æŠ½å‡º
- AIã‚’ä½¿ã‚ãšã« "å¸‚å ´è¾æ›¸" ã‚’ç”Ÿæˆ
"""

import os
import re
import json
import time
import csv
import glob
import logging
import pandas as pd
import requests
from datetime import datetime
from dotenv import load_dotenv
from collections import Counter

# ----------------------------
# ğŸŒ¸ ãƒ­ã‚¬ãƒ¼è¨­å®š
# ----------------------------
logger = logging.getLogger("KOTOHA_MARKET_ENRICHER")
if not logger.handlers:
    os.makedirs("logs", exist_ok=True)
    fh = logging.FileHandler(f"logs/market_enricher_{datetime.now().strftime('%Y%m%d')}.log", encoding="utf-8")
    sh = logging.StreamHandler()
    fmt = logging.Formatter("[%(asctime)s] %(levelname)s: %(message)s")
    fh.setFormatter(fmt); sh.setFormatter(fmt)
    logger.addHandler(fh); logger.addHandler(sh)
logger.setLevel(logging.INFO)

# ----------------------------
# âš™ï¸ è¨­å®šãƒ­ãƒ¼ãƒ‰
# ----------------------------
def load_configs():
    load_dotenv(".env.txt")
    rakuten_key = os.getenv("RAKUTEN_APP_ID")
    yahoo_key = os.getenv("YAHOO_APP_ID")

    if not rakuten_key or not yahoo_key:
        raise EnvironmentError("âŒ æ¥½å¤©ã¾ãŸã¯Yahooã®APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

    return {
        "rakuten_url": os.getenv("RAKUTEN_API_BASE_URL", "https://app.rakuten.co.jp/services/api/IchibaItem/Search/20220601"),
        "yahoo_url": os.getenv("YAHOO_API_BASE_URL", "https://shopping.yahooapis.jp/ShoppingWebService/V3/itemSearch"),
        "rakuten_key": rakuten_key,
        "yahoo_key": yahoo_key,
        "sleep_sec": 1.2,
        "max_hits": 10,
        "output_dir": "./"
    }

# ----------------------------
# ğŸ” APIå‘¼ã³å‡ºã—
# ----------------------------
def fetch_rakuten(keyword, cfg):
    try:
        params = {
            "applicationId": cfg["rakuten_key"],
            "keyword": keyword,
            "hits": cfg["max_hits"],
            "format": "json"
        }
        res = requests.get(cfg["rakuten_url"], params=params, timeout=10)
        if res.status_code != 200:
            return []
        data = res.json().get("Items", [])
        words = []
        for it in data:
            item = it.get("Item", {})
            text = " ".join([
                str(item.get("itemName", "")),
                str(item.get("catchcopy", "")),
                str(item.get("itemCaption", ""))
            ])
            words.extend(extract_keywords(text))
        return words
    except Exception as e:
        logger.warning(f"âš ï¸ Rakutenå–å¾—å¤±æ•—ï¼ˆ{keyword}ï¼‰: {e}")
        return []

def fetch_yahoo(keyword, cfg):
    try:
        params = {
            "appid": cfg["yahoo_key"],
            "query": keyword,
            "results": cfg["max_hits"]
        }
        res = requests.get(cfg["yahoo_url"], params=params, timeout=10)
        if res.status_code != 200:
            return []
        data = res.json().get("hits", [])
        words = []
        for it in data:
            text = " ".join([
                str(it.get("name", "")),
                str(it.get("headline", "")),
                str(it.get("description", ""))
            ])
            words.extend(extract_keywords(text))
        return words
    except Exception as e:
        logger.warning(f"âš ï¸ Yahooå–å¾—å¤±æ•—ï¼ˆ{keyword}ï¼‰: {e}")
        return []

# ----------------------------
# ğŸ§© ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆç°¡æ˜“æ—¥æœ¬èªå‡¦ç†ï¼‰
# ----------------------------
def extract_keywords(text):
    text = re.sub(r"[!-/:-@[-`{-~]", " ", text)  # åŠè§’è¨˜å·
    text = re.sub(r"[ï¼-ï¼™ï¼¡-ï¼ºï½-ï½š]", " ", text)  # å…¨è§’è‹±æ•°
    text = re.sub(r"\s+", " ", text)
    # 2æ–‡å­—ä»¥ä¸Šã®æ—¥æœ¬èªå˜èªã‚’æŠ½å‡º
    words = re.findall(r"[ä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ³ãƒ¼]{2,}", text)
    # ãƒã‚¤ã‚ºé™¤å»
    stopwords = ["é€æ–™ç„¡æ–™", "ãƒã‚¤ãƒ³ãƒˆ", "ã‚»ãƒ¼ãƒ«", "ç¨è¾¼", "å…¬å¼", "äººæ°—", "é™å®š"]
    return [w for w in words if w not in stopwords and len(w) < 12]

# ----------------------------
# ğŸ“Š é›†è¨ˆãƒ»ã‚¹ã‚³ã‚¢åŒ–
# ----------------------------
def summarize_vocab(vocab_dict):
    all_words = []
    for product, words in vocab_dict.items():
        all_words.extend(words)
    freq = Counter(all_words)
    df = pd.DataFrame(freq.items(), columns=["word", "count"]).sort_values("count", ascending=False)
    return df

# ----------------------------
# ğŸš€ ãƒ¡ã‚¤ãƒ³
# ----------------------------
def main():
    logger.info("ğŸŒ¸ KOTOHA ENGINE â€” Market Enricher èµ·å‹•")
    cfg = load_configs()

    files = sorted(glob.glob("query_batches_merged_*.jsonl"))
    if not files:
        logger.error("âŒ query_batches_merged_*.jsonl ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚query_merger.py ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return

    input_file = files[-1]
    logger.info(f"ğŸ“„ å…¥åŠ›: {input_file}")

    # èªå½™è¾æ›¸ {product_name: [words...]}
    vocab_dict = {}

    with open(input_file, "r", encoding="utf-8") as f:
        for line in f:
            data = json.loads(line)
            pname = data["representative_name"]
            queries = data["queries"]
            all_words = []

            for q in queries:
                words_r = fetch_rakuten(q, cfg)
                words_y = fetch_yahoo(q, cfg)
                all_words.extend(words_r + words_y)
                time.sleep(cfg["sleep_sec"])

            vocab_dict[pname] = all_words
            logger.info(f"ğŸ“¦ {pname}: {len(all_words)}èªåé›† ({len(set(all_words))}ç¨®é¡)")

    # ------------------ å‡ºåŠ› ------------------
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    json_path = f"market_vocab_{ts}.json"
    csv_path = f"market_vocab_summary_{ts}.csv"
    enriched_path = f"market_enriched_{ts}.csv"

    # è©³ç´°èªå½™JSON
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(vocab_dict, f, ensure_ascii=False, indent=2)

    # é »åº¦é›†è¨ˆ
    vocab_summary = summarize_vocab(vocab_dict)
    vocab_summary.to_csv(csv_path, index=False, encoding="utf-8-sig", quoting=csv.QUOTE_MINIMAL)

    # å•†å“å˜ä½ã§ä¸Šä½èªã‚’ä¿å­˜
    rows = []
    for pname, words in vocab_dict.items():
        c = Counter(words)
        top_words = [w for w, _ in c.most_common(20)]
        rows.append({"å•†å“å": pname, "å¸‚å ´èªå½™_TOP20": "|".join(top_words)})

    pd.DataFrame(rows).to_csv(enriched_path, index=False, encoding="utf-8-sig")

    logger.info(f"ğŸ’¾ è©³ç´°è¾æ›¸: {json_path}")
    logger.info(f"ğŸ’¾ èªå½™é »åº¦é›†è¨ˆ: {csv_path}")
    logger.info(f"ğŸ’¾ å•†å“åˆ¥èªå½™: {enriched_path}")
    logger.info(f"âœ… å®Œäº†: {len(vocab_dict)} å•†å“ç¾¤ã®å¸‚å ´èªå½™ã‚’åé›†ã—ã¾ã—ãŸã€‚")
    logger.info("ğŸ§­ æ¬¡ã¯ ai_writer.py ã§çŸ¥è­˜ï¼‹ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èåˆã—ã¦è‡ªç„¶æ–‡ç”Ÿæˆã¸ã€‚")

if __name__ == "__main__":
    main()
import atlas_autosave_core
