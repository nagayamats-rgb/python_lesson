"""
ğŸŒ¸ KOTOHA ENGINE v1.3 - Market Enricherï¼ˆçµ±åˆå‹ï¼‰
-------------------------------------------------
æ¥½å¤©å¸‚å ´APIã‚’ç”¨ã„ãŸå¸‚å ´èªå½™åé›†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚
å‰å›ã®è¾æ›¸çŠ¶æ…‹ã‚’æ¤œå‡ºã—ã€å·®åˆ†ãŒã‚ã‚Œã°æ›´æ–°ãƒ•ã‚§ãƒƒãƒã‚’è¡Œã†ã€‚
AIã¯ä½¿ç”¨ã›ãšã€ç´”å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§èªå½™è¾æ›¸ã‚’å½¢æˆã™ã‚‹ã€‚
"""

import os
import re
import json
import time
import glob
import hashlib
import logging
import pandas as pd
import requests
from datetime import datetime
from dotenv import load_dotenv
from collections import Counter

# ==========================================================
# ğŸŒ¸ ãƒ­ã‚¬ãƒ¼è¨­å®š
# ==========================================================
logger = logging.getLogger("KOTOHA_MARKET_ENRICHER")
if not logger.handlers:
    os.makedirs("logs", exist_ok=True)
    fh = logging.FileHandler(f"logs/market_enricher_{datetime.now().strftime('%Y%m%d_%H%M')}.log", encoding="utf-8")
    sh = logging.StreamHandler()
    fmt = logging.Formatter("[%(asctime)s] %(levelname)s: %(message)s")
    fh.setFormatter(fmt); sh.setFormatter(fmt)
    logger.addHandler(fh); logger.addHandler(sh)
logger.setLevel(logging.INFO)

# ==========================================================
# âš™ï¸ è¨­å®šãƒ­ãƒ¼ãƒ‰
# ==========================================================
def load_configs():
    load_dotenv(".env.txt")
    rakuten_key = os.getenv("RAKUTEN_APP_ID")

    if not rakuten_key:
        raise EnvironmentError("âŒ RAKUTEN_APP_ID ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

    cfg = {
        "rakuten_url": os.getenv("RAKUTEN_API_BASE_URL", "https://app.rakuten.co.jp/services/api/IchibaItem/Search/20220601"),
        "rakuten_key": rakuten_key,
        "sleep_sec": 1.2,
        "max_hits": 10
    }
    return cfg

# ==========================================================
# ğŸ§© ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
# ==========================================================
def extract_keywords(text):
    text = re.sub(r"[!-/:-@[-`{-~]", " ", text)  # åŠè§’è¨˜å·
    text = re.sub(r"[ï¼-ï¼™ï¼¡-ï¼ºï½-ï½š]", " ", text)  # å…¨è§’è‹±æ•°
    text = re.sub(r"\s+", " ", text)
    words = re.findall(r"[ä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ³ãƒ¼]{2,}", text)
    stop = ["é€æ–™ç„¡æ–™", "ãƒã‚¤ãƒ³ãƒˆ", "å…¬å¼", "äººæ°—", "é™å®š", "ç¨è¾¼", "å®‰å¿ƒ"]
    return [w for w in words if w not in stop and len(w) <= 10]

# ==========================================================
# ğŸ” æ¥½å¤©APIãƒ•ã‚§ãƒƒãƒ
# ==========================================================
def fetch_rakuten(keyword, cfg):
    try:
        params = {
            "applicationId": cfg["rakuten_key"],
            "keyword": keyword,
            "hits": cfg["max_hits"],
            "format": "json"
        }
        res = requests.get(cfg["rakuten_url"], params=params, timeout=10)
        if res.status_code != 200:
            logger.warning(f"âš ï¸ Rakutenãƒ¬ã‚¹ãƒãƒ³ã‚¹ç•°å¸¸({res.status_code}) {keyword}")
            return []
        data = res.json().get("Items", [])
        words = []
        for it in data:
            item = it.get("Item", {})
            text = " ".join([
                str(item.get("itemName", "")),
                str(item.get("catchcopy", "")),
                str(item.get("itemCaption", ""))
            ])
            words.extend(extract_keywords(text))
        return words
    except Exception as e:
        logger.warning(f"âš ï¸ Rakutenå–å¾—å¤±æ•—({keyword}): {e}")
        return []

# ==========================================================
# ğŸ§  å·®åˆ†æ¤œçŸ¥ãƒ­ã‚¸ãƒƒã‚¯
# ==========================================================
def hash_vocab_entry(vocab_entry):
    combined = "|".join(sorted(set(vocab_entry)))
    return hashlib.sha256(combined.encode("utf-8")).hexdigest()

def detect_differences(old_vocab, new_queries):
    old_hashes = {k: hash_vocab_entry(v) for k, v in old_vocab.items()}
    diffs = []
    for product in new_queries:
        if product not in old_hashes:
            diffs.append(product)
    return diffs

# ==========================================================
# ğŸ§© å¸‚å ´èªå½™åé›†ï¼ˆå…¨ä»¶ or å·®åˆ†ï¼‰
# ==========================================================
def collect_vocab(cfg, queries, mode="full", old_vocab=None):
    vocab_dict = {}
    for pname, qlist in queries.items():
        if mode == "diff" and old_vocab and pname in old_vocab:
            continue
        all_words = []
        for q in qlist:
            words = fetch_rakuten(q, cfg)
            all_words.extend(words)
            time.sleep(cfg["sleep_sec"])
        vocab_dict[pname] = all_words
        logger.info(f"ğŸ“¦ {pname}: {len(all_words)}èªåé›†ï¼ˆ{len(set(all_words))}ç¨®é¡ï¼‰")
    return vocab_dict

# ==========================================================
# ğŸ“Š é›†è¨ˆ
# ==========================================================
def summarize_vocab(vocab_dict):
    all_words = []
    for _, words in vocab_dict.items():
        all_words.extend(words)
    freq = Counter(all_words)
    df = pd.DataFrame(freq.items(), columns=["word", "count"]).sort_values("count", ascending=False)
    return df

# ==========================================================
# ğŸš€ ãƒ¡ã‚¤ãƒ³
# ==========================================================
def main():
    logger.info("ğŸŒ¸ KOTOHA ENGINE â€” Market Enricher v1.3 èµ·å‹•")
    cfg = load_configs()

    # === å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ ===
    batch_files = sorted(glob.glob("query_batches_merged_*.jsonl"))
    if not batch_files:
        logger.error("âŒ query_batches_merged_*.jsonl ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return
    input_file = batch_files[-1]
    logger.info(f"ğŸ“„ å…¥åŠ›: {input_file}")

    # === éå»è¾æ›¸ã®æ¤œå‡º ===
    latest_json = "market_vocab_latest.json"
    if os.path.exists(latest_json) and os.path.getsize(latest_json) > 0:
        days_since_update = (time.time() - os.path.getmtime(latest_json)) / 86400
        mode = "diff" if days_since_update < 7 else "full"
        with open(latest_json, "r", encoding="utf-8") as f:
            old_vocab = json.load(f)
        logger.info(f"ğŸ“˜ æ—¢å­˜è¾æ›¸æ¤œå‡º: {latest_json}ï¼ˆæ›´æ–° {days_since_update:.1f} æ—¥å‰ï¼‰â†’ {mode.upper()} ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ")
    else:
        mode = "full"
        old_vocab = {}
        logger.info("ğŸ†• éå»è¾æ›¸ãªã—: åˆå›å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã§é–‹å§‹")

    # === ã‚¯ã‚¨ãƒªèª­ã¿è¾¼ã¿ ===
    queries = {}
    with open(input_file, "r", encoding="utf-8") as f:
        for line in f:
            data = json.loads(line)
            queries[data["representative_name"]] = data["queries"]

    # === å·®åˆ†æ¤œå‡º ===
    if mode == "diff" and old_vocab:
        diff_products = detect_differences(old_vocab, queries)
        if not diff_products:
            logger.info("âœ… å·®åˆ†ãªã—: æœ€æ–°çŠ¶æ…‹ã§ã™ã€‚å®Ÿè¡Œã‚¹ã‚­ãƒƒãƒ—ã€‚")
            return
        logger.info(f"ğŸŸ¡ å·®åˆ†æ¤œå‡º: {len(diff_products)} å•†å“ç¾¤ã‚’æ›´æ–°å¯¾è±¡ã«é¸æŠã€‚")
        queries = {k: v for k, v in queries.items() if k in diff_products}

    # === å¸‚å ´èªå½™åé›† ===
    new_vocab = collect_vocab(cfg, queries, mode=mode, old_vocab=old_vocab)

    # === çµ±åˆæ›´æ–° ===
    merged_vocab = old_vocab.copy()
    merged_vocab.update(new_vocab)

    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    latest_path = "market_vocab_latest.json"
    diff_path = f"market_vocab_diff_{ts}.json"
    enriched_csv = f"market_enriched_{ts}.csv"

    with open(latest_path, "w", encoding="utf-8") as f:
        json.dump(merged_vocab, f, ensure_ascii=False, indent=2)
    with open(diff_path, "w", encoding="utf-8") as f:
        json.dump(new_vocab, f, ensure_ascii=False, indent=2)

    # === é›†è¨ˆã¨å‡ºåŠ› ===
    df_sum = summarize_vocab(merged_vocab)
    rows = [{"å•†å“å": pname, "å¸‚å ´èªå½™_TOP20": "|".join([w for w, _ in Counter(words).most_common(20)])}
            for pname, words in merged_vocab.items()]
    pd.DataFrame(rows).to_csv(enriched_csv, index=False, encoding="utf-8-sig")

    logger.info(f"ğŸ’¾ æ›´æ–°æ¸ˆã¿çµ±åˆè¾æ›¸: {latest_path}")
    logger.info(f"ğŸ’¾ å·®åˆ†ãƒ­ã‚°: {diff_path}")
    logger.info(f"ğŸ’¾ çµ±åˆå¸‚å ´èªå½™: {enriched_csv}")
    logger.info("âœ… Market Enricher v1.3 å®Ÿè¡Œå®Œäº†ã€‚")

if __name__ == "__main__":
    main()
import atlas_autosave_core
