#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ðŸŒ¸ KOTOHA ENGINE â€” Quality Filter & Final Export v1.0
Naturalness Scoring + CSV Export Integrator
---------------------------------------------------------
å…¥åŠ›:  ./output/normalized/normalized_*.json
å‡ºåŠ›:  ./output/final/output_final_YYYYMMDD_HHMM.csv
"""

import os
import json
import pandas as pd
import numpy as np
from datetime import datetime
import re
import unicodedata
from tqdm import tqdm

INPUT_JSON_DIR = "./output/normalized"
INPUT_CSV = "./input.csv"
OUTPUT_DIR = "./output/final"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# === ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°è¨­å®š ===
def naturalness_score(text: str) -> float:
    if not text:
        return 0
    text = unicodedata.normalize("NFKC", text)
    length = len(text)
    unique_ratio = len(set(text)) / max(1, len(text))
    has_comma = "ã€" in text or "," in text
    balance = 1.0 - abs(length - 80) / 80  # ALTåŸºæº–80å­—
    score = 0.4 * unique_ratio + 0.4 * balance + (0.2 if has_comma else 0)
    return round(max(0, min(score, 1.0)), 3)

# === è£œå®Œå¥ãƒ‘ã‚¿ãƒ¼ãƒ³ ===
COMPLETION_PHRASES = [
    "ä¸Šè³ªãªä½¿ã„å¿ƒåœ°ã‚’å±Šã‘ã¾ã™ã€‚",
    "ç†ã«ã‹ãªã£ãŸç¾Žã—ã•ã§ã™ã€‚",
    "æ¯Žæ—¥ã‚’æ”¯ãˆã‚‹æ©Ÿèƒ½ã§ã™ã€‚",
    "æš®ã‚‰ã—ã«å¯„ã‚Šæ·»ã†è¨­è¨ˆã§ã™ã€‚",
    "ç´°éƒ¨ã«ã¾ã§èª å®Ÿã•ãŒå®¿ã‚Šã¾ã™ã€‚"
]

def complete_sentence(text: str) -> str:
    text = text.strip()
    if len(text) < 40:
        text += " " + np.random.choice(COMPLETION_PHRASES)
    if not text.endswith("ã€‚"):
        text += "ã€‚"
    return text

# === æœ€æ–°JSONæŽ¢ç´¢ ===
def find_latest_normalized():
    files = [f for f in os.listdir(INPUT_JSON_DIR) if f.startswith("normalized_") and f.endswith(".json")]
    if not files:
        return None
    latest = max(files, key=lambda f: os.path.getmtime(os.path.join(INPUT_JSON_DIR, f)))
    return os.path.join(INPUT_JSON_DIR, latest)

def main():
    json_file = find_latest_normalized()
    if not json_file:
        print("ðŸš« normalized_*.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    with open(json_file, "r", encoding="utf-8") as f:
        clusters = json.load(f)

    print(f"ðŸ“˜ èª­ã¿è¾¼ã¿: {json_file}")
    print(f"ðŸ“Š ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {len(clusters)}")

    # === ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼‹è£œå®Œ ===
    refined = []
    for c in tqdm(clusters, desc="âœ¨ Scoring & Refining"):
        c["catch_copy_score"] = naturalness_score(c.get("catch_copy", ""))
        c["alts_scores"] = [naturalness_score(a) for a in c.get("alts", [])]
        c["catch_copy"] = complete_sentence(c.get("catch_copy", ""))

        # ä½Žã‚¹ã‚³ã‚¢ALTé™¤åŽ»ï¼‹è£œå®Œ
        alts = [a for a, s in zip(c["alts"], c["alts_scores"]) if s > 0.3]
        while len(alts) < 20:
            alts.append(alts[-1] if alts else "ä¸Šè³ªãªä»•ä¸ŠãŒã‚Šã§ã™ã€‚")
        c["alts"] = alts[:20]
        refined.append(c)

    # === DataFrameåŒ– for CSV Export ===
    df = pd.DataFrame(refined)
    alt_cols = [f"ALT{i+1}" for i in range(20)]
    df_csv = pd.DataFrame({
        "å•†å“å": [f"Cluster_{c['cluster_id']}" for c in refined],
        "ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼": [c["catch_copy"] for c in refined],
        **{alt_cols[i]: [c["alts"][i] for c in refined] for i in range(20)},
        "è‡ªç„¶åº¦ã‚¹ã‚³ã‚¢": [round(np.mean(c["alts_scores"]), 3) for c in refined]
    })

    # === å‡ºåŠ› ===
    ts = datetime.now().strftime("%Y%m%d_%H%M")
    json_out = os.path.join(OUTPUT_DIR, f"filtered_normalized_{ts}.json")
    csv_out = os.path.join(OUTPUT_DIR, f"output_final_{ts}.csv")

    with open(json_out, "w", encoding="utf-8") as f:
        json.dump(refined, f, ensure_ascii=False, indent=2)

    df_csv.to_csv(csv_out, index=False, encoding="cp932")

    print(f"\nâœ… Final Export å®Œäº†!")
    print(f"ðŸ“„ JSON: {json_out}")
    print(f"ðŸ“Š CSV : {csv_out}")
    print(f"ðŸ§© å•†å“ç¾¤: {len(refined)} ä»¶")


if __name__ == "__main__":
    main()
import atlas_autosave_core
