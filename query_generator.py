# query_generator.py
"""
ğŸŒ¸ KOTOHA ENGINE v1.0 - query_generator.py
-------------------------------------------
ç›®çš„:
- å•†å“å/ã‚¸ãƒ£ãƒ³ãƒ«IDï¼ˆ+ã‚ã‚Œã°ãƒ†ãƒ³ãƒ—ãƒ¬å‡ºåŠ›ï¼‰ã‹ã‚‰æ¤œç´¢ã‚¯ã‚¨ãƒªå€™è£œã‚’ç”Ÿæˆ
- ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã§ç¢ºå®Ÿã«å‹•ä½œï¼ˆAIãªã—ï¼‰ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
- OPENAI_ENABLE=true ã®å ´åˆã€ä½ã‚³ã‚¹ãƒˆã§è‡ªç„¶ãªã‚¯ã‚¨ãƒªã‚’è¿½åŠ ç”Ÿæˆ
- ä¸­é–“ç”Ÿç”£ç‰©ã‚’æ®µéšä¿å­˜ã—ã¦çµåˆãƒ†ã‚¹ãƒˆ/å†å®Ÿè¡Œã‚’å®¹æ˜“ã«

å…¥å‡ºåŠ›:
- å…¥åŠ›: structured_preview.csv ã¾ãŸã¯æœ€æ–°ã® output_templates_*.csv
- å‡ºåŠ›: query_seeds_*.csv / query_candidates_*.csv / query_batches_*.jsonl / logs/
"""

import os
import re
import csv
import json
import glob
import logging
from datetime import datetime

import pandas as pd
from dotenv import load_dotenv

# OpenAI (ä»»æ„)
OPENAI_OK = False
try:
    from openai import OpenAI  # v1.x
    OPENAI_OK = True
except Exception:
    OPENAI_OK = False

# ----------------------------
# ãƒ­ã‚¬ãƒ¼
# ----------------------------
logger = logging.getLogger("KOTOHA_QUERY")
if not logger.handlers:
    os.makedirs("logs", exist_ok=True)
    fh = logging.FileHandler(f"logs/query_generator_{datetime.now().strftime('%Y%m%d')}.log", encoding="utf-8")
    sh = logging.StreamHandler()
    fmt = logging.Formatter("[%(asctime)s] %(levelname)s: %(message)s")
    fh.setFormatter(fmt); sh.setFormatter(fmt)
    logger.addHandler(fh); logger.addHandler(sh)
logger.setLevel(logging.INFO)

# ----------------------------
# è¨­å®šãƒ­ãƒ¼ãƒ‰
# ----------------------------
def load_configs():
    # æ©Ÿå¯†
    load_dotenv(".env.txt")
    openai_enable = os.getenv("OPENAI_ENABLE", "false").lower() == "true"
    openai_key = os.getenv("OPENAI_API_KEY")

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®š
    if not os.path.exists("kotoha_config.json"):
        raise FileNotFoundError("kotoha_config.json ãŒã‚ã‚Šã¾ã›ã‚“ã€‚init_config.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    with open("kotoha_config.json", "r", encoding="utf-8") as f:
        global_cfg = json.load(f)

    # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®šï¼ˆãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ä½œæˆï¼‰
    mod_path = "config/modules/query_generator.json"
    if not os.path.exists(mod_path):
        os.makedirs("config/modules", exist_ok=True)
        default_cfg = {
            "description": "æ¤œç´¢ã‚¯ã‚¨ãƒªç”Ÿæˆï¼ˆãƒ«ãƒ¼ãƒ«ï¼‹ä»»æ„ã§AIï¼‰",
            "seed_max": 5,
            "candidates_per_item": 16,
            "include_longtail": True,
            "persona_mods": ["åˆå¿ƒè€…å‘ã‘", "ãƒ“ã‚¸ãƒã‚¹ç”¨", "å­¦ç”Ÿå‘ã‘", "ã‚®ãƒ•ãƒˆ", "é«˜è€ä¹…", "è»½é‡", "é™éŸ³", "æ€¥é€Ÿ", "çœã‚¹ãƒšãƒ¼ã‚¹"],
            "scene_mods": ["åœ¨å®…", "å‡ºå¼µ", "é€šå‹¤", "æ—…è¡Œ", "ã‚ªãƒ•ã‚£ã‚¹", "å¯å®¤", "ãƒªãƒ“ãƒ³ã‚°"],
            "generic_mods": ["æ¯”è¼ƒ", "ãŠã™ã™ã‚", "äººæ°—", "ãƒ©ãƒ³ã‚­ãƒ³ã‚°", "ä½¿ã„æ–¹", "å£ã‚³ãƒŸ", "ãƒ¬ãƒ“ãƒ¥ãƒ¼", "é¸ã³æ–¹"]
        }
        with open(mod_path, "w", encoding="utf-8") as f:
            json.dump(default_cfg, f, indent=2, ensure_ascii=False)
        logger.info("ğŸ› ï¸ query_generator.json ã‚’æ–°è¦ä½œæˆã—ã¾ã—ãŸï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šï¼‰")

    with open(mod_path, "r", encoding="utf-8") as f:
        module_cfg = json.load(f)

    output_dir = global_cfg.get("OUTPUT_DIR", "./")
    return openai_enable, openai_key, output_dir, module_cfg

# ----------------------------
# å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«æ±ºå®š
# ----------------------------
def pick_input_file(output_dir="./"):
    tpl = sorted(glob.glob(os.path.join(output_dir, "output_templates_*.csv")))
    if tpl:
        logger.info(f"ğŸ“„ å…¥åŠ›: æœ€æ–°ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ â†’ {tpl[-1]}")
        return tpl[-1]
    sp = os.path.join(output_dir, "structured_preview.csv")
    if os.path.exists(sp):
        logger.info(f"ğŸ“„ å…¥åŠ›: structured_preview.csv ã‚’ä½¿ç”¨ã—ã¾ã™")
        return sp
    raise FileNotFoundError("input ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆstructured_preview.csv / output_templates_*.csvï¼‰")

# ----------------------------
# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆå•†å“åâ†’ç¨®èªï¼‰
# ----------------------------
def extract_seed_keywords(name: str, seed_max=5):
    # ãƒã‚¤ã‚ºé™¤å»
    t = re.sub(r"[ã€ã€‘\[\]\(\)ï¼ˆï¼‰]", " ", str(name))
    t = re.sub(r"[0-9A-Za-z\-_/|:ï¼‹+ï¼Š*]", " ", t)
    t = re.sub(r"\s+", " ", t).strip()

    # æ—¥æœ¬èªèªç‰‡æŠ½å‡ºï¼ˆ2æ–‡å­—ä»¥ä¸Šï¼‰
    words = re.findall(r"[ä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ³ãƒ¼]{2,}", t)
    # é †ç•ªã‚’ä¿ã¡ã¤ã¤é‡è¤‡é™¤å»
    seen, seeds = set(), []
    for w in words:
        if w not in seen:
            seen.add(w)
            seeds.append(w)
        if len(seeds) >= seed_max:
            break
    return seeds or ([t[:6]] if t else [])

# ----------------------------
# ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ã‚¯ã‚¨ãƒªå±•é–‹
# ----------------------------
def expand_queries_rule(seeds, genre, cfg):
    base = list(seeds)
    mods = cfg.get("persona_mods", []) + cfg.get("scene_mods", []) + cfg.get("generic_mods", [])
    genre_part = f" {genre}" if genre else ""

    cand = set()

    # 1èª/2èª/ä¿®é£¾èªã®çµ„ã¿åˆã‚ã›
    for s in base:
        cand.add(s + genre_part)
        for m in mods:
            cand.add(f"{s} {m}")
            cand.add(f"{s}{genre_part} {m}")

    # ãƒ­ãƒ³ã‚°ãƒ†ãƒ¼ãƒ«ï¼ˆçœã‚¨ãƒã§å°‘ã—ã ã‘ï¼‰
    if cfg.get("include_longtail", True):
        heads = ["è²·ã„æ–¹", "æ¯”è¼ƒ", "ãŠã™ã™ã‚", "äººæ°—", "å®‰ã„", "é«˜å“è³ª", "æœ€æ–°", "å‹ç•ª", "ç´”æ­£", "äº’æ›"]
        for s in base:
            for h in heads:
                cand.add(f"{s} {h}")
                if genre:
                    cand.add(f"{s} {genre} {h}")

    # ç°¡æ˜“ã‚¹ã‚³ã‚¢ï¼ˆé•·ã•ã¨é‡è¤‡æ§é™¤ï¼‰ã§ä¸¦ã³æ›¿ãˆ
    scored = sorted(list(cand), key=lambda x: (len(x), x))
    # ãƒˆãƒƒãƒ—N
    N = max(10, cfg.get("candidates_per_item", 16))
    return scored[:N]

# ----------------------------
# AIã§è‡ªç„¶ãªã‚¯ã‚¨ãƒªã‚’è¿½åŠ ï¼ˆä»»æ„ï¼‰
# ----------------------------
def expand_queries_ai(client, name, genre, seeds, want=8):
    prompt = (
        "ä»¥ä¸‹ã®æƒ…å ±ã‹ã‚‰ã€æ—¥æœ¬äººãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå®Ÿéš›ã«æ¤œç´¢ã—ãã†ãªè‡ªç„¶ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç®‡æ¡æ›¸ãã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n"
        "ãƒ»å•†å“åã®ç‰¹å¾´èª\n"
        f"ãƒ»ã‚¸ãƒ£ãƒ³ãƒ«: {genre or 'ä¸æ˜'}\n"
        f"ãƒ»æŠ½å‡ºæ¸ˆã¿ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(seeds)}\n"
        f"æ¡ä»¶: 7ã€œ12å€‹ã€æ—¥æœ¬èªã€æ¤œç´¢ã«å®Ÿéš›ã«ä½¿ã„ãã†ãªçŸ­ã„èªå¥ã€è¨˜å·ãªã—ã€é‡è¤‡ã—ãªã„\n"
        "å‡ºåŠ›: å„è¡Œ1ã‚¯ã‚¨ãƒªã®ã¿\n"
    )
    try:
        res = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "system", "content": "ã‚ãªãŸã¯æ—¥æœ¬èªSEOã®å°‚é–€å®¶ã§ã™ã€‚"}, {"role": "user", "content": prompt}],
            temperature=0.5,
            max_tokens=180
        )
        text = res.choices[0].message.content.strip()
        lines = [re.sub(r"^[\-\d\.\sã€ãƒ»]+", "", ln).strip() for ln in text.splitlines()]
        lines = [ln for ln in lines if ln and len(ln) <= 30]
        # ä¸Šé™èª¿æ•´
        uniq, seen = [], set()
        for q in lines:
            if q not in seen:
                seen.add(q)
                uniq.append(q)
            if len(uniq) >= want:
                break
        return uniq
    except Exception as e:
        logger.warning(f"âš ï¸ OpenAIç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆ{e}ï¼‰")
        return []

# ----------------------------
# ãƒ¡ã‚¤ãƒ³
# ----------------------------
def main():
    try:
        openai_enable, openai_key, output_dir, cfg = load_configs()
    except Exception as e:
        logger.error(f"è¨­å®šèª­è¾¼ã‚¨ãƒ©ãƒ¼: {e}")
        return

    try:
        input_path = pick_input_file(output_dir)
        df = pd.read_csv(input_path, dtype=str).fillna("")
    except Exception as e:
        logger.error(f"å…¥åŠ›èª­è¾¼ã‚¨ãƒ©ãƒ¼: {e}")
        return

    # åˆ—åï¼ˆã‚ãªãŸã®CSVæº–æ‹ ï¼‰
    NAME, GENRE = "å•†å“å", "ã‚¸ãƒ£ãƒ³ãƒ«ID"

    # å‡ºåŠ›ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    seeds_path = os.path.join(output_dir, f"query_seeds_{ts}.csv")
    cands_path = os.path.join(output_dir, f"query_candidates_{ts}.csv")
    batch_path = os.path.join(output_dir, f"query_batches_{ts}.jsonl")

    # OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆä»»æ„ï¼‰
    client = None
    if openai_enable and OPENAI_OK and openai_key:
        client = OpenAI(api_key=openai_key)
        logger.info("ğŸ¤ OpenAIã‚¯ã‚¨ãƒªå±•é–‹ã‚’æœ‰åŠ¹åŒ–ï¼ˆä½ã‚³ã‚¹ãƒˆï¼‰")
    else:
        if openai_enable and not OPENAI_OK:
            logger.warning("âš ï¸ openai SDK(v1) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚AIå±•é–‹ã¯ç„¡åŠ¹åŒ–ã—ã¾ã™ã€‚")
        logger.info("ğŸª„ ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ã¿ã§ã‚¯ã‚¨ãƒªç”Ÿæˆã—ã¾ã™ï¼ˆAIã‚³ã‚¹ãƒˆ0ï¼‰")

    # ç”Ÿæˆçµæœæ ¼ç´
    seeds_rows = []
    cands_rows = []
    batch_items = []

    for _, row in df.iterrows():
        name = str(row.get(NAME, "")).strip()
        genre = str(row.get(GENRE, "")).strip()
        if not name:
            continue

        seeds = extract_seed_keywords(name, seed_max=cfg.get("seed_max", 5))
        seeds_rows.append({
            "å•†å“å": name,
            "ã‚¸ãƒ£ãƒ³ãƒ«ID": genre,
            "seed_keywords": "|".join(seeds)
        })

        # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹å€™è£œ
        rule_qs = expand_queries_rule(seeds, genre, cfg)

        # AIè¿½åŠ å€™è£œ
        ai_qs = expand_queries_ai(client, name, genre, seeds, want=max(4, cfg.get("candidates_per_item", 16)//3)) if client else []

        # ãƒãƒ¼ã‚¸ & é‡è¤‡æ’é™¤
        merged, seen = [], set()
        for q in rule_qs + ai_qs:
            if q not in seen:
                seen.add(q)
                merged.append(q)

        # cands rowï¼ˆå›ºå®šåˆ— 1..20ï¼‰
        record = {"å•†å“å": name, "ã‚¸ãƒ£ãƒ³ãƒ«ID": genre}
        for i in range(1, 21):
            record[f"Q{i}"] = merged[i-1] if i-1 < len(merged) else ""
        cands_rows.append(record)

        # ãƒãƒƒãƒï¼ˆAPIå‘¼ã³å‡ºã—ç”¨ï¼‰
        batch_items.append({
            "item_name": name,
            "genre_id": genre,
            "queries": merged
        })

    # -------- ä¿å­˜ï¼ˆä¸­é–“ç”Ÿç”£ç‰©ï¼‰ --------
    pd.DataFrame(seeds_rows).to_csv(seeds_path, index=False, encoding="utf-8-sig", quoting=csv.QUOTE_MINIMAL)
    pd.DataFrame(cands_rows).to_csv(cands_path, index=False, encoding="utf-8-sig", quoting=csv.QUOTE_MINIMAL)
    with open(batch_path, "w", encoding="utf-8") as f:
        for item in batch_items:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")

    logger.info(f"ğŸ’¾ ç¨®èªã‚’å‡ºåŠ›: {seeds_path}")
    logger.info(f"ğŸ’¾ å€™è£œã‚¯ã‚¨ãƒªã‚’å‡ºåŠ›: {cands_path}")
    logger.info(f"ğŸ’¾ ãƒãƒƒãƒï¼ˆJSONLï¼‰ã‚’å‡ºåŠ›: {batch_path}")
    logger.info(f"âœ… å®Œäº†: {len(cands_rows)} ä»¶")
    logger.info("ğŸ§­ æ¬¡ã¯ market_enricher.py ã§å¤–éƒ¨APIã¸æ³¨å…¥ï¼ˆæ¥½å¤©/Yahooï¼‰ã—ã¾ã™ã€‚")

if __name__ == "__main__":
    logger.info("ğŸŒ¸ KOTOHA ENGINE â€” Query Generator èµ·å‹•")
    main()
import atlas_autosave_core
