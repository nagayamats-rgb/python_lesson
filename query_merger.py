"""
ğŸŒ¸ KOTOHA ENGINE v1.1 - query_merger.py
--------------------------------------------
ç›®çš„:
- query_generator.py ã®å‡ºåŠ›ã‚’çµ±åˆï¼ˆã‚«ãƒ©ãƒãƒª/é‡è¤‡å•†å“ã‚’ãƒãƒ¼ã‚¸ï¼‰
- å•†å“å˜ä½ã§ä¸€æ„ãªæ¤œç´¢ã‚¯ã‚¨ãƒªç¾¤ã‚’æ§‹ç¯‰
- æ¬¡å·¥ç¨‹ï¼ˆmarket_enricherï¼‰ã¸ã®ãƒãƒƒãƒå…¥åŠ›ã‚’ç”Ÿæˆ
"""

import os
import re
import csv
import glob
import json
import logging
import pandas as pd
from datetime import datetime

# ----------------------------
# ğŸŒ¸ ãƒ­ã‚¬ãƒ¼
# ----------------------------
logger = logging.getLogger("KOTOHA_QUERY_MERGER")
if not logger.handlers:
    os.makedirs("logs", exist_ok=True)
    fh = logging.FileHandler(f"logs/query_merger_{datetime.now().strftime('%Y%m%d')}.log", encoding="utf-8")
    sh = logging.StreamHandler()
    fmt = logging.Formatter("[%(asctime)s] %(levelname)s: %(message)s")
    fh.setFormatter(fmt); sh.setFormatter(fmt)
    logger.addHandler(fh); logger.addHandler(sh)
logger.setLevel(logging.INFO)

# ----------------------------
# ğŸ” é¡ä¼¼åˆ¤å®šï¼ˆã‚«ãƒ©ãƒãƒªçµ±åˆã®ãŸã‚ï¼‰
# ----------------------------
def normalize_name(name: str) -> str:
    """ãƒã‚¤ã‚ºé™¤å»ã—ã€æ¯”è¼ƒç”¨ã®æ­£è¦åŒ–æ–‡å­—åˆ—ã‚’ç”Ÿæˆ"""
    if not name:
        return ""
    t = str(name)
    t = re.sub(r"[ã€ã€‘\[\]\(\)ï¼ˆï¼‰]", " ", t)
    t = re.sub(r"[0-9A-Za-z\-_/|:ï¼‹+ï¼Š*ï¼…%]", " ", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t

def is_variant(base: str, target: str) -> bool:
    """å•†å“åã®é¡ä¼¼åº¦ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆå…ˆé ­15æ–‡å­—ãŒä¸€è‡´ã™ã‚Œã°åŒç³»çµ±ã¨ã¿ãªã™ï¼‰"""
    if not base or not target:
        return False
    nb, nt = normalize_name(base), normalize_name(target)
    return nb[:15] == nt[:15] or nb in nt or nt in nb

# ----------------------------
# ğŸ“¦ çµ±åˆå‡¦ç†
# ----------------------------
def merge_queries(df):
    merged = []
    seen = set()
    groups = []

    for _, row in df.iterrows():
        name = row.get("å•†å“å", "")
        genre = row.get("ã‚¸ãƒ£ãƒ³ãƒ«ID", "")
        if not name:
            continue

        # ã‚¯ã‚¨ãƒªåˆ—
        qs = [str(row.get(f"Q{i}", "")).strip() for i in range(1, 21) if str(row.get(f"Q{i}", "")).strip()]
        if not qs:
            continue

        # æ—¢å­˜ã‚°ãƒ«ãƒ¼ãƒ—ã¨ã®ãƒãƒƒãƒ
        matched = None
        for g in groups:
            if is_variant(g["name"], name) or (genre and g["genre"] == genre):
                matched = g
                break

        if matched:
            matched["queries"].update(qs)
            matched["names"].add(name)
        else:
            groups.append({"name": name, "genre": genre, "queries": set(qs), "names": {name}})

    for g in groups:
        merged.append({
            "ä»£è¡¨å•†å“å": list(g["names"])[0],
            "ã‚¸ãƒ£ãƒ³ãƒ«ID": g["genre"],
            "é–¢é€£å•†å“æ•°": len(g["names"]),
            **{f"Q{i}": q for i, q in enumerate(sorted(g["queries"]), start=1)}
        })

    return pd.DataFrame(merged)

# ----------------------------
# ğŸš€ ãƒ¡ã‚¤ãƒ³
# ----------------------------
def main():
    logger.info("ğŸŒ¸ KOTOHA ENGINE â€” Query Merger èµ·å‹•")

    files = sorted(glob.glob("query_candidates_*.csv"))
    if not files:
        logger.error("âŒ query_candidates_*.csv ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚query_generator.py ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return

    input_file = files[-1]
    logger.info(f"ğŸ“„ å…¥åŠ›: {input_file}")

    df = pd.read_csv(input_file, dtype=str).fillna("")

    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
    raw_backup = "query_candidates_raw.csv"
    df.to_csv(raw_backup, index=False, encoding="utf-8-sig", quoting=csv.QUOTE_MINIMAL)
    logger.info(f"ğŸ’¾ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {raw_backup}")

    # ãƒãƒ¼ã‚¸å®Ÿè¡Œ
    merged_df = merge_queries(df)

    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    merged_csv = f"query_candidates_merged_{ts}.csv"
    merged_jsonl = f"query_batches_merged_{ts}.jsonl"

    merged_df.to_csv(merged_csv, index=False, encoding="utf-8-sig", quoting=csv.QUOTE_MINIMAL)

    # JSONLå½¢å¼ï¼ˆAPIãƒãƒƒãƒç”¨ï¼‰
    with open(merged_jsonl, "w", encoding="utf-8") as f:
        for _, row in merged_df.iterrows():
            qlist = [str(row.get(f"Q{i}", "")).strip() for i in range(1, 51) if str(row.get(f"Q{i}", "")).strip()]
            if not qlist:
                continue
            f.write(json.dumps({
                "representative_name": row.get("ä»£è¡¨å•†å“å", ""),
                "genre_id": row.get("ã‚¸ãƒ£ãƒ³ãƒ«ID", ""),
                "queries": qlist
            }, ensure_ascii=False) + "\n")

    logger.info(f"ğŸ’¾ ãƒãƒ¼ã‚¸æ¸ˆã¿CSV: {merged_csv}")
    logger.info(f"ğŸ’¾ APIãƒãƒƒãƒJSONL: {merged_jsonl}")
    logger.info(f"âœ… çµ±åˆå®Œäº†: {len(merged_df)} å•†å“ç¾¤ã«æ•´ç†ã•ã‚Œã¾ã—ãŸã€‚")
    logger.info("ğŸ§­ æ¬¡ã¯ market_enricher.py ã§å¤–éƒ¨APIæ³¨å…¥ã¸é€²ã‚ã¾ã™ã€‚")

if __name__ == "__main__":
    main()
import atlas_autosave_core
