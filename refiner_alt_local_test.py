import csv, json, os, re, random
from statistics import mean

# === ãƒ‘ã‚¹è¨­å®š ===
BASE_DIR = "/Users/tsuyoshi/Desktop/python_lesson"
INPUT_CSV  = f"{BASE_DIR}/alt_text_20251101_test.csv"
SEM_DIR    = f"{BASE_DIR}/output/semantics"
OUTPUT_DIR = f"{BASE_DIR}/output/refined"
OUTPUT_CSV = f"{OUTPUT_DIR}/alt_text_refined_test_v3.csv"

os.makedirs(OUTPUT_DIR, exist_ok=True)

# === JSONçŸ¥è¦‹ ===
def load_json(path):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {}

knowledge = {}
for fn in os.listdir(SEM_DIR):
    if fn.endswith(".json"):
        knowledge[fn.split(".")[0]] = load_json(os.path.join(SEM_DIR, fn))

# === ãƒ­ãƒ¼ã‚«ãƒ«èªå½™æŠ½å‡º ===
market_vocab = []
for v in knowledge.get("market_vocab_20251030_201906", []):
    if isinstance(v, dict) and "vocabulary" in v:
        market_vocab.append(v["vocabulary"])
core_vocab = set(market_vocab[:80])  # ä¸Šä½èªã‚’å„ªå…ˆä¿æŒ

# === æ­£è¦åŒ–é–¢æ•° ===
def normalize_text(s):
    if not s: return ""
    s = s.replace("\n", " ").replace("\r", " ").strip()
    s = re.sub(r"\s+", " ", s)
    s = s.replace("ã€ã€‚", "ã€‚").replace("ã€‚ã€‚", "ã€‚")
    s = re.sub(r"([ã€‚ã€])\1+", r"\1", s)
    return s

def fix_sentence_end(s):
    """èªå°¾ä¿®æ­£"""
    rules = {
        "ã—ã§ã™": "ã—ã¾ã™", "ã¾ã™ã§ã™": "ã¾ã™", "ã„ã§ã™": "ã„",
        "ã™ã‚‹ã§ã™": "ã—ã¾ã™", "ã‚Œã‚‹ã§ã™": "ã‚Œã¾ã™",
        "ã§ãã§ã™": "ã§ãã¾ã™", "ã§ãã‚‹ã§ã™": "ã§ãã¾ã™"
    }
    for k,v in rules.items():
        s = s.replace(k,v)
    return s

def compress_particles(s):
    """åŠ©è©ã®é‡è¤‡å‰Šé™¤"""
    s = re.sub(r"(ã§){2,}", "ã§", s)
    s = re.sub(r"(ã«){2,}", "ã«", s)
    s = re.sub(r"(ã‚’){2,}", "ã‚’", s)
    s = re.sub(r"(ã—ã¦){2,}", "ã—ã¦", s)
    return s

def simplify_verbs(s):
    """å‹•è©å†—é•·æ§‹æ–‡å‰Šé™¤ï¼ˆè»½é‡ï¼‰"""
    s = re.sub(r"ã§ãã‚‹ä½¿ã„ã‚„ã™ã„è¨­è¨ˆ", "ä½¿ã„ã‚„ã™ã„è¨­è¨ˆ", s)
    s = re.sub(r"ã§ãã‚‹ç°¡å˜æ“ä½œ", "ç°¡å˜æ“ä½œ", s)
    return s

def noun_stop_transform(s):
    """15%ç¢ºç‡ã§åè©çµ‚æ­¢åŒ–"""
    if random.random() < 0.15:
        s = re.sub(r"(ã¾ã™|ã§ã™)ã€‚$", "ã€‚", s)
        if not re.search(r"[ã€‚]$", s):
            s += "ã€‚"
    return s

def seo_filter(s):
    """ä½é »åº¦èªå‰Šé™¤"""
    words = re.split(r"(?<=ã€‚)|(?<=ã€)", s)
    filtered = []
    for w in words:
        if not any(k in w for k in core_vocab):
            # é »åº¦ä½ã„æ–‡ç¯€ã§ã‚‚å‰Šé™¤ã—ã™ããªã„ï¼ˆå®‰å…¨ç‡ï¼‰
            if len(w) > 8:
                filtered.append(w)
        else:
            filtered.append(w)
    return "".join(filtered)

def cleanse_text(text):
    t = normalize_text(text)
    t = fix_sentence_end(t)
    t = compress_particles(t)
    t = simplify_verbs(t)
    t = seo_filter(t)
    t = noun_stop_transform(t)
    if not t.endswith("ã€‚"):
        t += "ã€‚"
    return t

# === ãƒ¡ã‚¤ãƒ³å‡¦ç† ===
rows = []
with open(INPUT_CSV, "r", encoding="utf-8") as f:
    reader = csv.DictReader(f)
    fields = reader.fieldnames
    for row in reader:
        new_row = row.copy()
        for i in range(1, 21):
            col = f"ALT{i}"
            if col in new_row and new_row[col]:
                new_row[col] = cleanse_text(new_row[col])
        rows.append(new_row)

os.makedirs(OUTPUT_DIR, exist_ok=True)
with open(OUTPUT_CSV, "w", encoding="utf-8", newline="") as f:
    writer = csv.DictWriter(f, fieldnames=fields)
    writer.writeheader()
    writer.writerows(rows)

# === è©•ä¾¡ãƒ­ã‚° ===
lengths = [len(r[c]) for r in rows for c in r if c.startswith("ALT") and r[c]]
bad = [r[c] for r in rows for c in r if c.startswith("ALT") and "ã—ã§ã™" in r[c]]
avg_len = round(mean(lengths), 1) if lengths else 0

print("ğŸŒ¸ ALTãƒ­ãƒ¼ã‚«ãƒ«ãƒªãƒ•ã‚¡ã‚¤ãƒ³ v3 å®Œäº†")
print(f"âœ… å‡ºåŠ›: {OUTPUT_CSV}")
print(f"ğŸ“¦ å•†å“æ•°: {len(rows)}")
print(f"ğŸ“ å¹³å‡æ–‡å­—æ•°: {avg_len}ï¼ˆ{min(lengths)}ã€œ{max(lengths)}ï¼‰")
print(f"ğŸ’¬ èªå°¾å´©ã‚Œä¿®æ­£æ¤œå‡º: {len(bad)}ä»¶")
print(f"ğŸ·ï¸ å‚ç…§èªå½™: {len(core_vocab)}ä»¶ï¼ˆmarket_vocabãƒ™ãƒ¼ã‚¹ï¼‰")
import atlas_autosave_core
