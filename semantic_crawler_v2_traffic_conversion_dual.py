# -*- coding: utf-8 -*-
"""
semantic_crawler_v2_traffic_conversion_dual.py
- ç›®çš„:
  1) rakuten.csv / yahoo.csv ã®å•†å“åã‹ã‚‰æƒ³èµ·ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ
  2) æ¥½å¤©API / Yahoo APIï¼ˆå¿…è¦ãªã‚‰HTMLãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰ã§ 7ã€œ15ãƒšãƒ¼ã‚¸ç›¸å½“ã‚’åé›†
  3) å½¢æ…‹ç´ è§£æ + TF-IDF ã§é »å‡ºèªæŠ½å‡º
  4) Trafficèª / Conversionèª ã«äºŒå±¤åˆ†é¡ã—ã¦JSONå‡ºåŠ›
- å…¥åŠ›:
  /Users/tsuyoshi/Desktop/python_lesson/sauce/rakuten.csv  ï¼ˆUTF-8, ãƒ˜ãƒƒãƒ€: å•†å“åï¼‰
  /Users/tsuyoshi/Desktop/python_lesson/sauce/yahoo.csv    ï¼ˆUTF-8, ãƒ˜ãƒƒãƒ€: å•†å“å or nameï¼‰
- è¨­å®š: .envï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã§å›ºå®šï¼‰
  RAKUTEN_API_BASE_URL
  RAKUTEN_APP_ID
  YAHOO_API_BASE_URL
  YAHOO_APP_ID
  OPENAI_*ï¼ˆæœªä½¿ç”¨å¯ï¼‰
- å‡ºåŠ›:
  ./output/semantics/raw_crawl_{timestamp}.json  â€¦ ç´ æï¼‹é›†è¨ˆ
  ./output/semantics/traffic_conversion_{timestamp}.json â€¦ äºŒå±¤èªå½™ã‚»ãƒƒãƒˆï¼ˆãƒ©ã‚¤ã‚¿ãƒ¼å–ã‚Šè¾¼ã¿å‘ã‘ï¼‰
"""

import os
import re
import csv
import json
import time
import math
import random
import pathlib
from datetime import datetime
from collections import Counter, defaultdict

# -------------------------
# ä¾å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®è‡ªå·±è§£æ±º
# -------------------------
def _ensure(mod, pip_name=None):
    try:
        __import__(mod)
    except Exception:
        import subprocess, sys
        subprocess.run([sys.executable, "-m", "pip", "install", pip_name or mod, "-q"], check=False)

_ensure("requests")
_ensure("beautifulsoup4", "beautifulsoup4")
_ensure("tqdm")
_ensure("python-dotenv", "python-dotenv")
_ensure("janome")
_ensure("scikit-learn", "scikit-learn")

import requests
from bs4 import BeautifulSoup
from tqdm import tqdm
from dotenv import load_dotenv
from janome.tokenizer import Tokenizer
from sklearn.feature_extraction.text import TfidfVectorizer

# -------------------------
# å®šæ•°ãƒ»ãƒ‘ã‚¹
# -------------------------
BASE = pathlib.Path(__file__).resolve().parent
SAUCE = BASE / "sauce"
INPUT_RAKUTEN = SAUCE / "rakuten.csv"
INPUT_YAHOO   = SAUCE / "yahoo.csv"
OUT_DIR = BASE / "output" / "semantics"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# .envèª­è¾¼
load_dotenv(override=True)
RAKUTEN_API = os.getenv("RAKUTEN_API_BASE_URL", "").strip()
RAKUTEN_APP = os.getenv("RAKUTEN_APP_ID", "").strip()
YAHOO_API   = os.getenv("YAHOO_API_BASE_URL", "").strip()
YAHOO_APP   = os.getenv("YAHOO_APP_ID", "").strip()

# åé›†ãƒ¬ãƒ³ã‚¸ï¼ˆæ¤œç´¢7ã€œ15ãƒšãƒ¼ã‚¸ç›¸å½“ï¼‰
RAKUTEN_PAGES = list(range(7, 16))   # page=7..15
YAHOO_PAGES   = list(range(7, 16))   # start=(page-1)*10+1, results=10

# -------------------------
# å‰å‡¦ç†ãƒ»ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# -------------------------
JP_DIGIT = "ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™"
EN_DIGIT = "0123456789"
D2Z = str.maketrans(EN_DIGIT, JP_DIGIT)
Z2D = str.maketrans(JP_DIGIT, EN_DIGIT)

STOPWORDS = set("""
ã“ã‚Œ ãã‚Œ ã‚ã‚Œ ã“ã“ ãã“ ã‚ãã“ ç§ ã‚ãªãŸ ã™ã‚‹ ãªã‚‹ ã„ã‚‹ ã‚ã‚‹ ãŸã‚
ãã—ã¦ ã¾ãŸ ã®ã§ ãŒ ã¨ ã« ã¸ ã§ ã‚’ ã¯ ã‚‚ ã® ã‚ˆã‚Š ã‚„ ã‹ã‚‰ ã¾ã§ ã«ã¦
ã§ã™ ã¾ã™ ã§ã—ãŸ ã§ã—ãŸã‚‰ ã¾ã›ã‚“ ã§ã—ãŸãŒ ã§ãã‚‹ ã§ã
""".split())

# ãƒ«ãƒ¼ãƒ«ã‚·ãƒ¼ãƒ‰ï¼ˆåˆæœŸåˆ†é¡ã®è»¸ï¼‰
TRAFFIC_SEEDS = set("""
iPhone iPad Android Galaxy Xperia Pixel Apple Watch AirPods MagSafe
ã‚±ãƒ¼ã‚¹ ãƒ•ã‚£ãƒ«ãƒ  ã‚¬ãƒ©ã‚¹ å……é›» ã‚±ãƒ¼ãƒ–ãƒ« ã‚¢ãƒ€ãƒ—ã‚¿ ãƒ‰ãƒƒã‚¯ ãƒ¯ã‚¤ãƒ¤ãƒ¬ã‚¹ PD QC3.0
USB Type-C Lightning microUSB 15W 20W 30W 60W 100W 3A 5A 10Gbps
é˜²æ°´ é˜²å¡µ è€è¡æ’ƒ è€ä¹… æŠ—èŒ è–„å‹ è»½é‡ ãƒã‚°ãƒãƒƒãƒˆ æŠ˜ã‚ŠãŸãŸã¿ ã‚¹ã‚¿ãƒ³ãƒ‰
""".split())

CONVERSION_SEEDS = set("""
äººæ°— å£²ã‚Œç­‹ é«˜è©•ä¾¡ å®‰å¿ƒ å…¬å¼ è¿”å“ä¿è¨¼ ã‚®ãƒ•ãƒˆ ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ ãŠã™ã™ã‚ ä¾¿åˆ©
ãƒ“ã‚¸ãƒã‚¹ ç”¨é€” å¹…åºƒã„ ã‚·ãƒ¼ãƒ³ æ—…è¡Œ å‡ºå¼µ åœ¨å®… ãƒ¯ãƒ¼ã‚¯ å­¦ç”Ÿ
å¿«é© ä½¿ã„ã‚„ã™ã„ é•·æŒã¡ çœã‚¹ãƒšãƒ¼ã‚¹ ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆ é™éŸ³ é«˜å“è³ª
""".split())

def normalize_text(s: str) -> str:
    s = (s or "").strip()
    s = re.sub(r"\s+", " ", s)
    return s

def read_products(path: pathlib.Path) -> list:
    if not path.exists():
        return []
    products = []
    with path.open("r", encoding="utf-8", newline="") as f:
        reader = csv.DictReader(f)
        headers = [h.strip() for h in (reader.fieldnames or [])]
        key = "å•†å“å" if "å•†å“å" in headers else ("name" if "name" in headers else None)
        if not key:
            return []
        for r in reader:
            nm = normalize_text(r.get(key, ""))
            if nm:
                products.append(nm)
    # é‡è¤‡é™¤å»ï¼ˆé †åºç¶­æŒï¼‰
    seen, uniq = set(), []
    for nm in products:
        if nm not in seen:
            uniq.append(nm)
            seen.add(nm)
    return uniq

def generate_queries(product: str) -> list:
    """
    å•†å“åã‹ã‚‰æƒ³èµ·ã‚¯ã‚¨ãƒªã‚’æ•°æœ¬ç”Ÿæˆï¼ˆéå‰°ã«ãªã‚‰ãªã„ç¯„å›²ï¼‰ã€‚
    - ãƒ¢ãƒ‡ãƒ«å/å®¹é‡/è‰²ã£ã½ã„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŠ½å‡ºã—ã¤ã¤åŸå‹ã‚’æ®‹ã™
    """
    base = product
    # å‹ç•ª/æ•°å­—/å®¹é‡/ã‚¤ãƒ³ãƒ/è‰²ã®å€™è£œæŠ½å‡ºï¼ˆç·©ã‚ï¼‰
    toks = re.findall(r"[A-Za-z0-9\-+]+|[ï¼-ï¼™]+|[0-9]+(W|w|A|a|mm|MM|gb|GB|G|g|inch|ã‚¤ãƒ³ãƒ)?", product)
    toks = [t.translate(Z2D) for t in toks if t]
    keybits = [k for k in toks if len(k) >= 2][:4]
    # ã‚¯ã‚¨ãƒªå€™è£œ
    qs = [base]
    if keybits:
        qs.append(base + " " + " ".join(keybits))
    if len(base) > 12:
        qs.append(base.split()[0])
    # å›ºå®šã®åºƒç¾©èªã‚’æ··ãœã‚‹ï¼ˆéœ²å‡ºæ‹¾ã„ï¼‰
    qs.append(base + " å……é›»")
    qs.append(base + " ã‚±ãƒ¼ã‚¹")
    # ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–
    out, seen = [], set()
    for q in qs:
        qn = normalize_text(q)
        if qn and qn not in seen:
            out.append(qn)
            seen.add(qn)
    return out[:4]

# -------------------------
# æ¥½å¤© / Yahoo API å‘¼ã³å‡ºã—
# -------------------------
def rakuten_search(query: str, page: int, retry=2, timeout=20):
    params = {
        "keyword": query,
        "applicationId": RAKUTEN_APP,
        "page": page,
        "hits": 30,
        "format": "json",
        "sort": "+affiliateRate"  # éœ²å‡ºå¯„ã‚Š
    }
    url = RAKUTEN_API
    for _ in range(retry):
        try:
            r = requests.get(url, params=params, timeout=timeout)
            if r.status_code == 200:
                return r.json()
        except Exception:
            time.sleep(1.5)
    return None

def yahoo_search(query: str, page: int, retry=2, timeout=20):
    # v3 itemSearch: startï¼ˆ1-basedï¼‰, results
    start = (page - 1) * 10 + 1
    params = {
        "appid": YAHOO_APP,
        "query": query,
        "results": 10,
        "start": start,
        "sort": "-score"  # é–¢é€£åº¦
    }
    url = YAHOO_API
    for _ in range(retry):
        try:
            r = requests.get(url, params=params, timeout=timeout)
            if r.status_code == 200:
                return r.json()
        except Exception:
            time.sleep(1.5)
    return None

def fetch_html_text(url: str, timeout=15):
    try:
        r = requests.get(url, timeout=timeout, headers={"User-Agent": "Mozilla/5.0"})
        if r.status_code != 200:
            return ""
        soup = BeautifulSoup(r.text, "html.parser")
        # ã‚¿ã‚¤ãƒˆãƒ« + èª¬æ˜ + è¦‹å‡ºã—
        parts = []
        title = soup.find("title")
        if title: parts.append(title.get_text(" ", strip=True))
        for sel in ["meta[name='description']", "meta[property='og:description']"]:
            m = soup.select_one(sel)
            if m and m.get("content"):
                parts.append(m["content"])
        for tag in soup.find_all(["h1", "h2", "h3", "p", "li"]):
            txt = tag.get_text(" ", strip=True)
            if txt and len(txt) > 5:
                parts.append(txt)
        return " ".join(parts)
    except Exception:
        return ""

# -------------------------
# å½¢æ…‹ç´ è§£æãƒ»èªå½™æŠ½å‡º
# -------------------------
_tokenizer = Tokenizer()

def tokenize(text: str):
    words = []
    for t in _tokenizer.tokenize(text):
        base = t.base_form if t.base_form != "*" else t.surface
        base = base.strip()
        if not base or base in STOPWORDS:
            continue
        # è¨˜å·ãƒ»è‹±å­—ã®ã¿ç­‰ã‚’æŠ‘åˆ¶
        if re.fullmatch(r"[\W_]+", base):
            continue
        words.append(base)
    return words

def tfidf_top_terms(sentences, top_k=200):
    if not sentences:
        return []
    vec = TfidfVectorizer(tokenizer=tokenize, max_df=0.9, min_df=2)
    try:
        X = vec.fit_transform(sentences)
    except Exception:
        return []
    scores = X.sum(axis=0).A1
    terms = vec.get_feature_names_out()
    pairs = list(zip(terms, scores))
    pairs.sort(key=lambda x: x[1], reverse=True)
    return [w for w, _ in pairs[:top_k]]

def classify_terms(terms):
    """ã‚·ãƒ¼ãƒ‰ï¼‹èªå°¾/å“è©ã£ã½ã„ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã§Traffic/Conversionã«åˆ†é…"""
    traffic, conversion = [], []
    for w in terms:
        w0 = w
        # ã‚·ãƒ¼ãƒ‰ä¸€è‡´
        if w0 in TRAFFIC_SEEDS:
            traffic.append(w0)
            continue
        if w0 in CONVERSION_SEEDS:
            conversion.append(w0)
            continue
        # èªå°¾ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯
        if re.search(r"(å¯¾å¿œ|å……é›»|ä¿è­·|äº’æ›|å–ä»˜|å›ºå®š|æ­è¼‰|å†…è”µ|é˜²æ°´|è€è¡æ’ƒ|è»½é‡|è–„å‹)$", w0):
            traffic.append(w0)
            continue
        if re.search(r"(äººæ°—|å®‰å¿ƒ|å¿«é©|æœ€é©|ä¾¿åˆ©|é«˜å“è³ª|é«˜è©•ä¾¡|ãŠã™ã™ã‚)$", w0):
            conversion.append(w0)
            continue
        # è‹±æ•°å­—ãŒå¤šã„/å˜ä½é¢¨â†’Trafficå¯„ã›
        if re.search(r"[0-9A-Za-z]{2,}", w0):
            traffic.append(w0)
            continue
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Trafficå¯„ã›ï¼ˆéœ²å‡ºç‹™ã„å„ªå…ˆï¼‰
        traffic.append(w0)
    # é‡è¤‡é™¤å»
    traffic = list(dict.fromkeys(traffic))
    conversion = [w for w in dict.fromkeys(conversion) if w not in set(traffic)]
    return traffic, conversion

# -------------------------
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# -------------------------
def main():
    # å…¥åŠ›èª­è¾¼
    rakuten_products = read_products(INPUT_RAKUTEN)
    yahoo_products   = read_products(INPUT_YAHOO)
    products = rakuten_products + [p for p in yahoo_products if p not in set(rakuten_products)]

    if not RAKUTEN_API or not RAKUTEN_APP or not YAHOO_API or not YAHOO_APP:
        raise SystemExit("âŒ .envã®APIè¨­å®šãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚RAKUTEN_* / YAHOO_* ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    print(f"ğŸ” åé›†å¯¾è±¡å•†å“æ•°: {len(products)}")

    # åé›†æœ¬ä½“
    raw_docs = []   # [{"source","query","title","text","url"}...]
    for prod in tqdm(products, desc="ğŸ§² ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°/æ¤œç´¢API", total=len(products)):
        queries = generate_queries(prod)
        for q in queries:
            # æ¥½å¤©API 7..15ãƒšãƒ¼ã‚¸
            for page in RAKUTEN_PAGES:
                data = rakuten_search(q, page)
                if data and isinstance(data, dict):
                    items = (data.get("Items") or [])
                    for it in items:
                        ritem = it.get("Item", {})
                        title = normalize_text(ritem.get("itemName", ""))
                        cap   = normalize_text(ritem.get("itemCaption", ""))
                        url   = normalize_text(ritem.get("itemUrl", ""))
                        text  = " ".join([title, cap])
                        if text:
                            raw_docs.append({
                                "source": "rakuten",
                                "query": q,
                                "title": title,
                                "text": text,
                                "url": url
                            })
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§HTMLæ‹¡å¼µï¼ˆè»½ã‚ï¼‰
                        if url and random.random() < 0.08:
                            html_txt = fetch_html_text(url)
                            if html_txt and len(html_txt) > 120:
                                raw_docs.append({
                                    "source": "rakuten_html",
                                    "query": q,
                                    "title": title,
                                    "text": normalize_text(html_txt)[:4000],
                                    "url": url
                                })
            # Yahoo API 7..15ãƒšãƒ¼ã‚¸
            for page in YAHOO_PAGES:
                data = yahoo_search(q, page)
                if data and isinstance(data, dict):
                    hits = data.get("hits") or data.get("items") or []
                    # v3ã¯ "hits" ã‚­ãƒ¼é…åˆ—è¦ç´ ã« title / description / url ãŒå…¥ã‚‹
                    for h in hits:
                        title = normalize_text(h.get("name") or h.get("title") or "")
                        desc  = normalize_text(h.get("description") or h.get("caption") or "")
                        url   = normalize_text(h.get("url") or h.get("link") or "")
                        text  = " ".join([title, desc])
                        if text:
                            raw_docs.append({
                                "source": "yahoo",
                                "query": q,
                                "title": title,
                                "text": text,
                                "url": url
                            })
                        if url and random.random() < 0.08:
                            html_txt = fetch_html_text(url)
                            if html_txt and len(html_txt) > 120:
                                raw_docs.append({
                                    "source": "yahoo_html",
                                    "query": q,
                                    "title": title,
                                    "text": normalize_text(html_txt)[:4000],
                                    "url": url
                                })
        # ã‚¹ãƒ­ãƒƒãƒˆãƒªãƒ³ã‚°ï¼ˆAPIç¤¼å„€ï¼‰
        time.sleep(0.15)

    # æ–‡å˜ä½ã®ç´ æï¼ˆçŸ­æ–‡ã«å‰²ã‚‹ï¼‰
    sentences = []
    for d in raw_docs:
        for seg in re.split(r"[ã€‚!?ï¼ï¼Ÿã€\n]+", d["text"]):
            seg = normalize_text(seg)
            if 10 <= len(seg) <= 180:
                sentences.append(seg)

    # TF-IDFã§ä¸Šä½èªå½™
    top_terms = tfidf_top_terms(sentences, top_k=400)
    traffic_terms, conversion_terms = classify_terms(top_terms)

    # ã¾ã¨ã‚
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    raw_path = OUT_DIR / f"raw_crawl_{ts}.json"
    out_path = OUT_DIR / f"traffic_conversion_{ts}.json"

    payload_raw = {
        "meta": {
            "timestamp": ts,
            "products": len(products),
            "docs": len(raw_docs),
            "sentences": len(sentences),
            "rakuten_api": bool(RAKUTEN_API),
            "yahoo_api": bool(YAHOO_API)
        },
        "documents": raw_docs[:5000],  # ã‚µã‚¤ã‚ºä¿è­·ï¼ˆå¿…è¦ãªã‚‰å¢—ã‚„ã™ï¼‰
    }
    with raw_path.open("w", encoding="utf-8") as f:
        json.dump(payload_raw, f, ensure_ascii=False, indent=2)

    payload_tc = {
        "meta": payload_raw["meta"],
        "traffic_terms": traffic_terms[:200],
        "conversion_terms": conversion_terms[:120],
        "sample_sentences": sentences[:1000],
    }
    with out_path.open("w", encoding="utf-8") as f:
        json.dump(payload_tc, f, ensure_ascii=False, indent=2)

    print(f"âœ… ç´ æå‡ºåŠ›: {raw_path}")
    print(f"âœ… äºŒå±¤èªå½™: {out_path}")
    print("ğŸ¯ ãƒ©ã‚¤ã‚¿ãƒ¼å´ã¯ traffic_conversion_*.json ã® terms + sample_sentences ã‚’ä½¿ã†ã¨åŠ¹æœçš„ã§ã™ã€‚")

if __name__ == "__main__":
    main()
import atlas_autosave_core
