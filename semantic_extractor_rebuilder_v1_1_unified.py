# -*- coding: utf-8 -*-
"""
semantic_extractor_rebuilder_v1_1_unified.py
------------------------------------------------
ç›®çš„:
  1) /sauce/rakuten.csvï¼ˆåˆ—: å•†å“åï¼‰ã¨ /sauce/yahoo.csvï¼ˆåˆ—: nameï¼‰ã‚’ä¸€æ‹¬èª­è¾¼
  2) å•†å“åâ†’é–¢é€£ã‚¯ã‚¨ãƒªç”Ÿæˆï¼ˆOpenAIä»»æ„ã€‚ç„¡ã‘ã‚Œã°ãƒ­ãƒ¼ã‚«ãƒ«è¦å‰‡ã§å®‰å®šå‹•ä½œï¼‰
  3) æ¥½å¤©å•†å“æ¤œç´¢API(20220601) ã‚’ page=7..15, hits=30 ã§ã‚¯ãƒ­ãƒ¼ãƒ«
     â†’ itemName / catchcopy / itemCaption ã®ãƒ†ã‚­ã‚¹ãƒˆåé›†
  4) æ­£è¦åŒ–â†’èªå½™/æ§‹é€ æŠ½å‡ºâ†’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°â†’çŸ¥è¦‹JSONç¾¤ã‚’å‡ºåŠ›
  5) ãƒ©ã‚¤ã‚¿ãƒ¼ãŒä½¿ã„ã‚„ã™ã„æœ€çµ‚â€œèåˆçŸ¥è¦‹â€ JSON ã‚‚ã‚ã‚ã›ã¦å‡ºåŠ›

å…¥å‡ºåŠ›:
  - å…¥åŠ›CSV:
      /Users/tsuyoshi/Desktop/python_lesson/sauce/rakuten.csv   ï¼ˆåˆ—å: å•†å“åï¼‰
      /Users/tsuyoshi/Desktop/python_lesson/sauce/yahoo.csv     ï¼ˆåˆ—å: nameï¼‰
  - .env:
      RAKUTEN_APP_ID=xxxxxxxx
      OPENAI_API_KEY=xxxxxxï¼ˆä»»æ„ï¼‰
      OPENAI_MODEL=gpt-4o ãªã©ï¼ˆä»»æ„ã€‚ç„¡ã‘ã‚Œã° gpt-4o ã‚’æ—¢å®šï¼‰
  - å‡ºåŠ›å…ˆ:
      /Users/tsuyoshi/Desktop/python_lesson/output/semantics
    å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ä¾‹:
      lexical_clusters_YYYYmmdd_HHMMSS.json
      market_vocab_YYYYmmdd_HHMMSS.json
      structured_semantics_YYYYmmdd_HHMMSS.json
      styled_persona_YYYYmmdd_HHMMSS.json
      template_composer.json
      normalized_YYYYmmdd_HHMMSS.json
      knowledge_fused_structured.json     â† ãƒ©ã‚¤ã‚¿ãƒ¼ç›´çµã®æœ€çµ‚èåˆçŸ¥è¦‹
      knowledge_fused_text.txt            â† æ–‡ç« ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‘ã‘ã®çŸ¥è¦‹æ–‡

ä¾å­˜:
  è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: python-dotenv, requests, fugashi, unidic-lite, numpy, scikit-learn
  OpenAIã¯ä»»æ„ï¼ˆå­˜åœ¨æ™‚ã®ã¿ã‚¯ã‚¨ãƒªç”Ÿæˆã§è£œåŠ©ï¼‰
"""

# ========= 0) ä¾å­˜ã®è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« =========
import sys, subprocess, importlib

def _ensure(pkg, pip_name=None, version=None):
    try:
        importlib.import_module(pkg)
    except Exception:
        name = pip_name or pkg
        if version:
            name = f"{name}=={version}"
        print(f"ğŸ“¦ Installing {name} ...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", name])

_ensure("dotenv", "python-dotenv")
_ensure("requests")
_ensure("fugashi")
_ensure("unidic_lite")
_ensure("numpy")
_ensure("sklearn", "scikit-learn")

# OpenAIã¯ä»»æ„
_HAS_OPENAI = True
try:
    from openai import OpenAI
except Exception:
    _HAS_OPENAI = False

# ========= 1) ã‚¤ãƒ³ãƒãƒ¼ãƒˆ & å®šæ•° =========
import os, re, csv, json, time, html, random, unicodedata
from datetime import datetime
from collections import Counter
from dotenv import load_dotenv
import requests
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

BASE_DIR  = "/Users/tsuyoshi/Desktop/python_lesson"
IN_RAKU   = f"{BASE_DIR}/sauce/rakuten.csv"
IN_YAHOO  = f"{BASE_DIR}/sauce/yahoo.csv"
OUT_DIR   = f"{BASE_DIR}/output/semantics"

RAKUTEN_API = "https://app.rakuten.co.jp/services/api/IchibaItem/Search/20220601"

FORBIDDEN_DEFAULT = [
    "ç”»åƒ","å†™çœŸ","è¦‹ãŸç›®","å›³","ä¸Šã®ç”»åƒ","ä¸‹ã®å†™çœŸ",
    "å½“åº—","å½“ç¤¾","ãƒ¬ãƒ“ãƒ¥ãƒ¼","ãƒ©ãƒ³ã‚­ãƒ³ã‚°","ã‚¯ãƒªãƒƒã‚¯","ã“ã¡ã‚‰",
    "ç«¶åˆ","å„ªä½æ€§","æ¥­ç•Œæœ€é«˜","æœ€å®‰","No.1","ãƒŠãƒ³ãƒãƒ¼ãƒ¯ãƒ³","å£²ä¸ŠNo1",
    "ãƒªãƒ³ã‚¯","ãƒšãƒ¼ã‚¸","è³¼å…¥ã¯ã“ã¡ã‚‰","é€æ–™ç„¡æ–™ï¼ˆç¢ºç´„ï¼‰","è¿”é‡‘ä¿è¨¼",
]

# â€œæ§‹é€ â€ã®ãƒ’ãƒ³ãƒˆè»¸ï¼ˆç·©ã„è¾æ›¸ï¼‰
FEATURE_HINTS = ["è€è¡æ’ƒ","é˜²æ°´","é˜²å¡µ","è»½é‡","è–„å‹","é«˜è€ä¹…","ç£åŠ›","æ€¥é€Ÿå……é›»","é«˜é€Ÿ","ä½é…å»¶","çœé›»åŠ›","ä½ç™ºç†±","é™éŸ³","å®‰å®š","å¼·åŒ–ã‚¬ãƒ©ã‚¹","ã‚·ãƒªã‚³ãƒ³","TPU","ã‚¢ãƒ«ãƒŸ","ã‚¹ãƒ†ãƒ³ãƒ¬ã‚¹","æŠ—èŒ","é˜²æ±š"]
SCENE_HINTS   = ["é€šå‹¤","é€šå­¦","æ—…è¡Œ","å‡ºå¼µ","ã‚­ãƒ£ãƒ³ãƒ—","ãƒ“ã‚¸ãƒã‚¹","è‡ªå®…","ã‚ªãƒ•ã‚£ã‚¹","è»Šå†…","ã‚­ãƒƒãƒãƒ³","ã‚¢ã‚¦ãƒˆãƒ‰ã‚¢","ã‚¹ãƒãƒ¼ãƒ„","å‹‰å¼·","ä¼šè­°","ã‚¸ãƒ "]
TARGET_HINTS  = ["å­¦ç”Ÿ","ç¤¾ä¼šäºº","å­ã©ã‚‚","é«˜é½¢è€…","ã‚²ãƒ¼ãƒãƒ¼","ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼","ãƒ“ã‚¸ãƒã‚¹ãƒ‘ãƒ¼ã‚½ãƒ³","ä¸»å©¦","å®¶æ—"]
BENEFIT_HINTS = ["å¿«é©","ä¾¿åˆ©","å®‰å¿ƒ","åŠ¹ç‡åŒ–","çœåŠ›åŒ–","æ™‚çŸ­","æ•´ç†æ•´é “","ä¿è­·","ç¾è¦³","æºå¸¯æ€§","è€ä¹…æ€§","å®‰å®šæ€§","å®‰å…¨æ€§"]

DEVICE_REGEX = re.compile(
    r"(iPhone\s?(?:[0-9]{1,2}|SE|SE2|SE\s?2|SE\s?ç¬¬\dä¸–ä»£)|"
    r"iPad(?:\s?(?:Pro|Air|mini|ç¬¬\dä¸–ä»£))?|"
    r"Apple\s?Watch(?:\s?Series\s?\d+|SE2?|Ultra)?|"
    r"Galaxy\s?[A-Z]?\d+\w*|Xperia\s?\w+|Pixel\s?\d+\w*|"
    r"ä»»å¤©å ‚Switch|Switch|PS5|PS4|AirPods(?:\s?Pro|Max)?)",
    re.IGNORECASE
)

SPECS_REGEXES = [
    re.compile(r"\b(?:USB[- ]?C|Type[- ]?C|Lightning|Micro[- ]?USB)\b", re.IGNORECASE),
    re.compile(r"\b(?:Bluetooth\s?(?:4\.2|5(?:\.0|\.1|\.2|\.3)?)|Wi[- ]?Fi(?:6|6E|7)?)\b", re.IGNORECASE),
    re.compile(r"\b\d{3,5}\s?mAh\b", re.IGNORECASE),
    re.compile(r"\b(?:15W|20W|30W|45W|65W|100W|140W)\b", re.IGNORECASE),
    re.compile(r"\b(?:mm|cm|inch|ã‚¤ãƒ³ãƒ)\b"),
]

def now_tag():
    return datetime.now().strftime("%Y%m%d_%H%M%S")

def ensure_outdir():
    os.makedirs(OUT_DIR, exist_ok=True)

def normalize_text(s: str) -> str:
    if not s:
        return ""
    s = html.unescape(s)
    s = re.sub(r"<br\s*/?>", "ã€‚", s, flags=re.IGNORECASE)
    s = re.sub(r"<.*?>", " ", s)
    s = unicodedata.normalize("NFKC", s)
    s = re.sub(r"\s+", " ", s)
    return s.strip()

def read_csv_column(path, col):
    out = []
    if not os.path.exists(path):
        return out
    with open(path, "r", encoding="utf-8", newline="") as f:
        r = csv.DictReader(f)
        if col not in (r.fieldnames or []):
            return out
        for row in r:
            v = (row.get(col) or "").strip()
            if v:
                out.append(v)
    return out

def uniq_keep(seq):
    seen, out = set(), []
    for x in seq:
        if x not in seen:
            out.append(x); seen.add(x)
    return out

# ========= 2) OpenAI ä»»æ„ =========
def init_openai_client():
    if not _HAS_OPENAI:
        return None, None
    key = os.getenv("OPENAI_API_KEY", "").strip()
    if not key:
        return None, None
    model = os.getenv("OPENAI_MODEL", "gpt-4o").strip() or "gpt-4o"
    client = OpenAI(api_key=key)
    return client, model

def build_queries_local(name: str, k=8):
    base = re.sub(r"[\"'ã€,ã€‚()\[\]ã€ã€‘/\\]+", " ", name)
    base = re.sub(r"\s+", " ", base).strip()
    mods = [" é€æ˜"," è€è¡æ’ƒ"," è–„å‹"," è»½é‡"," æ€¥é€Ÿå……é›»"," ãƒã‚°ãƒãƒƒãƒˆ"," äº’æ›"," ç´”æ­£é¢¨"," é«˜å“è³ª"," ãŠã—ã‚ƒã‚Œ"," ãƒ“ã‚¸ãƒã‚¹"," å­¦ç”Ÿ"]
    seeds = [base] + [base+m for m in mods]
    random.shuffle(seeds)
    return uniq_keep(seeds)[:k]

def build_queries_openai(name: str, client, model: str):
    try:
        res = client.chat.completions.create(
            model=model,
            messages=[
                {"role":"system","content":"ã‚ãªãŸã¯æ—¥æœ¬ã®ECæ¤œç´¢ã«å¼·ã„SEOãƒ—ãƒ©ãƒ³ãƒŠãƒ¼ã§ã™ã€‚"},
                {"role":"user","content": f"å•†å“å: {name}\nã“ã®å•†å“ã¸ã®æµå…¥ã«åŠ¹ãé–¢é€£æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’10å€‹ã€æ—¥æœ¬èªã§ã€‚å‡ºåŠ›ã¯æ”¹è¡ŒåŒºåˆ‡ã‚Šã®ã¿ã€‚"}
            ],
            response_format={"type":"text"},
            max_completion_tokens=300,
            temperature=0.4
        )
        text = (res.choices[0].message.content or "").strip()
        qs = [q.strip("ãƒ»-â€”â— ") for q in text.split("\n") if q.strip()]
        return qs[:10] if qs else build_queries_local(name)
    except Exception:
        return build_queries_local(name)

# ========= 3) æ¥½å¤©APIåé›† =========
def fetch_rakuten_texts(app_id: str, query: str, start_page=7, end_page=15, hits=30, sleep=0.35):
    out = []
    for p in range(start_page, end_page+1):
        params = {
            "applicationId": app_id,
            "format": "json",
            "keyword": query,
            "page": p,
            "hits": hits
        }
        try:
            r = requests.get(RAKUTEN_API, params=params, timeout=20)
            if r.status_code != 200:
                time.sleep(sleep); continue
            data = r.json()
            items = data.get("Items") or []
            for it in items:
                d = it.get("Item") or {}
                for k in ("itemName","catchcopy","itemCaption"):
                    t = normalize_text(d.get(k) or "")
                    if t:
                        out.append(t)
            time.sleep(sleep)
        except Exception:
            time.sleep(sleep)
            continue
    return out

# ========= 4) å½¢æ…‹ç´ ãƒ»æŠ½å‡º =========
from fugashi import Tagger
_TAGGER = Tagger()

def tokenize(text: str):
    return [w.surface for w in _TAGGER(text)]

def split_sentences_jp(s: str):
    s = s.replace("ï¼","ã€‚").replace("ï¼Ÿ","ã€‚")
    parts = [p.strip() for p in s.split("ã€‚") if p.strip()]
    return parts

def pick_market_vocab(texts):
    vocab = set()
    for t in texts:
        for m in DEVICE_REGEX.findall(t):
            vocab.add(unicodedata.normalize("NFKC", m))
        for rx in SPECS_REGEXES:
            for m in rx.findall(t):
                vocab.add(unicodedata.normalize("NFKC", m))
        for m in re.findall(r"\b[A-Z0-9\-]{3,}\b", t, flags=re.IGNORECASE):
            if not m.isdigit():
                vocab.add(unicodedata.normalize("NFKC", m))
    return sorted(vocab)

def extract_semantics(texts):
    features, scenes, targets, benefits = Counter(), Counter(), Counter(), Counter()
    for t in texts:
        for sent in split_sentences_jp(t):
            s = unicodedata.normalize("NFKC", sent)
            for kw in FEATURE_HINTS:
                if kw in s: features[kw]+=1
            for kw in SCENE_HINTS:
                if kw in s: scenes[kw]+=1
            for kw in TARGET_HINTS:
                if kw in s: targets[kw]+=1
            for kw in BENEFIT_HINTS:
                if kw in s: benefits[kw]+=1
    def top(c, n): return [k for k,_ in c.most_common(n)]
    return {
        "features": top(features, 40),
        "scenes":   top(scenes,   30),
        "targets":  top(targets,  30),
        "benefits": top(benefits, 40),
    }

def build_lexical_clusters(texts, n_clusters=8, top_terms=15):
    docs = [t for t in texts if t]
    if not docs:
        return {"clusters":[]}
    try:
        vec = TfidfVectorizer(tokenizer=tokenize, token_pattern=None, min_df=2, max_df=0.8)
        X = vec.fit_transform(docs)
        n = min(n_clusters, max(2, X.shape[0]//10))
        km = KMeans(n_clusters=n, n_init="auto", random_state=42)
        labels = km.fit_predict(X)
        terms = np.array(vec.get_feature_names_out())
        centers = km.cluster_centers_.argsort()[:, ::-1]
        clusters=[]
        for i in range(n):
            idx = centers[i,:top_terms]
            words = [w for w in terms[idx] if len(w)>=2]
            clusters.append({"terms": words})
        return {"clusters": clusters}
    except Exception:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šé »å‡ºèªã§ç­‰åˆ†å‰²
        toks=[]
        for t in docs: toks.extend(tokenize(t))
        freq = [w for w,_ in Counter(toks).most_common(n_clusters*top_terms)]
        chunks = [freq[i:i+top_terms] for i in range(0,len(freq),top_terms)]
        return {"clusters":[{"terms":[w for w in ch if len(w)>=2]} for ch in chunks[:n_clusters]]}

# ========= 5) Persona / Template / Normalized =========
def default_persona():
    return {
        "tone": {
            "style": "è‡ªç„¶ä½“ã§çŸ¥çš„ã€éå‰°ãªè£…é£¾ã‚’é¿ã‘ã‚‹",
            "register": "ä¸å¯§ä½“ã§ç°¡æ½”ã€2æ–‡ä»¥å†…ã‚’åŸºæœ¬",
            "rhythm": "èª­ã¿ã‚„ã™ã•é‡è¦–ã€æ¥ç¶šè©ã®ä¹±ç”¨ã¯é¿ã‘ã‚‹"
        }
    }

def default_templates():
    return {
        "hints":[
            "ã‚¹ãƒšãƒƒã‚¯â†’å¼·ã¿â†’å¯¾è±¡â†’ã‚·ãƒ¼ãƒ³â†’ä¾¿ç›Š",
            "ç´ æ/ä»•æ§˜â†’ä¿è­·/å¿«é©â†’å¯¾è±¡â†’ä½¿ç”¨æ–‡è„ˆâ†’è§£æ±º",
            "äº’æ›/å¯¾å¿œâ†’æ“ä½œæ€§â†’å®‰å…¨/å®‰å¿ƒâ†’å°å…¥åŠ¹æœ"
        ]
    }

def normalized_forbidden():
    return {"forbidden_words": FORBIDDEN_DEFAULT}

# ========= 6) èåˆçŸ¥è¦‹ï¼ˆãƒ©ã‚¤ã‚¿ãƒ¼å‘ã‘æœ€çµ‚JSON/Textï¼‰ =========
def fuse_for_writer(lexical, market_vocab, semantics, persona, templates, normalized):
    # è»½ãä¸Šé™ã‚’è¨­ã‘ã¦éå­¦ç¿’å›é¿
    clusters  = (lexical or {}).get("clusters", [])
    top_terms = []
    for c in clusters:
        ts = c.get("terms") or []
        top_terms.extend([t for t in ts if isinstance(t,str)])
    top_terms = list(dict.fromkeys(top_terms))[:80]

    market = (market_vocab or [])[:80]
    sem    = {
        "features": (semantics.get("features") or [])[:25],
        "scenes":   (semantics.get("scenes")   or [])[:20],
        "targets":  (semantics.get("targets")  or [])[:20],
        "benefits": (semantics.get("benefits") or [])[:25],
    }
    fused = {
        "lexical_terms": top_terms,
        "market_vocab":  market,
        "semantics":     sem,
        "persona":       persona.get("tone", {}),
        "templates":     templates.get("hints", []),
        "forbidden":     (normalized or {}).get("forbidden_words", []),
    }
    return fused

def build_prompt_sentences(fused, aim_min=9, aim_max=14):
    # ãƒ©ã‚¤ã‚¿ãƒ¼ã® system/user ã«ãã®ã¾ã¾æ¸¡ã›ã‚‹â€œè‡ªç„¶æ–‡â€ã®çŸ¥è¦‹æ–‡ã‚»ãƒƒãƒˆ
    def pick(xs, n): return [x for x in xs[:n] if isinstance(x,str)]
    feats = pick(fused["semantics"].get("features",[]), 6)
    scenes= pick(fused["semantics"].get("scenes",[]),   6)
    targs = pick(fused["semantics"].get("targets",[]),  6)
    bens  = pick(fused["semantics"].get("benefits",[]), 6)
    vocab = pick(fused.get("market_vocab",[]),          10)

    sents=[]
    # ãªã‚‹ã¹ãâ€œè‡ªç„¶æ–‡â€ã§çŸ­ã‚ã«ã€‚å¥ç‚¹ã§çµ‚ãˆã‚‹ã€‚
    if feats: sents.append(f"é »å‡ºã™ã‚‹æ©Ÿèƒ½ãƒ»ä»•æ§˜ã¯ã€Œ{ 'ã€'.join(feats[:4]) }ã€ãªã©ã€‚")
    if vocab: sents.append(f"é–¢é€£ã™ã‚‹å‹ç•ªãƒ»è¦æ ¼ãƒ»èªå½™ã«ã¯ã€Œ{ 'ã€'.join(vocab[:6]) }ã€ãŒè¦‹ã‚‰ã‚Œã¾ã™ã€‚")
    if scenes: sents.append(f"ä½¿ç”¨ã‚·ãƒ¼ãƒ³ã¯ã€Œ{ 'ã€'.join(scenes[:4]) }ã€ãŒå¤šãæƒ³å®šã•ã‚Œã¾ã™ã€‚")
    if targs: sents.append(f"å¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã€Œ{ 'ã€'.join(targs[:4]) }ã€ãŒä¸­å¿ƒã§ã™ã€‚")
    if bens:  sents.append(f"è¨´æ±‚ã•ã‚Œã‚‹ä¾¿ç›Šã¯ã€Œ{ 'ã€'.join(bens[:4]) }ã€ãŒç›®ç«‹ã¡ã¾ã™ã€‚")
    sents.append("ç”»åƒæå†™èªã‚„åº—èˆ—ãƒ¡ã‚¿èªã¯ä½¿ç”¨ã—ã¾ã›ã‚“ã€‚")
    sents.append("æ–‡ã¯è‡ªç„¶ãªæ—¥æœ¬èªã§ã€éåº¦ãªç¾…åˆ—ã‚„ãƒ†ãƒ³ãƒ—ãƒ¬è¡¨ç¾ã‚’é¿ã‘ã¾ã™ã€‚")
    sents.append("å¯¾å¿œæ©Ÿç¨®ãƒ»è¦æ ¼ãƒ»ã‚·ãƒ¼ãƒ³ãƒ»ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’â€œè‡ªç„¶ã«â€ç¹”ã‚Šè¾¼ã¿ã¾ã™ã€‚")
    sents.append("ä½“è¨€æ­¢ã‚ã¯å¿…è¦ã«å¿œã˜ã¦è¨±å¯ã€å¥èª­ç‚¹ã¯æ—¥æœ¬èªã®ãƒªã‚ºãƒ ã‚’å„ªå…ˆã—ã¾ã™ã€‚")

    # ç¯„å›²ã«åã‚ã‚‹ï¼ˆä¸è¶³ãªã‚‰è£œè¶³ã€è¶…éãªã‚‰åˆ‡ã‚Šè©°ã‚ï¼‰
    if len(sents) < aim_min:
        sents.append("èª­ã¿ã‚„ã™ã•ã‚’æœ€å„ªå…ˆã—ã€2æ–‡ä»¥å†…ã‚’åŸºæœ¬ã¨ã—ã¾ã™ã€‚")
    return sents[:aim_max]

# ========= 7) æ›¸ãå‡ºã— =========
def write_json(obj, fname):
    ensure_outdir()
    p = os.path.join(OUT_DIR, fname)
    with open(p, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)
    print(f"âœ… {fname}")
    return p

def write_text(lines, fname):
    ensure_outdir()
    p = os.path.join(OUT_DIR, fname)
    with open(p, "w", encoding="utf-8") as f:
        f.write("\n".join(lines).strip() + "\n")
    print(f"âœ… {fname}")
    return p

# ========= 8) ãƒ¡ã‚¤ãƒ³ =========
def main():
    print("ğŸ§© semantic_extractor_rebuilder_v1.1 èµ·å‹•")

    load_dotenv(override=True)
    app_id = os.getenv("RAKUTEN_APP_ID", "").strip()
    if not app_id:
        raise SystemExit("âŒ .env ã« RAKUTEN_APP_ID ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    client, model = init_openai_client()

    # 1) å…¥åŠ›
    rakuten_names = read_csv_column(IN_RAKU,  "å•†å“å")
    yahoo_names   = read_csv_column(IN_YAHOO, "name")
    products = uniq_keep(rakuten_names + yahoo_names)
    print(f"ğŸ“¦ å¯¾è±¡å•†å“æ•°: {len(products)}")

    # 2) åé›†
    corpus = []
    for nm in products:
        qs = build_queries_openai(nm, client, model) if client else build_queries_local(nm)
        qs = uniq_keep(qs)
        for q in qs:
            corpus.extend(fetch_rakuten_texts(app_id, q, start_page=7, end_page=15, hits=30, sleep=0.35))
        time.sleep(0.2)

    corpus = [normalize_text(t) for t in corpus if t]
    corpus = uniq_keep(corpus)
    print(f"ğŸ§¾ åé›†ãƒ†ã‚­ã‚¹ãƒˆæ•°: {len(corpus)}")

    # 3) æŠ½å‡º
    market_vocab = pick_market_vocab(corpus)
    semantics    = extract_semantics(corpus)
    lexical      = build_lexical_clusters(corpus, n_clusters=8, top_terms=18)

    # 4) ä»˜å¸¯ï¼ˆpersona / template / normalizedï¼‰
    persona   = default_persona()
    template  = default_templates()
    normalized= normalized_forbidden()

    # æ—¢å­˜ normalized ãŒã‚ã‚Œã°ãƒãƒ¼ã‚¸
    exist_norm = os.path.join(OUT_DIR, "normalized_20251031_0039.json")
    if os.path.exists(exist_norm):
        try:
            with open(exist_norm, "r", encoding="utf-8") as f:
                ex = json.load(f)
            fw = list({*(ex.get("forbidden_words") or []), *normalized["forbidden_words"]})
            normalized["forbidden_words"] = fw
        except Exception:
            pass

    # 5) æ›¸ãå‡ºã—ï¼ˆæ™‚åˆ»ã‚¿ã‚°ï¼‰
    tag = now_tag()
    write_json(lexical,                  f"lexical_clusters_{tag}.json")
    write_json(market_vocab,             f"market_vocab_{tag}.json")
    write_json(semantics,                f"structured_semantics_{tag}.json")
    write_json(persona,                  f"styled_persona_{tag}.json")
    write_json(template,                 f"template_composer.json")   # ä¸Šæ›¸ãOK
    write_json(normalized,               f"normalized_{tag}.json")

    # 6) æœ€çµ‚â€œèåˆçŸ¥è¦‹â€ï¼ˆãƒ©ã‚¤ã‚¿ãƒ¼ã«ç›´çµï¼‰
    fused = fuse_for_writer(lexical, market_vocab, semantics, persona, template, normalized)
    write_json(fused, "knowledge_fused_structured.json")
    prompt_lines = build_prompt_sentences(fused, aim_min=9, aim_max=14)
    write_text(prompt_lines, "knowledge_fused_text.txt")

    print("ğŸ¯ å®Œäº†: /output/semantics ã«çŸ¥è¦‹JSONã‚’å†æ§‹ç¯‰ã—ã¾ã—ãŸã€‚")
    print("   â†’ ãƒ©ã‚¤ã‚¿ãƒ¼å´ã‹ã‚‰ã¯ knowledge_fused_structured.json / knowledge_fused_text.txt ã‚’èª­ã¿è¾¼ã‚€ã®ãŒæœ€çŸ­ã§ã™ã€‚")

if __name__ == "__main__":
    main()
import atlas_autosave_core
