# -*- coding: utf-8 -*-
"""
semantic_extractor_rebuilder_v1_1_unified_fixed.py
KOTOHA: ãƒ­ãƒ¼ã‚«ãƒ«çŸ¥è¦‹ï¼ˆsemanticsï¼‰å†æ§‹ç¯‰ãƒ¦ãƒ‹ãƒ•ã‚¡ã‚¤ãƒ‰ç‰ˆï¼ˆç’°å¢ƒå›ºå®šå¯¾å¿œï¼‰

â–  ç›®çš„
- /sauce/rakuten.csv ã¨ /sauce/yahoo.csv ã®å•†å“åã‹ã‚‰æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ
- æ¥½å¤©å•†å“æ¤œç´¢API(20220601)ã¸ 7ã€œ15ãƒšãƒ¼ã‚¸ç¯„å›²ã§ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆåé›†
- åé›†ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ä»¥ä¸‹ã®çŸ¥è¦‹JSONç¾¤ã‚’å†æ§‹ç¯‰ã—ã¦ ./output/semantics ã¸å‡ºåŠ›
  - lexical_clusters_YYYYmmdd_HHMMSS.json
  - market_vocab_YYYYmmdd_HHMMSS.json
  - structured_semantics_YYYYmmdd_HHMMSS.json
  - styled_persona_YYYYmmdd_HHMMSS.jsonï¼ˆæ—¢å­˜ãŒã‚ã‚Œã°ç¶™æ‰¿ã€ãªã‘ã‚Œã°æ±ç”¨Personaã‚’ç”Ÿæˆï¼‰
  - normalized_YYYYmmdd_HHMMSS.jsonï¼ˆç¦å‰‡èªãªã©ï¼‰
  - template_composer.jsonï¼ˆæ±ç”¨ãƒ†ãƒ³ãƒ—ãƒ¬éª¨å­ï¼‰
  - knowledge_fused_structured.jsonï¼ˆãƒ©ã‚¤ã‚¿ãƒ¼ãŒèª­ã¿ã‚„ã™ã„çµ±åˆçŸ¥è¦‹ï¼‰
  - knowledge_fused_text.txtï¼ˆäººé–“å¯èª­ã®è¦ç´„ï¼‰

â–  .env å›ºå®šï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã§æº–æ‹ ï¼‰
RAKUTEN_API_BASE_URL=https://app.rakuten.co.jp/services/api/IchibaItem/Search/20220601
RAKUTEN_APP_ID=...
YAHOO_API_BASE_URL=https://shopping.yahooapis.jp/ShoppingWebService/V3/itemSearch
YAHOO_APP_ID=...
OPENAI_API_KEY=...
OPENAI_MODEL="gpt-5"
OPENAI_MODE="chat"
OPENAI_TEMPERATURE="0.9"
OPENAI_MAX_TOKENS="2000"
OPENAI_TOP_P="0.9"
OPENAI_PRESENCE_PENALTY="0.4"
OPENAI_FREQUENCY_PENALTY="0.3"
USE_KOTOHA_PERSONA="ON"

â€» æœ¬ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ OpenAI ã‚’å‘¼ã³ã¾ã›ã‚“ï¼ˆä¸ŠæµçŸ¥è¦‹æ§‹ç¯‰ã®ã¿ï¼‰ã€‚
â€» MOCK_MODE="ON" ã§å®ŸAPIã‚’å©ã‹ãšæ‰‹å…ƒãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ“¬ä¼¼å‡¦ç†ã€‚

"""

import os
import re
import csv
import sys
import json
import time
import glob
import math
import textwrap
import datetime
from collections import Counter, defaultdict

# ========= ä¾å­˜ã®è‡ªå·±è§£æ±º =========
def ensure_packages(pkgs):
    import importlib
    missing = []
    for p in pkgs:
        try:
            importlib.import_module(p)
        except Exception:
            missing.append(p)
    if missing:
        print(f"ğŸ“¦ Installing: {', '.join(missing)} ...")
        import subprocess
        subprocess.check_call([sys.executable, "-m", "pip", "install", *missing])
        print("âœ… Install done.")

ensure_packages([
    "python-dotenv", "requests", "tqdm", "scikit-learn", "numpy"
])

from dotenv import load_dotenv
import requests
from tqdm import tqdm
from sklearn.feature_extraction.text import TfidfVectorizer

# ========= ãƒ‘ã‚¹ & å®šæ•° =========
BASE_DIR = "/Users/tsuyoshi/Desktop/python_lesson"
SAUCE_DIR = os.path.join(BASE_DIR, "sauce")
OUT_DIR = os.path.join(BASE_DIR, "output", "semantics")
os.makedirs(OUT_DIR, exist_ok=True)

# å…¥åŠ›CSVå›ºå®šï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ãƒ«ãƒ¼ãƒ«ï¼‰
RAKUTEN_CSV = os.path.join(SAUCE_DIR, "rakuten.csv")   # UTF-8, ãƒ˜ãƒƒãƒ€ã«ã€Œå•†å“åã€
YAHOO_CSV   = os.path.join(SAUCE_DIR, "yahoo.csv")     # UTF-8, ãƒ˜ãƒƒãƒ€ã«ã€Œnameã€

# åé›†å¯¾è±¡ãƒšãƒ¼ã‚¸ï¼ˆæ¥½å¤©APIï¼‰
RAKUTEN_PAGE_START = 7
RAKUTEN_PAGE_END   = 15
RAKUTEN_HITS       = 30  # 1ãƒšãƒ¼ã‚¸ã‚ãŸã‚Šä»¶æ•°ï¼ˆä¸Šé™ã¯APIä»•æ§˜ã«å¾“ã†ï¼‰

# åé›†ã®ãƒ¬ãƒ¼ãƒˆåˆ¶å¾¡
API_DELAY_SEC = 0.5  # éåº¦ãªè² è·ã‚’é¿ã‘ã‚‹è»½ã‚¹ãƒ­ãƒƒãƒˆãƒªãƒ³ã‚°

# ========= ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =========
def now_ts():
    return datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

def safe_read_csv_rows(path, encoding="utf-8"):
    rows = []
    if not os.path.exists(path):
        return rows
    with open(path, "r", encoding=encoding, newline="") as f:
        reader = csv.DictReader(f)
        for r in reader:
            rows.append(r)
    return rows

def unique_preserve(seq):
    seen = set()
    out = []
    for x in seq:
        if x not in seen:
            out.append(x)
            seen.add(x)
    return out

def normalize_text(s: str) -> str:
    s = (s or "").strip()
    s = re.sub(r"\s+", " ", s)
    # å¤‰ãªåˆ¶å¾¡æ–‡å­—ãªã©é™¤å»
    s = re.sub(r"[\x00-\x08\x0B\x0C\x0E-\x1F]", "", s)
    return s

def split_keywords(name: str):
    """
    å•†å“åã‚’ç´ æœ´ã«ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¦é‡è¦ãã†ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¿”ã™
    - è¨˜å·é™¤å»
    - åŠè§’/å…¨è§’æ•°å­—ãƒ»è‹±å­—ãƒ»ã‚«ã‚¿ã‚«ãƒŠ/ã²ã‚‰ãŒãª/æ¼¢å­—ã‚’å˜ç´”æŠ½å‡º
    """
    s = normalize_text(name)
    s = re.sub(r"[ã€ã€‘\[\]\(\)ï¼ˆï¼‰/ï½œ|ï¼ãƒ»\-â€”_:+ï¼š;ï¼›~ã€œ!ï¼?ï¼Ÿ,ï¼Œã€‚.\u3000]", " ", s)
    toks = [t for t in s.split() if len(t) >= 2]
    return toks[:10]  # å¤šã™ãã‚‹ã¨ãƒã‚¤ã‚ºã«ãªã‚‹ã®ã§ä¸Šä½10èªã«åˆ¶é™

def generate_queries(product_name: str):
    """
    ã‚¯ã‚¨ãƒªç”Ÿæˆï¼šå•†å“åã‹ã‚‰ 2ã€œ3 ãƒ‘ã‚¿ãƒ¼ãƒ³ã»ã©
    - å®Œå…¨å
    - ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸Šä½3ã€œ5èªã‚’ã‚¹ãƒšãƒ¼ã‚¹é€£çµ
    - å‹ç•ªã‚‰ã—ãè‹±æ•°ã‚’å„ªå…ˆ
    """
    toks = split_keywords(product_name)
    full = normalize_text(product_name)
    # å‹ç•ªå€™è£œï¼ˆè‹±æ•°å­—é€£ç¶šï¼‰
    model_like = [t for t in toks if re.search(r"[A-Za-z0-9]{3,}", t)]
    short = " ".join(toks[:5]) if toks else full
    queries = [full]
    if model_like:
        queries.append(" ".join(unique_preserve(model_like[:3])))
    if short and short not in queries:
        queries.append(short)
    return unique_preserve([q for q in queries if q])

def load_persona_if_any():
    """
    æ—¢å­˜ persona ã‚’æ‹¾ã†ã€‚ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’è¿”ã™
    """
    # æ—¢å­˜ã® styled_persona_* ã‚’æ¢ç´¢
    cand = sorted(glob.glob(os.path.join(OUT_DIR, "styled_persona_*.json")), reverse=True)
    if cand:
        try:
            with open(cand[0], "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            pass
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    return {
        "tone": {
            "style": "æ˜å¿«ãƒ»èª å®Ÿãƒ»éå‰°è¡¨ç¾ã‚’é¿ã‘ã‚‹",
            "register": "å¸¸ä½“ã¨æ•¬ä½“ã‚’è‡ªç„¶ã«ä½¿ã„åˆ†ã‘ã‚‹",
            "constraints": [
                "çµµæ–‡å­—ãƒ»ç‰¹æ®Šè¨˜å·ãƒ»ç”»åƒæå†™èªã¯ä½¿ã‚ãªã„",
                "ç«¶åˆæ¯”è¼ƒã‚„èª‡å¤§è¡¨ç¾ã¯é¿ã‘ã‚‹",
                "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç†è§£ã‚’åŠ©ã‘ã‚‹èªå½™ã‚’é¸ã¶"
            ]
        },
        "persona": {
            "role": "SEOã«å¼·ã„æ—¥æœ¬èªã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼",
            "focus": ["è‡ªç„¶æ–‡", "å•†å“ç†è§£", "ã‚µã‚¤ãƒˆå†…SEOæœ€é©åŒ–"]
        }
    }

# ========= .env èª­ã¿è¾¼ã¿ =========
load_dotenv(override=True)
MOCK_MODE = os.getenv("MOCK_MODE", "").strip().upper() == "ON"

RAKUTEN_API_BASE_URL = os.getenv("RAKUTEN_API_BASE_URL", "").strip()
RAKUTEN_APP_ID       = os.getenv("RAKUTEN_APP_ID", "").strip()
YAHOO_API_BASE_URL   = os.getenv("YAHOO_API_BASE_URL", "").strip()
YAHOO_APP_ID         = os.getenv("YAHOO_APP_ID", "").strip()

USE_KOTOHA_PERSONA   = os.getenv("USE_KOTOHA_PERSONA", "ON").strip().upper()

if not RAKUTEN_API_BASE_URL or not RAKUTEN_APP_ID:
    if not MOCK_MODE:
        print("âŒ .env ã®æ¥½å¤©APIè¨­å®šãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚RAKUTEN_API_BASE_URL / RAKUTEN_APP_ID ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)

# ========= ã‚¯ãƒ­ãƒ¼ãƒ©ï¼ˆæ¥½å¤©APIï¼‰ =========
def rakuten_search(keyword: str, page: int, hits: int = RAKUTEN_HITS):
    """
    æ¥½å¤©å•†å“æ¤œç´¢API 20220601
    https://app.rakuten.co.jp/services/api/IchibaItem/Search/20220601
    """
    params = {
        "format": "json",
        "applicationId": RAKUTEN_APP_ID,
        "keyword": keyword,
        "page": page,
        "hits": hits,
        # ä¸¦ã³é †ãªã©å¿…è¦ã«å¿œã˜ã¦
        # "sort": "+reviewCount"
    }
    try:
        r = requests.get(RAKUTEN_API_BASE_URL, params=params, timeout=20)
        r.raise_for_status()
        return r.json()
    except Exception as e:
        return {"error": str(e)}

def extract_texts_from_rakuten_response(data: dict):
    """
    æ¥½å¤©APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆç¾¤ã‚’æŠ½å‡º
    - itemName, itemCaption, catchcopy(ã‚ã‚Œã°), shopName ãªã©
    """
    texts = []
    if not isinstance(data, dict):
        return texts
    items = data.get("Items") or []
    for it in items:
        # 20220601ã¯ {"Item": {...}} æ§‹é€ 
        node = it.get("Item") if isinstance(it, dict) else None
        if not isinstance(node, dict):
            continue
        fields = [
            node.get("itemName"),
            node.get("itemCaption"),
            node.get("catchcopy"),
            node.get("shopName"),
        ]
        for t in fields:
            t = normalize_text(t)
            if t and len(t) > 3:
                texts.append(t)
    return texts

# ========= åé›†ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ =========
def load_product_names():
    r_rows = safe_read_csv_rows(RAKUTEN_CSV, "utf-8")
    y_rows = safe_read_csv_rows(YAHOO_CSV,   "utf-8")
    rak_names = [normalize_text(r.get("å•†å“å") or "") for r in r_rows if (r.get("å•†å“å") or "").strip()]
    yah_names = [normalize_text(r.get("name") or "") for r in y_rows if (r.get("name") or "").strip()]
    prods = unique_preserve(rak_names + yah_names)
    return [p for p in prods if p]

def collect_corpus_from_api(product_names):
    """
    å„å•†å“å -> 2ã€œ3å€‹ã®ã‚¯ã‚¨ãƒª -> æ¥½å¤©API 7ã€œ15ãƒšãƒ¼ã‚¸å·¡å› -> ãƒ†ã‚­ã‚¹ãƒˆåé›†
    MOCK_MODE=ON ã®å ´åˆã¯æ—¢å­˜ semantics ã®ãƒ†ã‚­ã‚¹ãƒˆã‚„å•†å“åã‹ã‚‰åˆæˆ
    """
    corpus = []
    if MOCK_MODE:
        print("ğŸ§ª MOCK_MODE=ON: APIã‚³ãƒ¼ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€ãƒ­ãƒ¼ã‚«ãƒ«ã§ç°¡æ˜“ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’åˆæˆã—ã¾ã™ã€‚")
        for nm in product_names:
            toks = split_keywords(nm)
            base = f"{nm} é«˜å“è³ª ä½¿ã„ã‚„ã™ã„ äº’æ›æ€§ å……é›» ä¾¿åˆ© è»½é‡ è€ä¹…æ€§ æ—¥å¸¸ åˆ©ç”¨ã‚·ãƒ¼ãƒ³ å¤šæ§˜"
            corpus.append(normalize_text(base + " " + " ".join(toks)))
        return corpus

    for nm in tqdm(product_names, desc="ğŸ” æ¥½å¤©APIåé›†", total=len(product_names)):
        queries = generate_queries(nm)
        for q in queries:
            for page in range(RAKUTEN_PAGE_START, RAKUTEN_PAGE_END + 1):
                data = rakuten_search(q, page, RAKUTEN_HITS)
                if "error" in data:
                    # è»½ãå¾…æ©Ÿã—ã¦æ¬¡ã¸
                    time.sleep(API_DELAY_SEC * 2)
                    continue
                texts = extract_texts_from_rakuten_response(data)
                corpus.extend(texts)
                time.sleep(API_DELAY_SEC)
    return corpus

# ========= ç‰¹å¾´æŠ½å‡ºï¼ˆTF-IDFãƒ™ãƒ¼ã‚¹ï¼‰ =========
def build_lexical_clusters(corpus, top_k=300):
    """
    ã‚³ãƒ¼ãƒ‘ã‚¹ã‹ã‚‰é‡è¦èªã‚’æŠ½å‡ºã—ã¦å˜ç´”ãªã‚¯ãƒ©ã‚¹ã‚¿è¡¨ç¾ã‚’ä½œã‚‹
    - æ—¥æœ¬èªã®å½¢æ…‹ç´ è§£æã¯ä½¿ã‚ãšã€N-gramã§è¿‘ä¼¼ï¼ˆ2-gram/3-gramï¼‰
    - TF-IDFä¸Šä½ã‚’ã€Œèªå½™å€™è£œã€ã¨ã—ã¦ clusters=[{"terms":[...]}] ã‚’1ã‚¯ãƒ©ã‚¹ã‚¿è¿”å´
    """
    if not corpus:
        return {"clusters": []}
    vectorizer = TfidfVectorizer(
        analyzer="char_wb",
        ngram_range=(2, 3),
        min_df=2,
        max_features=2000
    )
    X = vectorizer.fit_transform(corpus)
    feats = vectorizer.get_feature_names_out()
    # tf-idf ã®åˆ—æ¯æœ€å¤§å€¤ã§é‡è¦åº¦ã‚’è¿‘ä¼¼
    import numpy as np
    scores = np.asarray(X.max(axis=0)).ravel()
    idx = scores.argsort()[::-1][:top_k]
    terms = [feats[i] for i in idx]
    # ä½™è¨ˆãªã‚¹ãƒšãƒ¼ã‚¹ãƒ»è¨˜å·ã‚’æƒé™¤
    terms = [normalize_text(t) for t in terms if len(t.strip()) >= 2]
    return {"clusters": [{"terms": terms}]}

def build_market_vocab(corpus, top_k=150):
    """
    å¸‚å ´èªå½™ï¼šé »å‡ºã®å˜èª/ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’æŠ½å‡ºï¼ˆç©ºç™½åˆ†å‰²ï¼†n-gramï¼‰
    """
    tokens = []
    for s in corpus:
        s2 = re.sub(r"[ã€ã€‘\[\]\(\)ï¼ˆï¼‰/ï½œ|ï¼ãƒ»\-â€”_:+ï¼š;ï¼›~ã€œ!ï¼?ï¼Ÿ,ï¼Œã€‚.\u3000]", " ", s)
        parts = [p for p in s2.split() if len(p) >= 2]
        tokens.extend(parts)
    freq = Counter(tokens)
    vocab = [w for w, _ in freq.most_common(top_k)]
    return vocab

def build_structured_semantics(corpus):
    """
    æ§‹é€ åŒ–çŸ¥è¦‹ï¼ˆè¶…è»½é‡ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ï¼‰
    - features: æ€§èƒ½/æ©Ÿèƒ½ã«é–¢ã‚ã‚‹èªå½™
    - scenes: åˆ©ç”¨ã‚·ãƒ¼ãƒ³
    - targets: å¯¾è±¡/å¯¾å¿œ
    - benefits: ãƒ™ãƒãƒ•ã‚£ãƒƒãƒˆ
    """
    feats_cue = ["è€ä¹…", "è»½é‡", "è–„å‹", "é«˜é€Ÿ", "æ€¥é€Ÿ", "å¼·åŒ–", "ä¿è­·", "é˜²æ»´", "é˜²æ°´", "é˜²å¡µ", "é™éŸ³", "ä½é…å»¶", "å®‰å®š"]
    scenes_cue = ["é€šå‹¤", "é€šå­¦", "å‡ºå¼µ", "æ—…è¡Œ", "åœ¨å®…", "ãƒ“ã‚¸ãƒã‚¹", "ã‚¹ãƒãƒ¼ãƒ„", "å±‹å¤–", "ã‚ªãƒ•ã‚£ã‚¹", "è‡ªå®…", "å¯å®¤", "ãƒ‡ã‚¹ã‚¯å‘¨ã‚Š"]
    targets_cue = ["iPhone", "Android", "iPad", "Galaxy", "Apple Watch", "MacBook", "ãƒãƒ¼ãƒˆPC", "Switch", "PS5"]
    benefits_cue = ["æ™‚çŸ­", "å¿«é©", "çœã‚¹ãƒšãƒ¼ã‚¹", "çœé›»åŠ›", "æŒã¡é‹ã³ã‚„ã™ã„", "æ‰±ã„ã‚„ã™ã„", "å®‰å¿ƒ", "é•·æŒã¡", "ã‚³ã‚¹ãƒ‘"]

    txt = " ".join(corpus[:5000])  # éå¤§ãªé•·ã•ã‚’é¿ã‘ã‚‹
    def pick(cues):
        hits = [w for w in cues if w in txt]
        return unique_preserve(hits)

    return {
        "concepts": ["å•†å“ã‚¹ãƒšãƒƒã‚¯", "ã‚³ã‚¢ã‚³ãƒ³ãƒ”ã‚¿ãƒ³ã‚¹", "å¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼", "åˆ©ç”¨ã‚·ãƒ¼ãƒ³", "ãƒ™ãƒãƒ•ã‚£ãƒƒãƒˆ"],
        "features": pick(feats_cue),
        "scenes": pick(scenes_cue),
        "targets": pick(targets_cue),
        "benefits": pick(benefits_cue),
    }

def default_normalized():
    return {
        "forbidden_words": [
            "ç”»åƒ", "å†™çœŸ", "è¦‹ãŸç›®", "ä¸Šã®ç”»åƒ", "ä¸‹ã®å†™çœŸ", "å½“åº—", "å½“ç¤¾", "ãƒ¬ãƒ“ãƒ¥ãƒ¼", "ãƒ©ãƒ³ã‚­ãƒ³ã‚°",
            "ã‚¯ãƒªãƒƒã‚¯", "ã“ã¡ã‚‰", "ç«¶åˆ", "å„ªä½æ€§", "æ¥­ç•Œæœ€é«˜", "æœ€å®‰", "No.1", "ãƒŠãƒ³ãƒãƒ¼ãƒ¯ãƒ³", "å£²ä¸ŠNo1",
            "ãƒªãƒ³ã‚¯", "ãƒšãƒ¼ã‚¸", "ã‚«ãƒ¼ãƒˆ", "è³¼å…¥ã¯ã“ã¡ã‚‰", "è¿”é‡‘ä¿è¨¼", "é€æ–™ç„¡æ–™ï¼ˆç¢ºç´„ï¼‰",
        ]
    }

def default_template():
    return {
        "hints": [
            "ã‚¹ãƒšãƒƒã‚¯â†’å¼·ã¿â†’å¯¾è±¡â†’ã‚·ãƒ¼ãƒ³â†’ä¾¿ç›Š",
            "äº’æ›æ€§ãƒ»å¯¾å¿œæ©Ÿç¨®ã®æ˜è¨˜",
            "ã‚µã‚¤ã‚ºæ„Ÿãƒ»é‡é‡ãƒ»ç´ æãªã©ã®å®¢è¦³æƒ…å ±",
            "â€œä½•ãŒã©ã†ä¾¿åˆ©ã‹â€ã‚’1æ–‡ã§ä¼ãˆã‚‹"
        ]
    }

# ========= çµ±åˆçŸ¥è¦‹ï¼ˆãƒ©ã‚¤ã‚¿ãƒ¼å‘ã‘ï¼‰ =========
def fuse_knowledge(lexical, market, structured):
    """
    ãƒ©ã‚¤ã‚¿ãƒ¼ã«æ¸¡ã—ã‚„ã™ã„ â€œæ§‹é€ åŒ–ï¼†ãƒ†ã‚­ã‚¹ãƒˆâ€ ã®2ç³»çµ±ã‚’ä½œã‚‹
    """
    payload = {
        "templates": default_template().get("hints", []),
        "market": market or [],
        "features": structured.get("features", []),
        "scenes": structured.get("scenes", []),
        "targets": structured.get("targets", []),
        "benefits": structured.get("benefits", []),
    }
    # ãƒ†ã‚­ã‚¹ãƒˆç‰ˆ
    lines = []
    if payload["templates"]:
        lines.append("éª¨å­: " + " / ".join(payload["templates"][:3]))
    if payload["market"]:
        lines.append("å¸‚å ´èª: " + "ã€".join(payload["market"][:12]))
    if payload["features"]:
        lines.append("æ©Ÿèƒ½: " + "ã€".join(payload["features"][:10]))
    if payload["targets"]:
        lines.append("å¯¾å¿œ: " + "ã€".join(payload["targets"][:10]))
    if payload["scenes"]:
        lines.append("ã‚·ãƒ¼ãƒ³: " + "ã€".join(payload["scenes"][:10]))
    if payload["benefits"]:
        lines.append("ä¾¿ç›Š: " + "ã€".join(payload["benefits"][:10]))
    text = "ã€‚".join(lines) + ("ã€‚" if lines else "")
    return payload, text

# ========= ãƒ¡ã‚¤ãƒ³ =========
def main():
    print("ğŸ§© semantic_extractor_rebuilder_v1.1ï¼ˆunified fixedï¼‰èµ·å‹•")
    # 1) å…¥åŠ›ãƒ­ãƒ¼ãƒ‰
    products = load_product_names()
    print(f"ğŸ“¦ å¯¾è±¡å•†å“æ•°: {len(products)}")

    # 2) ã‚³ãƒ¼ãƒ‘ã‚¹åé›†
    corpus = collect_corpus_from_api(products)
    print(f"ğŸ§¾ åé›†ãƒ†ã‚­ã‚¹ãƒˆæ•°: {len(corpus)}")

    # 3) ç‰¹å¾´æŠ½å‡º
    lexical = build_lexical_clusters(corpus, top_k=300)
    market  = build_market_vocab(corpus, top_k=150)
    struct  = build_structured_semantics(corpus)
    persona = load_persona_if_any()
    normalz = default_normalized()
    tmpl    = default_template()

    # 4) çµ±åˆçŸ¥è¦‹
    fused_structured, fused_text = fuse_knowledge(lexical, market, struct)

    # 5) å‡ºåŠ›
    ts = now_ts()
    path_lexical = os.path.join(OUT_DIR, f"lexical_clusters_{ts}.json")
    path_market  = os.path.join(OUT_DIR, f"market_vocab_{ts}.json")
    path_struct  = os.path.join(OUT_DIR, f"structured_semantics_{ts}.json")
    path_persona = os.path.join(OUT_DIR, f"styled_persona_{ts}.json")
    path_norm    = os.path.join(OUT_DIR, f"normalized_{ts}.json")
    path_tmpl    = os.path.join(OUT_DIR, "template_composer.json")  # ãƒ†ãƒ³ãƒ—ãƒ¬ã¯å®‰å®šãƒ•ã‚¡ã‚¤ãƒ«ã§æŒã¤
    path_fused_s = os.path.join(OUT_DIR, "knowledge_fused_structured.json")
    path_fused_t = os.path.join(OUT_DIR, "knowledge_fused_text.txt")

    with open(path_lexical, "w", encoding="utf-8") as f:
        json.dump(lexical, f, ensure_ascii=False, indent=2)
    with open(path_market, "w", encoding="utf-8") as f:
        json.dump(market, f, ensure_ascii=False, indent=2)
    with open(path_struct, "w", encoding="utf-8") as f:
        json.dump(struct, f, ensure_ascii=False, indent=2)
    with open(path_persona, "w", encoding="utf-8") as f:
        json.dump(persona, f, ensure_ascii=False, indent=2)
    with open(path_norm, "w", encoding="utf-8") as f:
        json.dump(normalz, f, ensure_ascii=False, indent=2)
    # template_composer ã¯å¸¸ã«ä¸Šæ›¸ãï¼ˆå®‰å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
    with open(path_tmpl, "w", encoding="utf-8") as f:
        json.dump(tmpl, f, ensure_ascii=False, indent=2)
    with open(path_fused_s, "w", encoding="utf-8") as f:
        json.dump(fused_structured, f, ensure_ascii=False, indent=2)
    with open(path_fused_t, "w", encoding="utf-8") as f:
        f.write(fused_text or "")

    print(f"âœ… {os.path.basename(path_lexical)}")
    print(f"âœ… {os.path.basename(path_market)}")
    print(f"âœ… {os.path.basename(path_struct)}")
    print(f"âœ… {os.path.basename(path_persona)}")
    print(f"âœ… {os.path.basename(path_norm)}")
    print(f"âœ… {os.path.basename(path_tmpl)}")
    print(f"âœ… {os.path.basename(path_fused_s)}")
    print(f"âœ… {os.path.basename(path_fused_t)}")
    print("ğŸ¯ å®Œäº†: /output/semantics ã«çŸ¥è¦‹JSONã‚’å†æ§‹ç¯‰ã—ã¾ã—ãŸã€‚")
    print("   â†’ ãƒ©ã‚¤ã‚¿ãƒ¼å´ã‹ã‚‰ã¯ knowledge_fused_structured.json / knowledge_fused_text.txt ã‚’èª­ã¿è¾¼ã‚€ã®ãŒæœ€çŸ­ã§ã™ã€‚")

if __name__ == "__main__":
    main()
import atlas_autosave_core
