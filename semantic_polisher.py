import os
import json
import csv
import logging
from datetime import datetime
from tqdm import tqdm
from dotenv import load_dotenv
from openai import OpenAI, OpenAIError

# ============================================================
# ğŸŒ¸ KOTOHA ENGINE â€” Semantic Polisher v1.0
# ============================================================

logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s: %(message)s',
)

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    logging.error("âŒ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    exit(1)

client = OpenAI(api_key=OPENAI_API_KEY)

INPUT_DIR = "./output/ai_generated"
OUTPUT_DIR = "./output/polished"
LOG_DIR = "./logs"
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)


def find_latest_ai_output():
    files = [
        f for f in os.listdir(INPUT_DIR)
        if f.startswith("ai_generated_") and f.endswith(".json")
    ]
    if not files:
        logging.error("âŒ ai_generated_*.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None
    files.sort(key=lambda f: os.path.getmtime(os.path.join(INPUT_DIR, f)), reverse=True)
    latest = os.path.join(INPUT_DIR, files[0])
    logging.info(f"ğŸ“„ ä½¿ç”¨ãƒ•ã‚¡ã‚¤ãƒ«: {latest}")
    return latest


def refine_texts(cluster):
    """ã‚³ãƒ”ãƒ¼ã¨ALTç¾¤ã‚’è‡ªç„¶ã«å†æ•´å½¢"""
    catch = cluster["catch_copy"]
    alts = cluster["alt_texts"]
    joined = "\n".join([f"- {a}" for a in alts[:10]])

    prompt = f"""
æ¬¡ã®æ—¥æœ¬èªã‚³ãƒ”ãƒ¼ã¨ALTãƒ†ã‚­ã‚¹ãƒˆç¾¤ã‚’ã€äººãŒèª­ã‚“ã§è‡ªç„¶ã§è³¼è²·æ„æ¬²ã‚’å–šèµ·ã™ã‚‹è¡¨ç¾ã«ç£¨ã„ã¦ãã ã•ã„ã€‚
å¥èª­ç‚¹ã®ä½ç½®ã€ãƒªã‚ºãƒ ã€èªæ„Ÿã‚’èª¿æ•´ã—ã€æ–‡ã‚’çŸ­ãä¿ã¡ã€éåº¦ãªä¿®é£¾èªã‚’æ’é™¤ã—ã¾ã™ã€‚

å…¥åŠ›:
ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼:
{catch}

ALTç¾¤ï¼ˆæŠœç²‹ï¼‰:
{joined}

å‡ºåŠ›å½¢å¼:
{{
  "catch_copy": "ä¿®æ­£æ¸ˆã¿ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼",
  "alt_texts": ["ä¿®æ­£ALT1", "ä¿®æ­£ALT2", ...]
}}
"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯ç†Ÿç·´ã®æ—¥æœ¬èªã‚³ãƒ”ãƒ¼ã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼ã§ã™ã€‚"},
                {"role": "user", "content": prompt},
            ],
            temperature=0.4,
            max_tokens=800,
        )
        content = response.choices[0].message.content.strip()

        # JSONãƒ‘ãƒ¼ã‚¹ã‚’è©¦ã¿ã‚‹
        start, end = content.find("{"), content.rfind("}")
        if start != -1 and end != -1:
            return json.loads(content[start:end+1])
    except (OpenAIError, json.JSONDecodeError) as e:
        logging.warning(f"âš ï¸ ç£¨ãå‡¦ç†å¤±æ•—: {e}")

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆè»½æ•´å½¢ï¼‰
    refined_alts = [a.replace("ã€‚", "").strip() for a in alts]
    return {"catch_copy": catch.strip(" ã€‚"), "alt_texts": refined_alts}


def main():
    logging.info("ğŸŒ¸ KOTOHA ENGINE â€” Semantic Polisher èµ·å‹•")
    input_file = find_latest_ai_output()
    if not input_file:
        return

    with open(input_file, "r", encoding="utf-8") as f:
        clusters = json.load(f)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    polished_output = []
    raw_log = os.path.join(LOG_DIR, f"semantic_polisher_raw_{timestamp}.txt")

    for cluster in tqdm(clusters, desc="ğŸª ç£¨ãå‡¦ç†ä¸­", unit="cluster"):
        refined = refine_texts(cluster)
        polished_output.append({
            "cluster_id": cluster["cluster_id"],
            "keywords": cluster["keywords"],
            "catch_copy": refined["catch_copy"],
            "alt_texts": refined["alt_texts"],
        })

        with open(raw_log, "a", encoding="utf-8") as logf:
            logf.write(json.dumps(polished_output[-1], ensure_ascii=False))
            logf.write("\n")

    # å‡ºåŠ›
    json_out = os.path.join(OUTPUT_DIR, f"polished_{timestamp}.json")
    csv_out = os.path.join(OUTPUT_DIR, f"polished_{timestamp}.csv")

    with open(json_out, "w", encoding="utf-8") as f:
        json.dump(polished_output, f, ensure_ascii=False, indent=2)

    with open(csv_out, "w", encoding="utf-8", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["cluster_id", "keywords", "catch_copy", "alt_texts"])
        for o in polished_output:
            writer.writerow([
                o["cluster_id"],
                ", ".join(o["keywords"]),
                o["catch_copy"],
                "; ".join(o["alt_texts"]),
            ])

    logging.info(f"âœ… ç£¨ãå®Œäº†: {len(polished_output)}ä»¶")
    logging.info(f"ğŸ’¾ å‡ºåŠ›: {json_out}")
    logging.info(f"ğŸ’¾ CSVå‡ºåŠ›: {csv_out}")
    print("\nğŸ¨ KOTOHA ENGINE Semantic Polisher å®Œäº† â€” è¨€è‘‰ãŒç£¨ã‹ã‚Œã¾ã—ãŸã€‚\n")


if __name__ == "__main__":
    main()
import atlas_autosave_core
