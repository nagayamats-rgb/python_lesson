import os
import re
import json
import unicodedata
import pandas as pd
from datetime import datetime
from tqdm import tqdm

# ===============================
# ğŸŒ¸ KOTOHA ENGINE â€” Semantic Polisher v2.1 Pro
# ===============================

def count_zenkaku(s: str) -> int:
    """å…¨è§’æ–‡å­—æ•°ã‚«ã‚¦ãƒ³ãƒˆ"""
    return sum(2 if unicodedata.east_asian_width(ch) in "FWA" else 1 for ch in s) // 2

def normalize_text(text: str) -> str:
    """å¥èª­ç‚¹çµ±ä¸€ãƒ»ç¦å‰‡å‡¦ç†"""
    text = re.sub(r"[ï¼!]", "", text)
    text = re.sub(r"[ã€‚]+", "ã€‚", text)
    text = re.sub(r"\s+", " ", text).strip()
    if not text.endswith("ã€‚"):
        text += "ã€‚"
    return text

def generate_alt_variants(keywords):
    """ALTãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆï¼ˆ80ã€œ110å­—ï¼‰"""
    base_phrases = [
        "{core} {feature} {scene} {value}",
        "{scene} {core} {feature} {value}",
        "{core} ã® {feature} {scene} {value}",
        "{core} {scene} ã«æœ€é©ãª {feature} {value}",
    ]

    # ã‚µãƒ–èªå½™å€™è£œ
    value_terms = ["ä¾¿åˆ©", "å¿«é©", "é«˜å“è³ª", "äººæ°—", "å¤šæ©Ÿèƒ½", "é«˜è©•ä¾¡", "è€ä¹…æ€§æŠœç¾¤", "ãƒ‡ã‚¶ã‚¤ãƒ³æ€§ãŒé«˜ã„", "æŒã¡é‹ã³ã‚„ã™ã„", "ã‚®ãƒ•ãƒˆã«ã‚‚æœ€é©"]
    scene_terms = ["è‡ªå®…", "ã‚ªãƒ•ã‚£ã‚¹", "å¤–å‡ºå…ˆ", "æ—…è¡Œ", "ãƒ“ã‚¸ãƒã‚¹", "æ—¥å¸¸", "é€šå‹¤", "å‹‰å¼·ä¸­", "å°±å¯å‰", "å®¶æ—æ™‚é–“"]
    feature_terms = ["è»½é‡è¨­è¨ˆ", "æ€¥é€Ÿå……é›»", "é˜²æ°´ä»•æ§˜", "ã‚¹ãƒªãƒ ãƒœãƒ‡ã‚£", "è¡æ’ƒå¸å", "æ»‘ã‚Šæ­¢ã‚åŠ å·¥", "æ”¾ç†±è¨­è¨ˆ", "æŸ”ã‚‰ã‹ç´ æ", "å®‰å®šæ„ŸæŠœç¾¤", "é•·æŒã¡ãƒãƒƒãƒ†ãƒªãƒ¼"]

    alts = []
    for i in range(20):
        core = keywords[0] if keywords else "å•†å“"
        feature = feature_terms[i % len(feature_terms)]
        scene = scene_terms[i % len(scene_terms)]
        value = value_terms[i % len(value_terms)]
        template = base_phrases[i % len(base_phrases)]
        alt = template.format(core=core, feature=feature, scene=scene, value=value)
        alt = normalize_text(alt)
        # æ–‡å­—æ•°è£œæ­£
        while count_zenkaku(alt) < 80:
            alt += " " + value
        if count_zenkaku(alt) > 110:
            alt = alt[:110]
            if not alt.endswith("ã€‚"):
                alt += "ã€‚"
        alts.append(alt)
    return alts

def adjust_copy_length(copy_text: str) -> str:
    """ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼ã®é•·ã•èª¿æ•´"""
    copy_text = normalize_text(copy_text)
    l = count_zenkaku(copy_text)
    if l < 40:
        copy_text = copy_text + " " + "å¿«é©ã«ä½¿ãˆã‚‹é«˜å“è³ªãƒ¢ãƒ‡ãƒ«ã€‚"
    elif l > 60:
        copy_text = copy_text[:60]
        if not copy_text.endswith("ã€‚"):
            copy_text += "ã€‚"
    return normalize_text(copy_text)

def main():
    print("ğŸŒ¸ KOTOHA ENGINE â€” Semantic Polisher v2.1 Pro èµ·å‹•")

        # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«è‡ªå‹•æ¤œå‡º
    input_root = "./output"
    candidates = []
    for root, _, files in os.walk(input_root):
        for f in files:
            if f.startswith("polished_") and f.endswith(".json"):
                candidates.append(os.path.join(root, f))

    if not candidates:
        print("ğŸš« polished_*.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    input_path = sorted(candidates)[-1]  # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ
    print(f"ğŸ“„ å…¥åŠ›: {input_path}")


    with open(input_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    out_dir = "./output/polished_v2"
    os.makedirs(out_dir, exist_ok=True)
    output_json = os.path.join(out_dir, f"polished_pro_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    output_csv = os.path.join(out_dir, "polished_final_preview.csv")
    report_path = os.path.join(out_dir, "report_semantic_polisher.txt")

    summary = []
    records = []

    for cluster in tqdm(data, desc="ğŸª Polishing"):
        cid = cluster.get("cluster_id")
        copy_text = adjust_copy_length(cluster.get("catch_copy", ""))
        alt_texts = generate_alt_variants(cluster.get("keywords", []))

        records.append({
            "cluster_id": cid,
            "catch_copy": copy_text,
            "alt_texts": alt_texts
        })

        summary.append({
            "cluster": cid,
            "copy_len": count_zenkaku(copy_text),
            "alts": len(alt_texts),
            "alt_avg_len": round(sum(count_zenkaku(a) for a in alt_texts) / len(alt_texts), 1)
        })

    # JSONä¿å­˜
    with open(output_json, "w", encoding="utf-8") as f:
        json.dump(records, f, ensure_ascii=False, indent=2)

    # CSVãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼å‡ºåŠ›
    flat_rows = []
    for r in records:
        row = {"cluster_id": r["cluster_id"], "catch_copy": r["catch_copy"]}
        for i, a in enumerate(r["alt_texts"], start=1):
            row[f"alt_{i}"] = a
        flat_rows.append(row)
    pd.DataFrame(flat_rows).to_csv(output_csv, index=False, encoding="utf-8-sig")

    # ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
    with open(report_path, "w", encoding="utf-8") as f:
        f.write(f"=== KOTOHA ENGINE Semantic Polisher Report ===\n")
        f.write(f"ç·ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {len(summary)}\n")
        f.write(f"å¹³å‡ã‚­ãƒ£ãƒƒãƒæ–‡å­—æ•°: {sum(s['copy_len'] for s in summary)/len(summary):.1f}\n")
        f.write(f"ALTå¹³å‡æ–‡å­—æ•°: {sum(s['alt_avg_len'] for s in summary)/len(summary):.1f}\n")
        f.write(f"ALTå¹³å‡æ•°: {sum(s['alts'] for s in summary)/len(summary):.1f}\n\n")
        for s in summary:
            f.write(f"cluster {s['cluster']}: copy {s['copy_len']}å­— / ALT {s['alts']}æœ¬ (å¹³å‡{s['alt_avg_len']}å­—)\n")

    print(f"\nâœ… å®Œäº†! Polishedå‡ºåŠ›: {output_json}")
    print(f"ğŸ“Š CSVãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {output_csv}")
    print(f"ğŸ§¾ ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")

if __name__ == "__main__":
    main()
import atlas_autosave_core
