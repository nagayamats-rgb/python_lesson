#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ðŸŒ¸ KOTOHA ENGINE â€” SEO Grammar Normalizer v1.0
Natural Language Refinement + SEO Keyword Harmonizer
---------------------------------------------------------
å…¥åŠ›:  ./output/styled/styled_persona_*.json
å‡ºåŠ›:  ./output/normalized/normalized_YYYYMMDD_HHMM.json
"""

import os
import json
import re
import unicodedata
from datetime import datetime
from tqdm import tqdm

INPUT_DIR = "./output/styled"
OUTPUT_DIR = "./output/normalized"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# === å…±é€šãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾© ===
REPLACE_PATTERNS = {
    "ãƒžã‚°ã‚»ãƒ¼ãƒ•": "MagSafe",
    "ã‚¢ã‚¤ãƒ•ã‚©ãƒ³": "iPhone",
    "ã‚¹ãƒžãƒ›": "ã‚¹ãƒžãƒ¼ãƒˆãƒ•ã‚©ãƒ³",
    "ãŠã—ã‚ƒã‚Œ": "ä¸Šå“ã§çŸ¥çš„ãªãƒ‡ã‚¶ã‚¤ãƒ³",
    "ã‹ã‚ã„ã„": "ã•ã‚Šã’ãªãæ„›ã‚‰ã—ã„",
    "é«˜ç´šæ„Ÿ": "é™ã‹ã«ä¸Šè³ªã‚’æ„Ÿã˜ã•ã›ã‚‹",
    "ã‚·ãƒ³ãƒ—ãƒ«": "ç†ã«ã‹ãªã£ãŸã‚·ãƒ³ãƒ—ãƒ«ã•",
    "ä¾¿åˆ©": "è€ƒãˆæŠœã‹ã‚ŒãŸä¾¿åˆ©ã•",
    "è»½é‡": "è»½ã‚„ã‹ãªè»½ã•",
}

# === æ–‡æœ«èªžå°¾ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ ===
ENDINGS = [
    "ã€‚", "ã€‚", "ã€‚",  # é€šå¸¸çŽ‡ã‚’é«˜ã‚ã¦å®‰å®š
    "ã€‚ä¸Šè³ªã‚’æ”¯ãˆã‚‹æ§‹é€ ã€‚",
    "ã€‚èª å®Ÿã«ä»•ä¸Šã’ãŸè¨­è¨ˆã€‚",
    "ã€‚é™ã‹ãªä½‡ã¾ã„ã§é­…ã›ã‚‹ã€‚",
    "ã€‚ä¸å¯§ãªã‚‚ã®ã¥ãã‚Šã€‚",
    "ã€‚é•·ãæ„›ã•ã‚Œã‚‹å“è³ªã€‚",
]

# === é‡è¤‡ãƒ»å†—é•·èªžã‚’æ•´å½¢ ===
def clean_text(text: str) -> str:
    if not text:
        return ""
    text = unicodedata.normalize("NFKC", text)
    text = re.sub(r"[!ï¼]+", "", text)
    text = re.sub(r"ã€‚{2,}", "ã€‚", text)
    text = re.sub(r"(\s{2,}|ã€€+)", " ", text)
    text = text.strip()
    return text

# === SEOèªžå½™çµ±ä¸€ ===
def harmonize_keywords(text: str) -> str:
    for k, v in REPLACE_PATTERNS.items():
        text = text.replace(k, v)
    return text

# === è‡ªç„¶èªžæ§‹æ–‡ã¸å¤‰æ› ===
def normalize_sentence(text: str) -> str:
    text = clean_text(text)
    text = harmonize_keywords(text)
    if not text.endswith("ã€‚"):
        text += "ã€‚"
    if len(text) < 40 and "ã€‚" in text:
        text += " " + re.sub(r"^ã€‚", "", text)  # è»½ã„è‡ªç„¶æ–‡é€£çµ
    if random_chance := hash(text) % len(ENDINGS):
        text = re.sub(r"ã€‚$", ENDINGS[random_chance], text)
    return text

def find_latest_styled():
    files = [f for f in os.listdir(INPUT_DIR) if f.startswith("styled_persona_") and f.endswith(".json")]
    if not files:
        return None
    latest = max(files, key=lambda f: os.path.getmtime(os.path.join(INPUT_DIR, f)))
    return os.path.join(INPUT_DIR, latest)

def main():
    input_file = find_latest_styled()
    if not input_file:
        print("ðŸš« styled_persona_*.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    with open(input_file, "r", encoding="utf-8") as f:
        data = json.load(f)

    refined = []
    for cluster in tqdm(data, desc="ðŸª„ Grammar Normalizing"):
        new_catch = normalize_sentence(cluster.get("catch_copy", ""))
        new_alts = [normalize_sentence(a) for a in cluster.get("alts", [])]

        refined.append({
            "cluster_id": cluster.get("cluster_id"),
            "persona": cluster.get("persona"),
            "brand_tone": cluster.get("brand_tone"),
            "catch_copy": new_catch,
            "alts": new_alts
        })

    ts = datetime.now().strftime("%Y%m%d_%H%M")
    output_file = os.path.join(OUTPUT_DIR, f"normalized_{ts}.json")
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(refined, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… SEO Grammar Normalizer å®Œäº†: {output_file}")
    print(f"ðŸ“Š ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {len(refined)}")
    print("ðŸ’¡ æ¬¡ã¯ Quality Filter ã§è‡ªç„¶åº¦ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã¸ã€‚")


if __name__ == "__main__":
    main()
import atlas_autosave_core
