#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SEO Optimizer Pro v3.8.2 (multiapi_async_compliant)
Author: ChatGPT + [Your Name]
Date: 2025-10-30

- æ¥½å¤© / Yahoo å•†å“æ¤œç´¢API æ­£å¼æº–æ‹ 
- é€²æ—å¯è¦–åŒ– (tqdm_asyncio)
- APIåˆ¥ã‚¨ãƒ©ãƒ¼è©³ç´°è¡¨ç¤º
- ALT / ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼è‡ªå‹•ç”Ÿæˆ
"""

import os, sys, json, asyncio, aiohttp, random, re, logging, urllib.parse
import pandas as pd
from collections import defaultdict, Counter
from pathlib import Path
from janome.tokenizer import Tokenizer
from dotenv import load_dotenv
from tqdm.asyncio import tqdm_asyncio

# ------------------------------
# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
# ------------------------------
logging.basicConfig(level=logging.INFO, format="[%(asctime)s] %(levelname)s: %(message)s")
logger = logging.getLogger("seo-optimizer")

# ------------------------------
# ç’°å¢ƒå¤‰æ•°
# ------------------------------
load_dotenv()

RAKUTEN_API_BASE_URL = os.getenv("RAKUTEN_API_BASE_URL")
RAKUTEN_APP_ID = os.getenv("RAKUTEN_APP_ID")

YAHOO_API_BASE_URL = os.getenv("YAHOO_API_BASE_URL")
YAHOO_APP_ID = os.getenv("YAHOO_APP_ID")

OPENAI_API_BASE_URL = os.getenv("OPENAI_API_BASE_URL", "https://api.openai.com/v1/")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
OPENAI_ENABLE = os.getenv("OPENAI_ENABLE", "false").lower() == "true"
OPENAI_USE_BATCH = os.getenv("OPENAI_USE_BATCH", "true").lower() == "true"

CONCURRENCY = int(os.getenv("SEO_CONCURRENCY", "6"))

# ------------------------------
# CSV èª­è¾¼ï¼ˆShift_JISå¯¾å¿œï¼‰
# ------------------------------
def load_input_csv(path="input.csv"):
    path = Path(path)
    for enc in ("cp932", "utf-8-sig", "utf-8"):
        try:
            df = pd.read_csv(path, encoding=enc)
            logger.info(f"âœ… CSVèª­ã¿è¾¼ã¿æˆåŠŸ: encoding={enc}, shape={df.shape}")
            break
        except Exception as e:
            logger.warning(f"âš ï¸ {enc} èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
    else:
        raise RuntimeError("âŒ CSVèª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

    name_col = next((c for c in df.columns if "å•†å“" in c or "name" in c.lower()), df.columns[2])
    df = df[df[name_col].astype(str).str.strip() != ""].copy()
    df.reset_index(drop=True, inplace=True)
    return df, name_col

# ------------------------------
# APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆæº–æ‹ ï¼‹è©³ç´°ã‚¨ãƒ©ãƒ¼ï¼‰
# ------------------------------
class MarketAPIClient:
    def __init__(self, base_url, appid, service_name):
        self.base_url = base_url
        self.appid = appid
        self.name = service_name

    async def _fetch(self, session, params):
        async with session.get(self.base_url, params=params) as resp:
            txt = await resp.text()
            if resp.status == 200:
                return json.loads(txt)
            else:
                raise RuntimeError(f"{self.name} {resp.status} {txt[:180]}")

    async def fetch_with_retry(self, keyword, max_retries=3):
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆ†å²ï¼ˆå…¬å¼ä»•æ§˜æº–æ‹ ï¼‰
        encoded_kw = urllib.parse.quote(keyword)
        if "rakuten" in self.base_url:
            params = {
                "applicationId": self.appid,
                "keyword": encoded_kw,
                "hits": 30,
                "format": "json",
                "formatVersion": 2,
                "sort": "-reviewCount"
            }
        elif "yahooapis" in self.base_url:
            params = {
                "appid": self.appid,
                "query": encoded_kw,
                "results": 30,
                "sort": "-review_count"
            }
        else:
            raise ValueError(f"æœªå¯¾å¿œAPIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: {self.base_url}")

        async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(ssl=False)) as session:
            for attempt in range(1, max_retries+1):
                try:
                    data = await self._fetch(session, params)
                    if data:
                        return data
                except Exception as e:
                    logger.error(f"âŒ {self.name} APIå¤±æ•—({attempt}/{max_retries}) [{keyword}]: {e}")
                    await asyncio.sleep(2 * attempt)
            logger.error(f"ğŸš¨ {self.name} ãƒªãƒˆãƒ©ã‚¤ä¸Šé™è¶…é [{keyword}] â€” ã“ã®ã‚¯ã‚¨ãƒªã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚")
            return {}

# ------------------------------
# ã‚«ãƒ†ã‚´ãƒªæ¨å®š
# ------------------------------
def infer_category(name, tokenizer):
    tokens = [t.surface for t in tokenizer.tokenize(name) if t.part_of_speech.startswith("åè©")]
    if not tokens:
        return "æœªåˆ†é¡"
    hints = {
        "ã‚®ãƒ•ãƒˆ": ["ã‚®ãƒ•ãƒˆ","è´ˆã‚Šç‰©","ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ"],
        "å¥åº·": ["å¥åº·","ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯","ç„¡æ·»åŠ "],
        "æ—¥ç”¨å“": ["é›‘è²¨","åç´","æƒé™¤"],
        "é£Ÿå“": ["é£Ÿå“","ã‚¹ã‚¤ãƒ¼ãƒ„","èª¿å‘³æ–™"],
    }
    for cat, words in hints.items():
        if any(w in tokens for w in words):
            return cat
    return "".join(tokens[:2]) + "ã‚«ãƒ†ã‚´ãƒª"

# ------------------------------
# èªå½™è¾æ›¸æ§‹ç¯‰ï¼ˆ7ã€œ15ä½ï¼‰
# ------------------------------
async def build_vocab_dictionary(client, df, name_col, tokenizer):
    vocab_map = defaultdict(lambda: defaultdict(list))
    async def process_item(name):
        cat = infer_category(name, tokenizer)
        data = await client.fetch_with_retry(name)
        items = []
        if "Items" in data:
            items = data.get("Items", [])
        elif "hits" in data:
            items = data.get("hits", [])
        if not items:
            return cat, []
        titles = []
        for item in items[6:15]:
            if isinstance(item, dict):
                title = (
                    item.get("Item", {}).get("itemName") or
                    item.get("name") or
                    item.get("Title") or ""
                )
                titles.append(title)
        words = []
        for t in titles:
            for token in tokenizer.tokenize(t):
                if token.part_of_speech.startswith("åè©") and len(token.surface) > 1:
                    words.append(token.surface)
        freq = Counter(words)
        return cat, [w for w, _ in freq.most_common(20)]

    tasks = [process_item(str(row[name_col])) for _, row in df.iterrows()]
    results = []
    for f in tqdm_asyncio.as_completed(tasks, desc=f"ğŸ“Š {client.name} èªå½™è¾æ›¸ç”Ÿæˆä¸­"):
        try:
            res = await f
            results.append(res)
        except Exception as e:
            logger.error(f"{client.name} èªå½™å‡¦ç†ä¸­ã‚¨ãƒ©ãƒ¼: {e}")

    for cat, words in results:
        vocab_map[cat]["vocab"].extend(words)
    return vocab_map

# ------------------------------
# semantic blockæ§‹é€ 
# ------------------------------
SEMANTIC_TEMPLATES = {
    "spec": ["{name} {keyword}ä»•æ§˜", "äººæ°—ã®{keyword}æ­è¼‰", "{keyword}ãƒ‡ã‚¶ã‚¤ãƒ³ {name}"],
    "feature": ["{keyword}ãŒç‰¹é•·", "{keyword}ã§å¥½è©•", "{keyword}ãŒé­…åŠ›"],
    "scene": ["{keyword}ã«ãŠã™ã™ã‚", "{keyword}ã§æ´»èº", "{keyword}ç”¨é€”ã«æœ€é©"],
    "benefit": ["{keyword}ãŒã†ã‚Œã—ã„ãƒã‚¤ãƒ³ãƒˆ", "{keyword}ã§æ¯æ—¥ã‚’å¿«é©ã«", "{keyword}ã ã‹ã‚‰é¸ã°ã‚Œã¦ã„ã¾ã™"]
}

def inject_market_vocabulary(local_templates, market_vocab):
    for cat, data in market_vocab.items():
        words = data.get("vocab", [])
        if not words:
            continue
        for block in SEMANTIC_TEMPLATES.keys():
            merged = local_templates.setdefault(cat, {}).setdefault(block, [])
            for w in words:
                merged.append(random.choice(SEMANTIC_TEMPLATES[block]).format(keyword=w))
    return local_templates

# ------------------------------
# æ–‡ç”Ÿæˆ
# ------------------------------
def _clean(s): return re.sub(r"[ ã€€]+"," ",s).replace("ã€ã€","ã€").strip()

def compose_alt(name, genre, vocab, templates, n=20):
    results = []
    base_words = vocab.get(genre, {}).get("vocab", [genre])
    for _ in range(n):
        parts = []
        for block in ["spec","feature","scene","benefit"]:
            tpl = random.choice(templates.get(genre, {}).get(block, ["{keyword}"]))
            kw = random.choice(base_words)
            parts.append(tpl.format(name=name, keyword=kw))
        text = _clean(f"{name} {'ã€'.join(parts)}")
        results.append(text[:110])
    return list(dict.fromkeys(results))

def compose_catchcopy(name, brand, genre):
    base = f"{genre}ãªã‚‰ã€{brand}ã®ã€Œ{name}ã€"
    pads = ["æ¯æ—¥ã«ã¡ã‚‡ã†ã©ã„ã„", "ã‚®ãƒ•ãƒˆã«ã‚‚äººæ°—", "ãƒ¬ãƒ“ãƒ¥ãƒ¼é«˜è©•ä¾¡"]
    while len(base) < 30:
        base += "ã€" + random.choice(pads)
    return base[:60]

# ------------------------------
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ------------------------------
async def main_async(input_csv="input.csv", output_csv="output_alts.csv"):
    df, name_col = load_input_csv(input_csv)
    tokenizer = Tokenizer()
    rakuten = MarketAPIClient(RAKUTEN_API_BASE_URL, RAKUTEN_APP_ID, "Rakuten")
    yahoo = MarketAPIClient(YAHOO_API_BASE_URL, YAHOO_APP_ID, "Yahoo")

    logger.info("ğŸª„ å¸‚å ´èªå½™è¾æ›¸æ§‹ç¯‰é–‹å§‹")
    rakuten_vocab = await build_vocab_dictionary(rakuten, df, name_col, tokenizer)
    yahoo_vocab = await build_vocab_dictionary(yahoo, df, name_col, tokenizer)

    market_vocab = defaultdict(lambda: defaultdict(list))
    for src in [rakuten_vocab, yahoo_vocab]:
        for cat, data in src.items():
            market_vocab[cat]["vocab"].extend(data.get("vocab", []))

    templates = inject_market_vocabulary(defaultdict(lambda: defaultdict(list)), market_vocab)

    outputs = []
    for _, row in tqdm_asyncio.tqdm(df.iterrows(), total=len(df), desc="ğŸ§  ALT/ã‚³ãƒ”ãƒ¼ç”Ÿæˆä¸­"):
        name = str(row[name_col]).strip()
        brand = str(row.get("ãƒ–ãƒ©ãƒ³ãƒ‰å",""))
        genre = infer_category(name, tokenizer)
        alts = compose_alt(name, genre, market_vocab, templates)
        catch = compose_catchcopy(name, brand, genre)
        outputs.append({"name": name, "category": genre, "catchcopy": catch, "alts": " | ".join(alts)})

    out_df = pd.DataFrame(outputs)
    out_df.to_csv(output_csv, index=False, encoding="utf-8-sig")
    logger.info(f"ğŸ’¾ å‡ºåŠ›å®Œäº†: {output_csv} ({len(out_df)}ä»¶)")

def main():
    asyncio.run(main_async())

if __name__ == "__main__":
    logger.info("ğŸš€ SEO Optimizer Pro v3.8.2 èµ·å‹•")
    main()
import atlas_autosave_core
