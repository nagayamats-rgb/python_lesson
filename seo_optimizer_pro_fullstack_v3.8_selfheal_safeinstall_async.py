#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SEO Optimizer Pro v3.8 (multiapi_async, semantic blocks, persona-ready)
Author: ChatGPT + [ã‚ãªãŸã®å]
Date: 2025-10-30

ç›®çš„:
  - Shift_JISå¯¾å¿œCSVã‹ã‚‰å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
  - .envã«è¨˜è¼‰ã®Rakuten / Yahoo / OpenAI APIã‚’å®‰å…¨ã«å‘¼ã³å‡ºã™
  - å„APIã‹ã‚‰æ¤œç´¢çµæœ7ã€œ15ä½ã®å•†å“æƒ…å ±ã‚’åé›†ã—ã¦èªå½™è¾æ›¸ã‚’é›ãˆã‚‹
  - æ„Ÿæƒ…ãƒˆãƒ¼ãƒ³å¯¾å¿œã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼ï¼ˆ30ã€œ60å­—ï¼‰
  - æ„å›³æ§‹é€ ALTæ–‡ï¼ˆ90ã€œ110å­—ï¼‰
  - å®Œå…¨éåŒæœŸãƒ»APIè‡ªå·±ä¿®å¾©å¯¾å¿œ
"""

# ===============================
# Imports & Auto-install
# ===============================
import os, sys, time, json, asyncio, aiohttp, random, re, logging, pickle
from pathlib import Path
from collections import defaultdict, Counter

REQUIRED = ["aiohttp", "pandas", "tqdm", "janome", "python-dotenv"]
for mod in REQUIRED:
    try:
        __import__(mod)
    except ImportError:
        print(f"âš™ï¸ Installing missing module: {mod}")
        os.system(f"{sys.executable} -m pip install -U {mod}")

import pandas as pd
from tqdm.asyncio import tqdm_asyncio
from janome.tokenizer import Tokenizer
from dotenv import load_dotenv

# ===============================
# è¨­å®šãƒ»ãƒ­ã‚®ãƒ³ã‚°
# ===============================
load_dotenv()
LOG_FORMAT = "[%(asctime)s] %(levelname)s: %(message)s"
logging.basicConfig(level=logging.INFO, format=LOG_FORMAT)
logger = logging.getLogger("seo-optimizer")

# ===============================
# ç’°å¢ƒå¤‰æ•°ï¼ˆ.envã‹ã‚‰èª­è¾¼ï¼‰
# ===============================
RAKUTEN_API_BASE_URL = os.getenv("RAKUTEN_API_BASE_URL")
RAKUTEN_APP_ID = os.getenv("RAKUTEN_APP_ID")

YAHOO_API_BASE_URL = os.getenv("YAHOO_API_BASE_URL")
YAHOO_APP_ID = os.getenv("YAHOO_APP_ID")

OPENAI_API_BASE_URL = os.getenv("OPENAI_API_BASE_URL", "https://api.openai.com/v1/")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
OPENAI_ENABLE = os.getenv("OPENAI_ENABLE", "false").lower() == "true"
OPENAI_USE_BATCH = os.getenv("OPENAI_USE_BATCH", "true").lower() == "true"

CONCURRENCY = int(os.getenv("SEO_CONCURRENCY", "6"))
CHECKPOINT_FILE = "checkpoint.pkl"
ERROR_LOG_FILE = "api_error.jsonl"

# ===============================
# å…¥åŠ›èª­ã¿è¾¼ã¿ï¼ˆShift_JISå¯¾å¿œï¼‰
# ===============================
def load_input_csv(path: str) -> pd.DataFrame:
    path_obj = Path(path)
    if not path_obj.exists():
        raise FileNotFoundError(f"å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {path}")
    for enc in ("cp932", "utf-8-sig", "utf-8"):
        try:
            df = pd.read_csv(path_obj, encoding=enc)
            logger.info(f"âœ… CSVèª­ã¿è¾¼ã¿æˆåŠŸ: encoding={enc}, shape={df.shape}")
            break
        except Exception as e:
            logger.warning(f"âš ï¸ èª­ã¿è¾¼ã¿å¤±æ•—ï¼ˆencoding={enc}ï¼‰: {e}")
    else:
        raise UnicodeError("CSVã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’åˆ¤å®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

    # å•†å“åç©ºæ¬„è¡Œã¯ç”Ÿæˆå¯¾è±¡å¤–ï¼ˆã‚«ãƒ©ãƒãƒªï¼‰
    if "å•†å“å" in df.columns:
        name_col = "å•†å“å"
    else:
        name_col = df.columns[2]  # Fallback
    df = df[df[name_col].astype(str).str.strip() != ""].copy()
    df.reset_index(drop=True, inplace=True)
    return df, name_col

# ===============================
# APIè‡ªå·±ä¿®å¾©ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
# ===============================
class MarketAPIClient:
    def __init__(self, base_url, appid, service_name):
        self.base_url = base_url
        self.appid = appid
        self.name = service_name

    async def _fetch(self, session, params):
        async with session.get(self.base_url, params=params) as resp:
            txt = await resp.text()
            if resp.status == 200:
                return json.loads(txt)
            elif resp.status in (401,403):
                raise PermissionError(f"{self.name}: èªè¨¼ã‚¨ãƒ©ãƒ¼ {resp.status}")
            elif resp.status == 429:
                raise ConnectionRefusedError(f"{self.name}: ãƒ¬ãƒ¼ãƒˆåˆ¶é™ {resp.status}")
            elif resp.status >= 500:
                raise ConnectionError(f"{self.name}: ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼ {resp.status}")
            else:
                raise RuntimeError(f"{self.name}: äºˆæœŸã—ãªã„å¿œç­” {resp.status}: {txt[:150]}")

    async def fetch_with_retry(self, keyword, max_retries=3):
        params = {"applicationId": self.appid, "appid": self.appid, "keyword": keyword, "query": keyword, "hits": 30, "results": 30}
        async with aiohttp.ClientSession() as session:
            for attempt in range(max_retries):
                try:
                    data = await self._fetch(session, params)
                    return data
                except PermissionError as e:
                    logger.error(f"ğŸ”‘ {self.name}: APIã‚­ãƒ¼ç„¡åŠ¹ã€‚{e}")
                    await asyncio.sleep(2)
                except ConnectionRefusedError:
                    logger.warning(f"â³ {self.name}: åˆ¶é™ä¸­ã€‚ãƒªãƒˆãƒ©ã‚¤å¾…æ©Ÿ({attempt+1}/3)")
                    await asyncio.sleep(30)
                except ConnectionError:
                    logger.warning(f"ğŸ” {self.name}: ã‚µãƒ¼ãƒãƒ¼å†è©¦è¡Œ({attempt+1}/3)")
                    await asyncio.sleep(5)
                except Exception as e:
                    logger.error(f"âŒ {self.name}: ä¸æ˜ã‚¨ãƒ©ãƒ¼ {e}")
                    await asyncio.sleep(2)
        raise RuntimeError(f"{self.name}: APIå†è©¦è¡Œå¤±æ•— ({keyword})")

# ===============================
# ã‚«ãƒ†ã‚´ãƒªç”Ÿæˆï¼ˆæ¤œç´¢æ„å›³ã«åŸºã¥ãï¼‰
# ===============================
def infer_category(name: str, tokenizer=None) -> str:
    """å•†å“åã‹ã‚‰æ¤œç´¢æ„å›³ã‚«ãƒ†ã‚´ãƒªã‚’é¡æ¨ã™ã‚‹"""
    if not name:
        return "æœªåˆ†é¡"
    if tokenizer is None:
        tokenizer = Tokenizer()

    tokens = [t.surface for t in tokenizer.tokenize(name)
              if t.part_of_speech.startswith("åè©")]
    if not tokens:
        return "ãã®ä»–"

    # æ„å›³ã‚¯ãƒ©ã‚¹ã‚¿è¾æ›¸ï¼ˆåˆæœŸãƒ’ãƒ³ãƒˆï¼‰
    CATEGORY_HINTS = {
        "ã‚®ãƒ•ãƒˆ": ["ã‚®ãƒ•ãƒˆ", "è´ˆã‚Šç‰©", "ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ", "ãŠç¥ã„"],
        "å¥åº·": ["ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯", "ç„¡æ·»åŠ ", "å¥åº·", "ãƒŠãƒãƒ¥ãƒ©ãƒ«"],
        "æ—¥ç”¨å“": ["ã‚­ãƒƒãƒãƒ³", "æƒé™¤", "åç´", "é›‘è²¨"],
        "ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³": ["ãƒãƒƒã‚°", "ã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼", "ã‚·ãƒ£ãƒ„", "é´"],
        "é£Ÿå“": ["ã‚¹ã‚¤ãƒ¼ãƒ„", "é£Ÿå“", "èª¿å‘³æ–™", "ã”é£¯", "ãƒ‰ãƒªãƒ³ã‚¯"],
    }

    for cat, hints in CATEGORY_HINTS.items():
        if any(h in tokens for h in hints):
            return cat

    # æœªå®šç¾©ã‚«ãƒ†ã‚´ãƒª â†’ ãƒˆãƒƒãƒ—2åè©é€£çµ
    return "".join(tokens[:2]) + "ã‚«ãƒ†ã‚´ãƒª"

# ===============================
# å¸‚å ´èªå½™æŠ½å‡ºï¼ˆ7ã€œ15ä½ï¼‰â†’ å…±èµ·èªè¾æ›¸ç”Ÿæˆ
# ===============================
async def build_vocab_dictionary(client: MarketAPIClient, df, name_col, tokenizer):
    """APIã‹ã‚‰7ã€œ15ä½ã®å•†å“ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºã—ã€ã‚«ãƒ†ã‚´ãƒªåˆ¥èªå½™è¾æ›¸ã‚’æ§‹ç¯‰"""
    vocab_map = defaultdict(lambda: defaultdict(list))

    async def fetch_and_extract(name):
        cat = infer_category(name, tokenizer)
        data = await client.fetch_with_retry(name)
        items = []
        if "Items" in data:  # Rakutenå½¢å¼
            items = data["Items"]
        elif "hits" in data:  # Yahooå½¢å¼
            items = data["hits"]
        if not items:
            return cat, []

        # 7ã€œ15ä½ã®ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰åè©æŠ½å‡º
        titles = []
        for item in items[6:15]:
            title = ""
            if isinstance(item, dict):
                title = (
                    item.get("Item", {}).get("itemName") or
                    item.get("name") or
                    item.get("Title") or ""
                )
            titles.append(title)

        words = []
        for t in titles:
            for token in tokenizer.tokenize(t):
                if token.part_of_speech.startswith("åè©") and len(token.surface) > 1:
                    words.append(token.surface)
        freq = Counter(words)
        common_words = [w for w, _ in freq.most_common(20)]
        return cat, common_words

    tasks = [fetch_and_extract(str(n)) for n in df[name_col].head(50)]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    for res in results:
        if isinstance(res, tuple):
            cat, words = res
            for w in words:
                vocab_map[cat]["vocab"].append(w)
    return vocab_map

# ===============================
# semantic block æ§‹é€ ï¼ˆspec, feature, scene, benefitï¼‰
# ===============================
SEMANTIC_TEMPLATES = {
    "spec": [
        "{name} {keyword}ä»•æ§˜", "äººæ°—ã®{keyword}æ­è¼‰", "{keyword}ãƒ‡ã‚¶ã‚¤ãƒ³ {name}"
    ],
    "feature": [
        "{keyword}ãŒç‰¹é•·", "{keyword}ã§å¥½è©•", "{keyword}ãŒé­…åŠ›"
    ],
    "scene": [
        "{keyword}ã«ãŠã™ã™ã‚", "{keyword}ã‚·ãƒ¼ãƒ³ã«æœ€é©", "{keyword}ã§æ´»èº"
    ],
    "benefit": [
        "{keyword}ã§æ¯æ—¥ã‚’å¿«é©ã«", "{keyword}ãŒã†ã‚Œã—ã„ãƒã‚¤ãƒ³ãƒˆ", "{keyword}ã ã‹ã‚‰é¸ã°ã‚Œã¦ã„ã¾ã™"
    ],
}

def inject_market_vocabulary(local_templates, market_vocab):
    """å¸‚å ´èªå½™è¾æ›¸ã‚’ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«æ³¨å…¥"""
    for cat, data in market_vocab.items():
        words = data.get("vocab", [])
        if not words:
            continue
        for block in SEMANTIC_TEMPLATES.keys():
            merged = local_templates.setdefault(cat, {}).setdefault(block, [])
            for w in words:
                merged.append(random.choice(SEMANTIC_TEMPLATES[block]).format(keyword=w))
    return local_templates

# ===============================
# ãƒ­ãƒ¼ã‚«ãƒ«åˆæœŸèªå½™ï¼‹ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆ
# ===============================
def bootstrap_local_vocab_and_templates(df, name_col):
    tokenizer = Tokenizer()
    vocab = defaultdict(lambda: defaultdict(list))
    templates = defaultdict(lambda: defaultdict(list))
    for _, row in df.iterrows():
        name = str(row[name_col]).strip()
        cat = infer_category(name, tokenizer)
        words = [t.surface for t in tokenizer.tokenize(name)
                 if t.part_of_speech.startswith("åè©") and len(t.surface) > 1]
        vocab[cat]["vocab"].extend(words)
        for block in SEMANTIC_TEMPLATES.keys():
            templates[cat][block] = SEMANTIC_TEMPLATES[block].copy()
    return vocab, templates
# ===============================
# Utilityï¼šå¥èª­ç‚¹æ•´å½¢ãƒ»ãƒˆãƒªãƒ 
# ===============================
def _clean_text(s: str) -> str:
    s = re.sub(r"[ ã€€]+", " ", s)
    s = s.replace("ã€ã€", "ã€").replace("ã€‚ã€‚", "ã€‚")
    s = s.replace("ã€œ", "").replace("..", "ã€‚")
    return s.strip()

# ===============================
# ALTç”Ÿæˆï¼ˆè¨˜è€…ï¼‹SEOã‚¢ãƒŠãƒªã‚¹ãƒˆè¦–ç‚¹ï¼‰
# ===============================
def compose_alt_variations(name, brand, genre, vocab, templates, n=20):
    results = []
    base_words = vocab.get(genre, {}).get("vocab", [])
    if not base_words:
        base_words = [genre, name]
    for _ in range(n):
        parts = []
        for block in ["spec", "feature", "scene", "benefit"]:
            block_tpls = templates.get(genre, {}).get(block, [])
            if block_tpls:
                tpl = random.choice(block_tpls)
                kw = random.choice(base_words)
                parts.append(tpl.format(name=name, keyword=kw))
        text = "ã€".join(parts)
        alt = f"{name} {text}"
        alt = _clean_text(alt)
        # å¥èª­ç‚¹å¯†åº¦ã‚’åˆ¶å¾¡ã—ã€110æ–‡å­—ã«ä¸¸ã‚
        if len(alt) > 110:
            alt = alt[:110].rsplit("ã€", 1)[0]
        results.append(alt)
    return list(dict.fromkeys(results))  # é‡è¤‡å‰Šé™¤

# ===============================
# ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼ç”Ÿæˆï¼ˆç·¨é›†è€…è¦–ç‚¹ãƒ»30ã€œ60æ–‡å­—ï¼‰
# ===============================
def compose_catchcopy(name, brand, genre, emotion, benefits):
    JP_MAX, JP_MIN = 60, 30
    base = f"{emotion}{genre}ãªã‚‰ã€{brand}ã®ã€Œ{name}ã€"
    base = _clean_text(base)
    if len(base) < JP_MIN:
        for p in benefits:
            if len(base) >= JP_MIN:
                break
            base += "ã€" + p
    if len(base) > JP_MAX:
        base = base[:JP_MAX]
    if len(base) < JP_MIN:
        base += "ã€‚"  # å®‰å…¨å¼
    return base

# ===============================
# OpenAIè£œå®Œï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
# ===============================
async def enhance_with_openai(batch_texts):
    if not OPENAI_ENABLE or not OPENAI_API_KEY:
        return batch_texts  # ä½¿ã‚ãªã„å ´åˆã¯ãã®ã¾ã¾è¿”ã™
    headers = {
        "Authorization": f"Bearer {OPENAI_API_KEY}",
        "Content-Type": "application/json"
    }
    url = f"{OPENAI_API_BASE_URL}chat/completions"
    prompt = (
        "æ¬¡ã®ALTã¾ãŸã¯ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼ã‚’è‡ªç„¶ã§æµæš¢ãªæ—¥æœ¬èªã«æ•´ãˆã¦ãã ã•ã„ã€‚\n"
        "å¥èª­ç‚¹ã®éä¸è¶³ã‚’ç›´ã—ã€æ„å‘³ã‚’ä¿ã£ãŸã¾ã¾110æ–‡å­—ä»¥å†…ã«ã¾ã¨ã‚ã¾ã™ã€‚"
    )
    payload = {
        "model": "gpt-4o-mini",
        "messages": [{"role": "system", "content": prompt},
                     {"role": "user", "content": "\n".join(batch_texts)}],
        "temperature": 0.5,
        "max_tokens": 3000
    }
    async with aiohttp.ClientSession() as session:
        async with session.post(url, headers=headers, json=payload) as resp:
            if resp.status != 200:
                txt = await resp.text()
                logger.error(f"OpenAIè£œå®Œå¤±æ•—: {resp.status} {txt[:200]}")
                return batch_texts
            data = await resp.json()
            out = data["choices"][0]["message"]["content"].splitlines()
            return [o.strip() for o in out if o.strip()]

# ===============================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ===============================
async def main_async(input_csv="input.csv", output_csv="output_alts.csv"):
    df, name_col = load_input_csv(input_csv)
    tokenizer = Tokenizer()

    # åˆæœŸèªå½™ãƒ»ãƒ†ãƒ³ãƒ—ãƒ¬ç”Ÿæˆ
    local_vocab, local_templates = bootstrap_local_vocab_and_templates(df, name_col)

    # APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
    rakuten_client = MarketAPIClient(RAKUTEN_API_BASE_URL, RAKUTEN_APP_ID, "Rakuten")
    yahoo_client = MarketAPIClient(YAHOO_API_BASE_URL, YAHOO_APP_ID, "Yahoo")

    logger.info("ğŸª„ å¸‚å ´èªå½™è¾æ›¸æ§‹ç¯‰é–‹å§‹ï¼ˆæ¥½å¤©ï¼‰")
    rakuten_vocab = await build_vocab_dictionary(rakuten_client, df, name_col, tokenizer)
    logger.info("ğŸª„ å¸‚å ´èªå½™è¾æ›¸æ§‹ç¯‰é–‹å§‹ï¼ˆYahooï¼‰")
    yahoo_vocab = await build_vocab_dictionary(yahoo_client, df, name_col, tokenizer)

    # èªå½™çµ±åˆ
    market_vocab = defaultdict(lambda: defaultdict(list))
    for src in [rakuten_vocab, yahoo_vocab, local_vocab]:
        for cat, data in src.items():
            market_vocab[cat]["vocab"].extend(data.get("vocab", []))

    templates = inject_market_vocabulary(local_templates, market_vocab)

    # ç”Ÿæˆå®Ÿè¡Œ
    outputs = []
    for _, row in df.iterrows():
        name = str(row[name_col]).strip()
        brand = str(row.get("ãƒ–ãƒ©ãƒ³ãƒ‰å", "")) if "ãƒ–ãƒ©ãƒ³ãƒ‰å" in df.columns else ""
        genre = infer_category(name, tokenizer)
        vocab = market_vocab
        alts = compose_alt_variations(name, brand, genre, vocab, templates, n=20)
        catch = compose_catchcopy(name, brand, genre, "", ["ã‚®ãƒ•ãƒˆã«ã‚‚äººæ°—", "æ¯æ—¥ã«ã¡ã‚‡ã†ã©ã„ã„", "ãƒ¬ãƒ“ãƒ¥ãƒ¼é«˜è©•ä¾¡"])
        if OPENAI_ENABLE and OPENAI_USE_BATCH:
            alts = await enhance_with_openai(alts)
        outputs.append({
            "name": name,
            "category": genre,
            "catchcopy": catch,
            "alts": " | ".join(alts)
        })

    out_df = pd.DataFrame(outputs)
    out_df.to_csv(output_csv, index=False, encoding="utf-8-sig")
    logger.info(f"âœ… å‡ºåŠ›å®Œäº†: {output_csv} ({len(out_df)}ä»¶)")

def main():
    asyncio.run(main_async())

if __name__ == "__main__":
    main()
import atlas_autosave_core
