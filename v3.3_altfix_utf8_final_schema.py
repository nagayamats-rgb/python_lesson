# ==========================================================
# v3.3_altfix_utf8_final_schema_compat.py
# ----------------------------------------------------------
# ==========================================================
# v3.3_altfix_utf8_final_schema_compat.py
# ==========================================================
import csv, json, re, time, os
from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()  # â† ã“ã“ã‚’è¿½åŠ ï¼ï¼

MODEL_NAME = "gpt-4o-mini"
client = OpenAI()

# ======== è¨­å®š ========
MODEL_NAME = "gpt-4o-mini"
INPUT_FILE = "./rakuten.csv"
OUTPUT_FILE = "./output/ai_writer/alt_text_20251101_test.csv"
os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)

client = OpenAI()

# ======== ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ========
TARGET_MIN = 80
TARGET_MAX = 110
TARGET_PREFERRED_MIN = 95
TARGET_PREFERRED_MAX = 105

# ======== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ========
def _count_zenkaku(text: str) -> int:
    return len(text)

def _split_sentences(text: str) -> list[str]:
    chunks = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ])\s*', text.strip())
    return [c for c in chunks if c]

def _trim_to_range(text: str) -> str:
    """è‡ªç„¶çµ‚ç«¯ã‚«ãƒƒãƒˆï¼ˆç†æƒ³95ã€œ105ï¼‰"""
    sents = _split_sentences(text)
    if not sents:
        return text.strip()

    center = (TARGET_PREFERRED_MIN + TARGET_PREFERRED_MAX) // 2
    cands = []
    for k in range(len(sents), 0, -1):
        cand = ''.join(sents[:k]).strip()
        n = _count_zenkaku(cand)
        cands.append((abs(center - n),
                      (TARGET_PREFERRED_MIN <= n <= TARGET_PREFERRED_MAX),
                      (TARGET_MIN <= n <= TARGET_MAX),
                      n, cand))

    ideal = [c for c in cands if c[1]]
    if ideal:
        return sorted(ideal)[0][4]
    ok = [c for c in cands if c[2]]
    if ok:
        return sorted(ok)[0][4]

    nearest = sorted(cands)[0][4]
    nearest = re.sub(r'(ã€‚\s*)+$', 'ã€‚', nearest)
    nearest = re.sub(r'(ã§ã™|ã¾ã™|ã¨ãªã‚Šã¾ã™)[ã€‚]*$', 'ã§ã™ã€‚', nearest)
    while _count_zenkaku(nearest) > TARGET_MAX and 'ã€' in nearest:
        nearest = nearest.rsplit('ã€', 1)[0] + 'ã€‚'
    return nearest.strip()

# ======== ALTç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ ========
def _build_alt_system_prompt(knowledge_text: str = "", forbidden_words: list[str] = None) -> str:
    forb = 'ã€'.join(sorted(set(forbidden_words or [])))
    return (
        "ã‚ãªãŸã¯ECãƒ¢ãƒ¼ãƒ«ï¼ˆæ¥½å¤©ãƒ»Yahooï¼‰å°‚é–€ã®æ—¥æœ¬èªã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚"
        "ç”»åƒã®æå†™ã¯ä¸€åˆ‡ã—ã¾ã›ã‚“ã€‚å•†å“ç†è§£ã¨åˆ©ç”¨ã‚·ãƒ¼ãƒ³ã€ä¾¿ç›Šã‚’è‡ªç„¶ãªæ—¥æœ¬èªã§ä¼ãˆã‚‹ALTï¼ˆä»£æ›¿ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã‚’ä½œæˆã—ã¾ã™ã€‚\n"
        "è¦ä»¶ï¼š\n"
        "ãƒ»1ã€œ2æ–‡ã®è‡ªç„¶æ–‡ã€‚çµµæ–‡å­—ãƒ»è¨˜å·ãƒ»HTMLã‚¿ã‚°ç¦æ­¢ã€‚ç«¶åˆæ¯”è¼ƒã‚„â€œç«¶åˆå„ªä½æ€§â€ãªã©ã®ãƒ¡ã‚¿è¡¨ç¾ã¯ç¦æ­¢ã€‚\n"
        "ãƒ»æ§‹æˆãƒ’ãƒ³ãƒˆï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ã§ã¯ãªã„ï¼‰ï¼šã€å•†å“ã‚¹ãƒšãƒƒã‚¯â†’ã‚³ã‚¢ã‚³ãƒ³ãƒ”ã‚¿ãƒ³ã‚¹â†’ã©ã‚“ãªäººâ†’ã©ã‚“ãªã‚·ãƒ¼ãƒ³â†’ã©ã‚“ãªãƒ™ãƒãƒ•ã‚£ãƒƒãƒˆã€ã‚’ç„¡ç†ãªãå«ã‚ã‚‹ã€‚\n"
        "ãƒ»ç”»åƒèª¬æ˜èªï¼ˆç”»åƒãƒ»å†™çœŸãƒ»æ˜ ã£ã¦ã„ã‚‹ãƒ»ã‚¯ãƒªãƒƒã‚¯ç­‰ï¼‰ã¯ä½¿ç”¨ç¦æ­¢ã€‚\n"
        f"ãƒ»ç¦æ­¢èªï¼š{forb if forb else 'ï¼ˆãªã—ï¼‰'}\n"
        "ãƒ»ã¾ãš120ã€œ130å­—ã§ä½œæˆï¼ˆå¾Œæ®µã§è‡ªç„¶çµ‚ç«¯ã‚«ãƒƒãƒˆã—ã¦80ã€œ110å­—ã«æ•´å½¢ï¼‰ã€‚\n"
        "ãƒ»ã‚µã‚¤ãƒˆå†…SEOã‚’æ„è­˜ã—ã€å‹ç•ªãƒ»ç´ æãªã©ã‚’è‡ªç„¶ã«ç¹”ã‚Šè¾¼ã‚€ã€‚\n"
        "ãƒ»å‡ºåŠ›ã¯ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã€‚\n\n"
        f"çŸ¥è¦‹è¦ç´„ï¼š{knowledge_text.strip()}"
    )

def _build_alt_user_prompt(product_name: str) -> str:
    return (
        f"å•†å“åï¼š{product_name}\n"
        "ä¸Šè¨˜å•†å“ã«å¯¾ã™ã‚‹ALTã‚’æ—¥æœ¬èªã§20ä»¶ä½œæˆã—ã¦ãã ã•ã„ã€‚"
        "ãã‚Œãã‚Œ120ã€œ130å­—ã®1ã€œ2æ–‡ã¨ã—ã¦ãã ã•ã„ã€‚\n"
        "å¯èƒ½ãªã‚‰æ¬¡ã®JSONå½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ï¼š\n"
        "{\n"
        '  "alts": ["ALT1", "ALT2", â€¦]\n'
        "}\n"
        "JSONãŒé›£ã—ã‘ã‚Œã°ãƒ†ã‚­ã‚¹ãƒˆã§20è¡Œã§ã‚‚æ§‹ã„ã¾ã›ã‚“ã€‚"
    )

def _recover_alts_from_text(raw: str) -> list[str]:
    lines = [ln.strip(" ãƒ»-â€¢\t") for ln in (raw or "").splitlines()]
    return [ln for ln in lines if ln and len(ln) > 10]

# ======== ALTç”Ÿæˆæœ¬ä½“ ========
def ai_generate_alt(product_name: str, knowledge_text: str = "", forbidden_words: list[str] = None) -> list[str]:
    """é•·æ–‡ALTç”Ÿæˆï¼‹è‡ªç„¶çµ‚ç«¯ãƒˆãƒªãƒ """
    system_prompt = _build_alt_system_prompt(knowledge_text, forbidden_words)
    user_prompt = _build_alt_user_prompt(product_name)

    raw = ""
    try:
        res = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "system", "content": system_prompt},
                      {"role": "user", "content": user_prompt}],
            response_format={"type": "text"},
            max_completion_tokens=1000,
        )
        raw = (res.choices[0].message.content or "").strip()
    except Exception as e:
        print(f"âš ï¸ OpenAIã‚¨ãƒ©ãƒ¼: {e}")

    alts = []
    if raw:
        m = re.search(r'\{[\s\S]*\}', raw)
        if m:
            try:
                obj = json.loads(m.group(0))
                if isinstance(obj, dict) and isinstance(obj.get("alts"), list):
                    alts = [str(x).strip() for x in obj["alts"] if str(x).strip()]
            except Exception:
                pass

    if not alts:
        alts = _recover_alts_from_text(raw)

    if len(alts) < 20:
        alts += [alts[-1]] * (20 - len(alts))

    alts = [_trim_to_range(a) for a in alts[:20]]
    return alts

# ======== ãƒ¡ã‚¤ãƒ³å‡¦ç† ========
def main():
    print("ğŸŒ¸ v3.3_altfix_utf8_final_schema_compat å®Ÿè¡Œé–‹å§‹ï¼ˆALTé•·æ–‡â†’è‡ªç„¶çµ‚ç«¯ãƒˆãƒªãƒ ãƒ¢ãƒ¼ãƒ‰ï¼‰")

    # CSVèª­è¾¼ï¼ˆUTF-8å›ºå®šï¼‰
    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        products = [r["å•†å“å"] for r in reader if r.get("å•†å“å")]

    print(f"âœ… å•†å“åæŠ½å‡º: {len(products)}ä»¶ï¼ˆé‡è¤‡é™¤å»æ¸ˆï¼‰")

    results = []
    for idx, name in enumerate(products, 1):
        print(f"ğŸ§  ALTç”Ÿæˆä¸­ {idx}/{len(products)}: {name[:30]}...")
        alts = ai_generate_alt(name)
        results.append({"å•†å“å": name, **{f"ALT{i+1}": a for i, a in enumerate(alts)}})
        time.sleep(1)

    # æ›¸ãå‡ºã—
    fieldnames = ["å•†å“å"] + [f"ALT{i+1}" for i in range(20)]
    with open(OUTPUT_FILE, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(results)

    print(f"âœ… å‡ºåŠ›å®Œäº†: {OUTPUT_FILE}")
    print("ğŸŒ¸ å…¨å‡¦ç†å®Œäº†")

# ===== ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ =====
if __name__ == "__main__":
    main()
import atlas_autosave_core
