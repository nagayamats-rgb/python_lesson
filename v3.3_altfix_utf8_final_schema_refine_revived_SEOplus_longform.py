import csv, json, re, os, time
from tqdm import tqdm
from openai import OpenAI
from dotenv import load_dotenv

# ======== åˆæœŸè¨­å®š ========
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
INPUT_CSV = "rakuten.csv"
OUTPUT_CSV = "./output/ai_writer/alt_text_refined_final_revived_SEOplus_longform.csv"
ENCODING = "utf-8"

SEMANTIC_DIR = "./output/semantics"
MAX_RETRY = 3
SLEEP_BETWEEN = 3

# ======== ãƒ­ãƒ¼ã‚«ãƒ«JSONèª­è¾¼ ========
def load_json(path):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except:
        return {}

persona = load_json(f"{SEMANTIC_DIR}/styled_persona_20251031_0031.json")
lexical = load_json(f"{SEMANTIC_DIR}/lexical_clusters_20251030_223013.json")
market = load_json(f"{SEMANTIC_DIR}/market_vocab_20251030_201906.json")
semantic = load_json(f"{SEMANTIC_DIR}/structured_semantics_20251030_224846.json")
norm = load_json(f"{SEMANTIC_DIR}/normalized_20251031_0039.json")

# ======== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ========
def clean_text(text):
    """å¥èª­ç‚¹ãƒ»ç¦å‰‡ãƒ»é‡è¤‡é™¤å»"""
    if not text:
        return ""
    text = re.sub(r"[\"'\n\r]", "", text)
    text = re.sub(r"ã€€+", " ", text)
    text = re.sub(r"\s+", " ", text)
    text = re.sub(r"(ã€‚){2,}", "ã€‚", text)
    text = text.strip()
    return text

def trim_to_range_natural(txt, min_len=100, max_len=120):
    """æ–‡æœ«å„ªå…ˆã§è‡ªç„¶ãªé•·ã•ã«æ•´å½¢"""
    txt = txt.strip()
    if len(txt) <= max_len:
        return txt
    cut = txt[:max_len]
    last_p = max(cut.rfind("ã€‚"), cut.rfind("ã€"))
    if last_p >= min_len:
        return cut[:last_p+1]
    return cut[:max_len]

def summarize_keywords():
    """SEOã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
    kws = []
    for source in [market, lexical]:
        if isinstance(source, list):
            for v in source[:15]:
                if isinstance(v, dict):
                    for k in v.values():
                        if isinstance(k, str):
                            kws.append(k)
        elif isinstance(source, dict):
            kws += [v for v in source.values() if isinstance(v, str)]
    kws = list(set(kws))
    return "ã€".join(kws[:30])

SEO_KEYWORDS = summarize_keywords()

# ======== AIå‘¼ã³å‡ºã— ========
def ai_generate_alts(product_name):
    prompt = f"""
ã‚ãªãŸã¯SEOãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°ã«é•·ã‘ãŸæ—¥æœ¬èªã®ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã§ã™ã€‚
ä»¥ä¸‹ã®æ¡ä»¶ã§ALTãƒ†ã‚­ã‚¹ãƒˆã‚’20ä»¶ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ã€ç›®çš„ã€‘
ãƒ»æ¥½å¤©ã®å•†å“ç”»åƒã«è¨­å®šã™ã‚‹ALTãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚
ãƒ»æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³æœ€é©åŒ–ï¼ˆSEOï¼‰åŠ¹æœã‚’æœ€å¤§åŒ–ã—ã¾ã™ã€‚

ã€æŒ‡ç¤ºã€‘
ãƒ»å„æ–‡ã¯è‡ªç„¶ã§èª­ã¿ã‚„ã™ãã€æƒ…å ±é‡ã®ã‚ã‚‹æ—¥æœ¬èªã§ã€‚
ãƒ»1æ–‡ã§å®Œçµã—ã€ç†æƒ³ã¯120ã€œ140å­—ã€‚æœ€ä½ã§ã‚‚100å­—ä»¥ä¸Šã«ã—ã¦ãã ã•ã„ã€‚
ãƒ»å•†å“åã€ä¸»è¦æ©Ÿèƒ½ã€å¯¾å¿œæ©Ÿç¨®ã€ç´ æã€ç”¨é€”ã‚’2ã€œ3å›è‡ªç„¶ã«å«ã‚ã‚‹ã€‚
ãƒ»SEOã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å€™è£œã‚’è‡ªç„¶ã«æ•£ã‚Šã°ã‚ã‚‹ã€‚
ãƒ»ç¦æ­¢èªï¼šã€Œæœ€å®‰ã€ã€ŒNo.1ã€ã€Œç«¶åˆã€ã€Œä»–ç¤¾ã€ã€Œç”»åƒã€ã€Œå†™çœŸã€ã€Œè¦‹ãŸç›®ã€ã€Œå•†å“ç”»åƒã€ã€Œæ˜ ãˆã‚‹ã€ãªã©ã¯ç¦æ­¢ã€‚
ãƒ»çµµæ–‡å­—ã€ç‰¹æ®Šè¨˜å·ã€ã‚¿ã‚°ã¯ç¦æ­¢ã€‚
ãƒ»å¥ç‚¹ã€Œã€‚ã€ã§æ–‡ã‚’çµ‚ãˆã‚‹ã€‚

ã€SEOã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å€™è£œã€‘
{SEO_KEYWORDS}

ã€å•†å“åã€‘
{product_name}

å‡ºåŠ›å½¢å¼:
20å€‹ã®æ—¥æœ¬èªæ–‡ã‚’JSONé…åˆ—ã¨ã—ã¦å‡ºåŠ›ã€‚
"""

    for attempt in range(MAX_RETRY):
        try:
            res = client.chat.completions.create(
    model=model,
    messages=[
        {"role": "system", "content": "ã‚ãªãŸã¯SEOã«æœ€é©åŒ–ã•ã‚ŒãŸæ—¥æœ¬èªãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚"},
        {"role": "user", "content": prompt}
    ],
    temperature=0.8,
    max_completion_tokens=900,
    response_format="text"
)
text = res.choices[0].message.content
alts = re.findall(r"[^ã€‚]+ã€‚", text)[:20]
            if isinstance(data, list) and len(data) > 0:
                return data
        except Exception as e:
            print(f"âš ï¸ ç”Ÿæˆã‚¨ãƒ©ãƒ¼({attempt+1}/{MAX_RETRY}): {e}")
            time.sleep(SLEEP_BETWEEN)
    return []

# ======== ãƒ¡ã‚¤ãƒ³å‡¦ç† ========
def main():
    with open(INPUT_CSV, "r", encoding=ENCODING) as f:
        reader = csv.DictReader(f)
        products = [r["å•†å“å"] for r in reader if r.get("å•†å“å")]

    print(f"ğŸŒ¸ ALTç”Ÿæˆé–‹å§‹ï¼ˆSEOå¼·åŒ–ï¼‹é•·æ–‡ãƒ¢ãƒ¼ãƒ‰ï¼‰")
    print(f"âœ… å¯¾è±¡å•†å“æ•°: {len(products)}ä»¶")

    results = []
    for nm in tqdm(products, desc="ğŸ§  ç”Ÿæˆä¸­"):
        alts = ai_generate_alts(nm)
        alts_cleaned = [trim_to_range_natural(clean_text(a), 100, 120) for a in alts]
        row = {"å•†å“å": nm}
        for i, a in enumerate(alts_cleaned[:20]):
            row[f"ALT{i+1}"] = a
        results.append(row)
        time.sleep(1.5)

    os.makedirs(os.path.dirname(OUTPUT_CSV), exist_ok=True)
    with open(OUTPUT_CSV, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, fieldnames=["å•†å“å"] + [f"ALT{i}" for i in range(1, 21)])
        writer.writeheader()
        writer.writerows(results)

    print(f"âœ… å‡ºåŠ›å®Œäº†: {OUTPUT_CSV}")
    print("âœ… å„ALTã¯SEOèªã‚’å«ã‚€è‡ªç„¶æ–‡ã§120ã€œ140å­—ç†æƒ³ã€å¥ç‚¹æ•´å½¢æ¸ˆã€‚")

if __name__ == "__main__":
    main()
import atlas_autosave_core
