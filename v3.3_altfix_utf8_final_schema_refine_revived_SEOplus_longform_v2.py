# -*- coding: utf-8 -*-
"""
v3.3_altfix_utf8_final_schema_refine_revived_SEOplus_longform_v2.py
ALTé•·æ–‡ï¼ˆ80ã€œ110å­—ï¼‰Ã—20æœ¬ã‚’ã€ãƒ­ãƒ¼ã‚«ãƒ«çŸ¥è¦‹ã‚’æ´»ã‹ã—ã¦è‡ªç„¶æ–‡ã§ç”Ÿæˆã€‚
- å…¥åŠ›:  /Users/tsuyoshi/Desktop/python_lesson/rakuten.csvï¼ˆUTF-8, å…ˆé ­è¡Œãƒ˜ãƒƒãƒ€, ã€Œå•†å“åã€åˆ—ï¼‰
- çŸ¥è¦‹:  ./output/semantics/ é…ä¸‹ã®JSONç¾¤ï¼ˆå­˜åœ¨ã™ã‚Œã°èª­ã¿è¾¼ã‚€ã ã‘ã§OKï¼‰
- å‡ºåŠ›:  ./output/ai_writer/alt_text_refined_final_longform_v2.csvï¼ˆå•†å“å,ALT1..ALT20ï¼‰
"""

import os
import csv
import json
import re
import time
from typing import List, Dict, Any
from collections import defaultdict

from tqdm import tqdm
from dotenv import load_dotenv
from openai import OpenAI, OpenAIError

# ====== åˆæœŸè¨­å®š ======
load_dotenv()  # .env èª­ã¿è¾¼ã¿ï¼ˆOPENAI_API_KEY, OPENAI_BASE_URL ç­‰ï¼‰
client = OpenAI()  # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—

MODEL = os.getenv("OPENAI_MODEL", "gpt-4o")  # å›ºå®šï¼šgpt-4oï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šï¼‰
MAX_COMPLETION_TOKENS = 1200                 # é•·æ–‡è€æ€§
TEMPERATURE = 1                              # ä»•æ§˜äº’æ›ã®ãŸã‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆ=1ï¼‰ã«å›ºå®š
RETRY = 3
SLEEP_ON_FAIL = 3.0

# å…¥å‡ºåŠ›
BASE_DIR = "/Users/tsuyoshi/Desktop/python_lesson"
INPUT_CSV = os.path.join(BASE_DIR, "rakuten.csv")  # UTF-8 & ã€Œå•†å“åã€
OUT_DIR = os.path.join(BASE_DIR, "output/ai_writer")
os.makedirs(OUT_DIR, exist_ok=True)
OUT_CSV = os.path.join(OUT_DIR, "alt_text_refined_final_longform_v2.csv")

# çŸ¥è¦‹ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå­˜åœ¨ã™ã‚Œã°ä½¿ã†ï¼‰
SEM_DIR = os.path.join(BASE_DIR, "output/semantics")
SEM_FILES = {
    "lexical": "lexical_clusters_20251030_223013.json",
    "structured": "structured_semantics_20251030_224846.json",
    "market": "market_vocab_20251030_201906.json",
    "persona": "styled_persona_20251031_0031.json",
    "normalized": "normalized_20251031_0039.json",
}

# ç¦å‰‡èªï¼ˆç”»åƒæå†™èªãƒ»ãƒ¡ã‚¿èªï¼‰
FORBIDDEN = {
    "ç”»åƒ", "å†™çœŸ", "æ˜ ã£ã¦", "å†™ã£ã¦", "å†™ã‚‹", "è¦‹ãˆã¦ã„ã‚‹", "ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ",
    "ALT", "ä»£æ›¿ãƒ†ã‚­ã‚¹ãƒˆ", "ç”»åƒèª¬æ˜æ–‡", "ç”»åƒæè¿°", "Image", "Picture"
}

# ====== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ======
def read_products_from_csv(path: str) -> List[str]:
    products = []
    with open(path, "r", encoding="utf-8", newline="") as f:
        reader = csv.DictReader(f)
        for r in reader:
            name = (r.get("å•†å“å") or "").strip()
            if name:
                products.append(name)
    # é‡è¤‡é™¤å»ãƒ»é †åºç¶­æŒ
    seen = set()
    uniq = []
    for p in products:
        if p not in seen:
            uniq.append(p)
            seen.add(p)
    return uniq

def load_json_safe(path: str) -> Any:
    if not os.path.exists(path):
        return None
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None

def collect_local_knowledge() -> Dict[str, Any]:
    """output/semantics é…ä¸‹ã®çŸ¥è¦‹ã‚’èª­ã¿è¾¼ã‚“ã§ã€ä½¿ã„ã‚„ã™ã„å½¢ã«ã¾ã¨ã‚ã‚‹"""
    data = {}
    for key, fname in SEM_FILES.items():
        full = os.path.join(SEM_DIR, fname)
        data[key] = load_json_safe(full)
    return data

def top_strings_from(obj, keys: List[str], cap: int = 20) -> List[str]:
    """JSONãŒ list/dict ã©ã¡ã‚‰ã§ã‚‚ã€ä¸ãˆã‚‰ã‚ŒãŸ key å€™è£œã‹ã‚‰æ–‡å­—ã‚’æ‹¾ã†"""
    out = []
    if isinstance(obj, list):
        for x in obj:
            if isinstance(x, dict):
                for k in keys:
                    v = x.get(k)
                    if isinstance(v, str) and v.strip():
                        out.append(v.strip())
            elif isinstance(x, str) and x.strip():
                out.append(x.strip())
    elif isinstance(obj, dict):
        for k in keys:
            v = obj.get(k)
            if isinstance(v, list):
                for t in v:
                    if isinstance(t, str) and t.strip():
                        out.append(t.strip())
            elif isinstance(v, str) and v.strip():
                out.append(v.strip())
    # é‡è¤‡é™¤å»
    seen = set()
    dedup = []
    for s in out:
        if s not in seen:
            dedup.append(s)
            seen.add(s)
    return dedup[:cap]

def build_knowledge_text(product: str, kb: Dict[str, Any]) -> str:
    """
    ãƒ­ãƒ¼ã‚«ãƒ«çŸ¥è¦‹ã‚’è»½ãè¦ç´„ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆåŒ–ã€‚
    - å¸‚å ´èªå½™ï¼ˆmarket.vocabularyï¼‰
    - æ§‹é€ èªå½™ï¼ˆstructured.concepts / featuresï¼‰
    - ã‚¯ãƒ©ã‚¹ã‚¿èªï¼ˆlexical.clustersï¼‰
    - ã‚¹ã‚¿ã‚¤ãƒ«ï¼ˆpersonaï¼‰
    - æ­£è¦åŒ–ï¼ˆnormalized ã®ç¦å‰‡ãªã©ï¼‰
    """
    market_terms = top_strings_from(kb.get("market"), ["vocabulary", "term", "keyword"], cap=25)
    structured_terms = top_strings_from(kb.get("structured"), ["concepts", "features", "benefits"], cap=25)
    lexical_terms = top_strings_from(kb.get("lexical"), ["cluster", "tokens", "words"], cap=25)
    persona_tone = top_strings_from(kb.get("persona"), ["tone", "style", "voice"], cap=10)
    normalized_forbidden = []
    norm = kb.get("normalized")
    if isinstance(norm, dict):
        fw = norm.get("forbidden_words") or []
        if isinstance(fw, list):
            normalized_forbidden = [w for w in fw if isinstance(w, str)]

    # ç¦å‰‡ã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«ã®FORBIDDENã«åŠ ç®—ï¼ˆå‚ç…§ã®ã¿ï¼‰
    local_forbidden_text = "ã€".join(normalized_forbidden[:20]) if normalized_forbidden else ""

    parts = []
    if market_terms:
        parts.append(f"å¸‚å ´èªå½™: { 'ã€'.join(market_terms[:15]) }")
    if structured_terms:
        parts.append(f"æ§‹é€ èªå½™: { 'ã€'.join(structured_terms[:15]) }")
    if lexical_terms:
        parts.append(f"ã‚¯ãƒ©ã‚¹ã‚¿èª: { 'ã€'.join(lexical_terms[:15]) }")
    if persona_tone:
        parts.append(f"æ–‡ä½“: { 'ã€'.join(persona_tone[:5]) }")
    if local_forbidden_text:
        parts.append(f"ç¦å‰‡å€™è£œ: { local_forbidden_text }")

    summed = " / ".join(parts) if parts else "ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«çŸ¥è¦‹ã¯æœ€å°é™ï¼‰"
    # è£½å“åã‚’å…¥ã‚ŒãŸå°å…¥
    lead = f"å•†å“å: {product}"
    return f"{lead}\nçŸ¥è¦‹è¦ç´„: {summed}"


# ====== ãƒ†ã‚­ã‚¹ãƒˆæ•´å½¢ ======
NUM_PREFIX = re.compile(r"^\s*([0-9ï¼-ï¼™]+|[â‘ -â‘³]|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å])[\.\)\ï¼š:ã€]\s*")
QUOTE_EDGES = re.compile(r"^[\"'â€œâ€â€˜â€™ã€Œã€ï¼ˆ\(\[]+|[\"'â€œâ€â€˜â€™ã€ã€ï¼‰\)\]]+$")
EXTRA_SPACES = re.compile(r"\s+")

def sanitize_line(s: str) -> str:
    if not s:
        return ""
    # è¡Œé ­ã®ç•ªå·/ç®‡æ¡æ›¸ãæ¥é ­ã‚’é™¤å»
    s = NUM_PREFIX.sub("", s.strip())
    # ALT/ç”»åƒé–¢é€£èªã®ãƒ©ãƒ™ãƒ«æ’é™¤ï¼ˆALT: ãªã©ï¼‰
    s = re.sub(r"^\s*(ALT|Alt|alt)\s*[:ï¼š]\s*", "", s)
    # å¤–å´ã®å¼•ç”¨ç¬¦ç³»ã‚’å‰¥ãŒã™
    s = s.strip().strip('"').strip("'").strip("ã€Œ").strip("ã€").strip("ã€").strip("ã€").strip()
    # ç”»åƒæå†™èªã®æ˜ç¤ºãƒ©ãƒ™ãƒ«é™¤å»ï¼ˆæ–‡ä¸­ã¯å¾Œã§ãƒã‚§ãƒƒã‚¯ï¼‰
    return s

def ends_with_kuten(s: str) -> bool:
    return s.endswith("ã€‚")

def contains_forbidden(s: str) -> bool:
    for w in FORBIDDEN:
        if w in s:
            return True
    return False

def natural_clip_80_110(s: str) -> str:
    """
    80ã€œ110å­—ã¸è‡ªç„¶ã‚¯ãƒªãƒƒãƒ—ã€‚
    - 110å­—ã‚’è¶…ãˆã¦ã„ã‚Œã°æœ€å¾Œã®ã€Œã€‚ã€ã§åˆ‡ã‚‹
    - å¥ç‚¹ãŒç„¡ã„ or 80æœªæº€ãªã‚‰ã€ãã®ã¾ã¾ï¼ˆå¾Œæ®µã®AIå‡ºåŠ›å´ãŒæ¦‚ã­æ•´ãˆã‚‹æƒ³å®šï¼‰
    """
    s = s.strip()
    length = len(s)
    if length <= 110:
        return s
    # 110ä»¥å†…ã§æœ€å¾Œã®å¥ç‚¹
    cut = s[:110]
    last_kuten = cut.rfind("ã€‚")
    if last_kuten >= 70:  # 70 ä»¥ä¸Šã§å¥ç‚¹ãŒã‚ã‚Œã°ãã“ã¾ã§
        return cut[: last_kuten + 1]
    # å¥ç‚¹ãŒè¦‹å½“ãŸã‚‰ãªã‘ã‚Œã° 110 ã§åˆ‡ã‚‹ï¼ˆæœ€å¾Œã«å¥ç‚¹ä»˜ä¸ï¼‰
    clipped = s[:110]
    if not ends_with_kuten(clipped):
        clipped = clipped.rstrip("ã€ï¼Œ,") + "ã€‚"
    return clipped

def post_refine_line(s: str) -> str:
    """å¥ç‚¹å¿…é ˆãƒ»å¤‰ãªæœ«å°¾è¨˜å·é™¤å»ãƒ»ã‚¹ãƒšãƒ¼ã‚¹æ•´å½¢ãƒ»ç¦å‰‡å†ãƒã‚§ãƒƒã‚¯"""
    s = s.strip()
    # å¤‰ãªå¼•ç”¨ç¬¦/è¨˜å·ãŒæœ«å°¾ã«æ®‹ã£ã¦ã„ã‚Œã°èª¿æ•´
    s = s.rstrip('ã€ï¼Œ,;ï¼›:ï¼šâ€¦')
    # å¿…ãšå¥ç‚¹ã§çµ‚ãˆã‚‹
    if not ends_with_kuten(s):
        s += "ã€‚"
    # ç”»åƒæå†™èªãŒç´›ã‚ŒãŸã‚‰ã‚„ã•ã—ãé™¤å»ï¼ˆèªãã®ã‚‚ã®ã‚’è½ã¨ã™ï¼‰
    for w in list(FORBIDDEN):
        s = s.replace(w, "")
    # é€£ç¶šç©ºç™½ã‚’å˜ä¸€åŒ–
    s = EXTRA_SPACES.sub(" ", s).strip()
    return s


# ====== OpenAI ã‚³ãƒ¼ãƒ« ======
PROMPT_TEMPLATE = """ã‚ãªãŸã¯SEOæœ€é©åŒ–ã•ã‚ŒãŸå•†å“èª¬æ˜æ–‡ã®å°‚é–€ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚
ä»¥ä¸‹ã®ã€Œå•†å“åã€ã¨ã€ŒçŸ¥è¦‹è¦ç´„ã€ã‚’ã‚‚ã¨ã«ã€ç”»åƒã®æå†™ã‚’é¿ã‘ã¤ã¤ã€è‡ªç„¶ã§èª¬å¾—åŠ›ã®ã‚ã‚‹æ—¥æœ¬èªã®ALTãƒ†ã‚­ã‚¹ãƒˆã‚’20ä»¶ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

å³å®ˆæ¡ä»¶:
- ç®‡æ¡æ›¸ãã‚„ç•ªå·(1. 2. ãªã©)ã¯ç¦æ­¢ã€‚è¡Œé ­ã«æ•°å­—ã‚„è¨˜å·ã‚’ç½®ã‹ãªã„ã€‚
- å„ALTã¯1ã€œ2æ–‡æ§‹æˆã§è‡ªç„¶ãªæ–‡ä½“ã«ã™ã‚‹ã€‚
- å„ALTã¯å…¨è§’80ã€œ110æ–‡å­—ç¨‹åº¦ã‚’ç›®å®‰ã¨ã™ã‚‹ã€‚
- å¿…ãšå¥ç‚¹ï¼ˆã€‚ï¼‰ã§çµ‚ãˆã‚‹ã€‚
- ã€Œç”»åƒã€ã€Œå†™çœŸã€ã€Œæ˜ ã£ã¦ã„ã‚‹ã€ã€ŒALTã€ãªã©ã®èªã¯ä½¿ã‚ãªã„ã€‚
- å•†å“ã‚¹ãƒšãƒƒã‚¯ï¼æ©Ÿèƒ½ã€ã‚³ã‚¢ã‚³ãƒ³ãƒ”ã‚¿ãƒ³ã‚¹ã€æƒ³å®šãƒ¦ãƒ¼ã‚¶ãƒ¼ã€åˆ©ç”¨ã‚·ãƒ¼ãƒ³ã€ãƒ™ãƒãƒ•ã‚£ãƒƒãƒˆã‚’è‡ªç„¶ã«ç¹”ã‚Šäº¤ãœã‚‹ã€‚
- SEOã‚’æ„è­˜ã—ã€é©åˆ‡ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆå‹ç•ª/ç«¯å­/æ©Ÿç¨®/ã‚¹ãƒšãƒƒã‚¯/ç”¨é€”ãªã©ï¼‰ã‚’ä¸è‡ªç„¶ã«ãªã‚‰ãªã„ç¯„å›²ã§æ•£ã‚Šã°ã‚ã‚‹ã€‚
- å‡ºåŠ›ã¯ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€‚ALT 1ã€œ20ã‚’å„è¡Œã«1ã¤ãšã¤ã€åˆè¨ˆ20è¡Œã€‚JSONã‚„ãƒ©ãƒ™ãƒ«ã¯ä¸è¦ã€‚

ã€å•†å“åã€‘
{product}

ã€çŸ¥è¦‹è¦ç´„ã€‘
{knowledge}
"""

def call_openai_alt_lines(product: str, knowledge_text: str) -> List[str]:
    """OpenAIã«æŠ•ã’ã¦20è¡Œã®ALTãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ï¼ˆtextå‡ºåŠ›ï¼‰"""
    sys = "You are a helpful Japanese copywriter specialized in SEO-optimized e-commerce ALT texts."
    usr = PROMPT_TEMPLATE.format(product=product, knowledge=knowledge_text)

    for attempt in range(1, RETRY + 1):
        try:
            res = client.chat.completions.create(
    model=MODEL,
    messages=[
        {"role": "system", "content": sys},
        {"role": "user", "content": usr}
    ],
    max_completion_tokens=MAX_COMPLETION_TOKENS,
    response_format={"type": "text"},
)
            
            text = (res.choices[0].message.content or "").strip()
            if not text:
                raise ValueError("Empty content")
            # è¡Œã«åˆ†å‰²ã€ç©ºè¡Œã‚„ç®‡æ¡æ›¸ãè¨˜å·ã‚’å¾Œæ®µã§é™¤å»
            lines = [ln for ln in text.splitlines() if ln.strip()]
            return lines
        except Exception as e:
            if attempt < RETRY:
                print(f"âš ï¸ OpenAIã‚¨ãƒ©ãƒ¼({attempt}/{RETRY}): {e}")
                time.sleep(SLEEP_ON_FAIL)
                continue
            else:
                print(f"âŒ OpenAIå¤±æ•—ï¼ˆproduct={product[:18]}â€¦ï¼‰: {e}")
                return []

# ====== ALT ç”Ÿæˆï¼ˆæ•´å½¢è¾¼ã¿ï¼‰ ======
def generate_20_alts(product: str, kb_text: str) -> List[str]:
    raw_lines = call_openai_alt_lines(product, kb_text)

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šç©ºãªã‚‰é©å½“ãªé››å½¢ã‚’è¿”ã™ï¼ˆæœ€ä½é™ã®é€²è¡Œä¿è­·ï¼‰
    if not raw_lines:
        fallback = [f"{product}ã®ç‰¹é•·ã‚’æ´»ã‹ã—ã€æ—¥å¸¸ã®ä½¿ã„ã‚„ã™ã•ã¨å®‰å¿ƒæ„Ÿã‚’é«˜ã‚ã‚‹è¨­è¨ˆã§ã™ã€‚"]
        raw_lines = fallback * 20

    # ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼†ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    cleaned = []
    for ln in raw_lines:
        s = sanitize_line(ln)
        if not s:
            continue
        # ALTã‚„ç”»åƒæå†™èªã£ã½ã„ã‚‚ã®ãŒãƒ©ãƒ™ãƒ«çš„ã«ã‚ã‚Œã°é™¤å»æ¸ˆã¿ã€æ–‡ä¸­ã¯æœ€çµ‚ã§å‰Šã‚‹
        cleaned.append(s)

    # 20ä»¶ã¸æ•´å½¢
    alts = []
    for s in cleaned:
        if len(alts) >= 20:
            break
        # é•·ã•èª¿æ•´ï¼ˆè‡ªç„¶ã‚¯ãƒªãƒƒãƒ—ï¼‰
        s = natural_clip_80_110(s)
        # ä»•ä¸Šã’
        s = post_refine_line(s)
        # æ¥µç«¯ã«çŸ­ã„ã‚‚ã®/ç¦å‰‡æ··å…¥ã¯è¿‚å›ï¼ˆè»½ã„å†ç”Ÿæˆã¯ã›ãšã‚¹ã‚­ãƒƒãƒ—ã—ã¦æ¬¡ï¼‰
        if len(s) < 60 or contains_forbidden(s):
            continue
        alts.append(s)

    # ä¸è¶³ãªã‚‰ã€æ—¢å­˜ã‚’å°‘ã—å¤‰å½¢ã—ã¦è£œå……ï¼ˆèªå°¾ãƒ»æ¥ç¶šã‚’å¾®ä¿®æ­£ï¼‰
    if len(alts) < 20:
        base = alts[:] if alts else [f"{product}ã®ä½¿ã„å‹æ‰‹ã‚’é«˜ã‚ã€å®‰å¿ƒã—ã¦æ—¥å¸¸åˆ©ç”¨ã§ãã‚‹ãƒãƒ©ãƒ³ã‚¹è¨­è¨ˆã§ã™ã€‚"]
        j = 0
        while len(alts) < 20:
            seed = base[j % len(base)]
            # æœ«å°¾ã‚’è»½ãå¤‰ãˆã‚‹ï¼ˆèªå°¾ãƒ»å‰¯è©è¶³ã—ï¼‰
            variant = seed
            if variant.endswith("ã€‚"):
                variant = variant[:-1]
            tail = ["ã€‚", "ã€‚æ¯æ—¥ã®æºå¸¯æ€§ã«å„ªã‚Œã‚‹ã€‚", "ã€‚æ“ä½œãŒç›´æ„Ÿçš„ã§æ‰±ã„ã‚„ã™ã„ã€‚", "ã€‚å¿™ã—ã„æ—¥å¸¸ã§ã‚‚é ¼ã‚Œã‚‹ä»•æ§˜ã€‚"]
            variant = natural_clip_80_110((variant + tail[(j % len(tail))]).strip())
            variant = post_refine_line(variant)
            alts.append(variant)
            j += 1

    # æœ€çµ‚å®‰å…¨ç¶²ï¼šã¡ã‚‡ã†ã©20ã«åˆ‡ã‚Šè©°ã‚
    return alts[:20]


# ====== ãƒ¡ã‚¤ãƒ³ ======
def main():
    print("ğŸŒ¸ ALTé•·æ–‡ï¼ˆSEOï¼‹è‡ªç„¶æ–‡ï¼‰ç”Ÿæˆé–‹å§‹")
    products = read_products_from_csv(INPUT_CSV)
    print(f"âœ… å¯¾è±¡å•†å“æ•°: {len(products)}ä»¶")

    kb = collect_local_knowledge()

    rows = []
    for product in tqdm(products, desc="ğŸ§  ç”Ÿæˆä¸­", ncols=80):
        knowledge_text = build_knowledge_text(product, kb)
        alts = generate_20_alts(product, knowledge_text)
        row = {"å•†å“å": product}
        for i in range(20):
            row[f"ALT{i+1}"] = alts[i]
        rows.append(row)

    # CSVæ›¸ãå‡ºã—
    fieldnames = ["å•†å“å"] + [f"ALT{i+1}" for i in range(20)]
    with open(OUT_CSV, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for r in rows:
            writer.writerow(r)

    print(f"âœ… å‡ºåŠ›å®Œäº†: {OUT_CSV}")
    print("âœ… ä»•æ§˜: ALTã¯1ã€œ2æ–‡ãƒ»80ã€œ110å­—ãƒ»å¥ç‚¹çµ‚æ­¢ãƒ»ç”»åƒæå†™èª/ç®‡æ¡æ›¸ã/ç•ªå·ã¯ç¦æ­¢ã€‚ãƒ­ãƒ¼ã‚«ãƒ«çŸ¥è¦‹ã‚’æ´»ç”¨ã€‚")


if __name__ == "__main__":
    main()
import atlas_autosave_core
