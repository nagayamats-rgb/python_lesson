# -*- coding: utf-8 -*-
# -*- coding: utf-8 -*-
"""
ALTé•·æ–‡ï¼ˆSEOï¼‹è‡ªç„¶æ–‡ï¼‰ç”Ÿæˆ v3
- å…¥åŠ›: ./rakuten.csv ï¼ˆUTF-8, ãƒ˜ãƒƒãƒ€ã«ã€Œå•†å“åã€ï¼‰
- å‡ºåŠ›:
  1) output/ai_writer/alt_text_ai_raw_longform_v3.csv           â€¦ AIã®ç”Ÿå‡ºåŠ›ï¼ˆ20æœ¬/å•†å“ï¼‰
  2) output/ai_writer/alt_text_refined_final_longform_v3.csv    â€¦ ãƒ­ãƒ¼ã‚«ãƒ«æ•´å½¢å¾Œï¼ˆ80ã€œ110å­—ï¼‰
  3) output/ai_writer/alt_text_diff_longform_v3.csv             â€¦ raw/refined ã®æ¨ªä¸¦ã³æ¯”è¼ƒ
- çŸ¥è¦‹: ./output/semantics/ å†…ã® JSON/CSV ç¾¤ã‚’ã‚†ã‚‹ãé›†ç´„ã—ã€å•†å“ã”ã¨ã®ã€ŒçŸ¥è¦‹è¦ç´„ã€ã‚’ä»˜ä¸
- OpenAI:
    model            = gpt-4oï¼ˆ.env å›ºå®šï¼‰
    response_format  = {"type":"text"}
    max_completion_tokens = 1000
    temperature      = 1ï¼ˆå›ºå®šï¼šã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ä»»æ„æ¸©åº¦ãŒä¸å®‰å®šè¦å› ã«ãªã‚Šã‚„ã™ã„ãŸã‚ï¼‰
"""

import os
import re
import csv
import glob
import json
import time
from collections import defaultdict

from dotenv import load_dotenv

# ğŸŒ¸ KOTOHA å‡çµç®¡ç†è¦ï¼ˆã‹ã‚“ãªã‚ï¼‰çµ±åˆ
import freeze_manager_extended as freezer
freezer.auto_freeze_on_start(__file__, note="ALTé•·æ–‡ç”Ÿæˆ_v3ï¼ˆSEOï¼‹è‡ªç„¶æ–‡ï¼‰ï¼KOTOHA å‡çµç®¡ç†è¦ èµ·å‹•")

# tqdm ã¯è¦–è¦šçš„é€²æ—ã€æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ã‚‚å‹•ãã‚ˆã†ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
try:
    from tqdm.auto import tqdm
except Exception:
    def tqdm(x, **k): return x

# =========================
# 0) OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆç”¨
# =========================
try:
    from openai import OpenAI
except Exception:
    # æ–°æ—§SDKæ··åœ¨å¯¾ç­–ï¼šã‚¤ãƒ³ãƒãƒ¼ãƒˆå¤±æ•—æ™‚ã¯æ˜ç¤º
    raise SystemExit("openai SDK ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚`pip install openai python-dotenv` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

# =========================
# 1) å®šæ•°ãƒ»ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =========================
INPUT_CSV = "./rakuten.csv"  # UTF-8, ãƒ˜ãƒƒãƒ€ã«ã€Œå•†å“åã€
OUT_DIR = "./output/ai_writer"
RAW_PATH = os.path.join(OUT_DIR, "alt_text_ai_raw_longform_v3.csv")
REF_PATH = os.path.join(OUT_DIR, "alt_text_refined_final_longform_v3.csv")
DIFF_PATH = os.path.join(OUT_DIR, "alt_text_diff_longform_v3.csv")

SEMANTICS_DIR = "./output/semantics"  # çŸ¥è¦‹ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆå­˜åœ¨ã—ãªã„/ç©ºã§ã‚‚OKï¼‰

# ç¦å‰‡èªï¼ˆç”»åƒæå†™èªãƒ»ãƒ¡ã‚¿ãƒ»åº—èˆ—ãƒ¡ã‚¿ãªã©ï¼‰
FORBIDDEN = [
    "ç”»åƒ", "å†™çœŸ", "è¦‹ãŸç›®", "ä¸Šã®ç”»åƒ", "ä¸‹ã®å†™çœŸ", "å½“åº—", "å½“ç¤¾", "ãƒ¬ãƒ“ãƒ¥ãƒ¼", "ãƒ©ãƒ³ã‚­ãƒ³ã‚°",
    "ã‚¯ãƒªãƒƒã‚¯", "ã“ã¡ã‚‰", "ç«¶åˆ", "å„ªä½æ€§", "æ¥­ç•Œæœ€é«˜", "æœ€å®‰", "No.1", "ãƒŠãƒ³ãƒãƒ¼ãƒ¯ãƒ³", "å£²ä¸ŠNo1",
    "ãƒªãƒ³ã‚¯", "ãƒšãƒ¼ã‚¸", "ã‚«ãƒ¼ãƒˆ", "è³¼å…¥ã¯ã“ã¡ã‚‰", "ã‚¯ãƒªãƒƒã‚¯ã—ã¦", "é€æ–™ç„¡æ–™ï¼ˆç¢ºç´„ï¼‰", "è¿”é‡‘ä¿è¨¼",
]

# å¥èª­ç‚¹ãƒ»ç®‡æ¡æ›¸ããƒ‘ã‚¿ãƒ¼ãƒ³ cleanup
LEADING_ENUM_RE = re.compile(r"^\s*[\dâ‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©\-\*\ãƒ»\u2022]\s*[\.ï¼ã€]?\s*")
MULTI_COMMA_RE = re.compile(r"ã€{3,}")
WHITESPACE_RE = re.compile(r"\s+")

# æ–‡å­—æ•°åˆ¶å¾¡
RAW_MIN, RAW_MAX = 100, 130    # ã¾ãšAIã§ã“ã®ãƒ¬ãƒ³ã‚¸ã‚’ç‹™ã†
FINAL_MIN, FINAL_MAX = 80, 110  # ãƒ­ãƒ¼ã‚«ãƒ«ã§ã“ã®ãƒ¬ãƒ³ã‚¸ã«æ•´å½¢

# =========================
# 2) ç’°å¢ƒåˆæœŸåŒ–
# =========================
def init_env_and_client():
    load_dotenv(override=True)
    api_key = os.getenv("OPENAI_API_KEY", "").strip()
    if not api_key:
        raise SystemExit("OPENAI_API_KEY ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚.env ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    # ãƒ¢ãƒ‡ãƒ«ã¯ .env ã§ gpt-4o ã‚’å›ºå®šé‹ç”¨ï¼ˆä»–ã‚’æŒ‡å®šã—ã¦ã„ã¦ã‚‚ gpt-4o ã‚’å¼·åˆ¶ä½¿ç”¨ï¼‰
    model = "gpt-4o"
    client = OpenAI(api_key=api_key)
    return client, model

# =========================
# 3) å…¥åŠ›ï¼ˆå•†å“åï¼‰
# =========================
def load_products_from_csv(path: str):
    if not os.path.exists(path):
        raise SystemExit(f"å…¥åŠ›CSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path}")
    products = []
    with open(path, "r", encoding="utf-8", newline="") as f:
        reader = csv.DictReader(f)
        # ã€Œå•†å“åã€åˆ—ã‚’ç´ ç›´ã«å‚ç…§ã€‚å­˜åœ¨ã—ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼
        if "å•†å“å" not in reader.fieldnames:
            raise SystemExit("å…¥åŠ›CSVã«ã€å•†å“åã€ãƒ˜ãƒƒãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        for r in reader:
            nm = (r.get("å•†å“å") or "").strip()
            if nm:
                products.append(nm)
    # é‡è¤‡é™¤å»ï¼ˆé †åºç¶­æŒï¼‰
    seen, uniq = set(), []
    for nm in products:
        if nm not in seen:
            uniq.append(nm)
            seen.add(nm)
    return uniq

# =========================
# 4) çŸ¥è¦‹ã‚µãƒãƒªç”Ÿæˆï¼ˆã‚†ã‚‹çµåˆï¼‰
# =========================
def safe_load_json(path):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None

def summarize_knowledge():
    """
    ./output/semantics/ é…ä¸‹ã® JSON/CSV ã‚’ã–ã£ãã‚Šè¦ç´„ãƒ†ã‚­ã‚¹ãƒˆåŒ–
    - lexical_clusters_*.json    â†’ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç¾¤
    - structured_semantics_*.json â†’ æ§‹é€ çš„è¦³ç‚¹ï¼ˆç”¨é€”ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç­‰ï¼‰
    - market_vocab_*.json        â†’ å¸‚å ´èªå½™
    - styled_persona_*.json      â†’ ãƒˆãƒ¼ãƒ³ãƒ»æ–‡ä½“
    - normalized_*.json          â†’ ç¦å‰‡ãƒ»æ•´å½¢ãƒ«ãƒ¼ãƒ«
    - template_composer.json     â†’ è¡¨ç¾éª¨å­
    * å­˜åœ¨ã—ãªã„ï¼å½¢å¼ãŒé•ã†å ´åˆã¯ç„¡è¦–ï¼ˆå …ç‰¢é‹ç”¨ï¼‰
    """
    if not os.path.isdir(SEMANTICS_DIR):
        return "çŸ¥è¦‹: ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»ç”¨é€”ãƒ»å¯¾è±¡ãƒ»ã‚¹ãƒšãƒƒã‚¯ãƒ»é–¢é€£æ©Ÿç¨®åã‚’è‡ªç„¶ã«å«ã‚ã€ç”»åƒæå†™èªã¯ä½¿ã‚ãšã€2æ–‡ä»¥å†…ã§èª­ã¿ã‚„ã™ãã€‚", []

    files = glob.glob(os.path.join(SEMANTICS_DIR, "*.json"))
    # CSVã¯ä»Šå›ã¯èª­ã¿é£›ã°ã—ï¼ˆå¿…è¦ãªã‚‰å°†æ¥å¯¾å¿œï¼‰
    clusters, semantics, market, tone, normalized, template = [], [], [], [], [], []
    forbidden_local = []

    for p in files:
        name = os.path.basename(p).lower()
        data = safe_load_json(p)
        if not data:
            continue
        try:
            if "lexical_clusters" in name or "lexical" in name:
                # ex. {"clusters":[{"terms":[...]}]} / or [{"terms":[...]}]
                if isinstance(data, dict):
                    arr = data.get("clusters") or data.get("lexical") or []
                elif isinstance(data, list):
                    arr = data
                else:
                    arr = []
                for c in arr:
                    terms = c.get("terms") if isinstance(c, dict) else None
                    if isinstance(terms, list):
                        clusters.extend([t for t in terms if isinstance(t, str)])
            elif "structured_semantics" in name or "semantic" in name:
                # ex. {"concepts":[...], "scenes":[...], "targets":[...]}
                if isinstance(data, dict):
                    semantics.extend([w for k in ["concepts", "scenes", "targets", "use_cases"]
                                      for w in (data.get(k) or []) if isinstance(w, str)])
            elif "market_vocab" in name or "market" in name:
                # ex. [{"vocabulary":"MagSafe"}, ...] or ["MagSafe", "PD"]
                if isinstance(data, list):
                    for v in data:
                        if isinstance(v, dict) and "vocabulary" in v and isinstance(v["vocabulary"], str):
                            market.append(v["vocabulary"])
                        elif isinstance(v, str):
                            market.append(v)
                elif isinstance(data, dict):
                    vocab = data.get("vocabulary") or data.get("vocab") or []
                    if isinstance(vocab, list):
                        market.extend([x for x in vocab if isinstance(x, str)])
            elif "styled_persona" in name or "persona" in name:
                # ex. {"tone":{"style":"ã€œ","register":"ã€œ"}}
                if isinstance(data, dict):
                    t = data.get("tone") or {}
                    if isinstance(t, dict):
                        for v in t.values():
                            if isinstance(v, str):
                                tone.append(v)
            elif "normalized" in name or "forbid" in name:
                # ex. {"forbidden_words":["ç”»åƒ","å†™çœŸ",...]}
                if isinstance(data, dict):
                    fw = data.get("forbidden_words") or []
                    forbidden_local.extend([w for w in fw if isinstance(w, str)])
            elif "template_composer" in name:
                # ex. {"hints":["ã‚¹ãƒšãƒƒã‚¯â†’å¼·ã¿â†’å¯¾è±¡â†’ã‚·ãƒ¼ãƒ³â†’ä¾¿ç›Š"]}
                if isinstance(data, dict):
                    hints = data.get("hints") or data.get("templates") or []
                    template.extend([h for h in hints if isinstance(h, str)])
        except Exception:
            # å½¢å¼ãŒé•ã£ã¦ã„ã¦ã‚‚ç„¡è¦–
            pass

    # ç¦å‰‡ã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«ã¨ãƒãƒ¼ã‚¸ã—ã¦ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–
    all_forbidden = list({*FORBIDDEN, *forbidden_local})

    # è»½ãè¦ç´„ãƒ†ã‚­ã‚¹ãƒˆåŒ–ï¼ˆé•·æ–‡ã«ã—ã™ããªã„ï¼‰
    def cap_join(xs, n):  # nå€‹ã¾ã§æ‹¾ã£ã¦ "ã€" ã§ã¤ãªã
        xs = [x for x in xs if isinstance(x, str)]
        return "ã€".join(xs[:n]) if xs else ""

    cluster_txt  = cap_join(list(dict.fromkeys(clusters)), 12)
    market_txt   = cap_join(list(dict.fromkeys(market)),   12)
    sem_txt      = cap_join(list(dict.fromkeys(semantics)), 8)
    tone_txt     = cap_join(list(dict.fromkeys(tone)),      4)
    tmpl_txt     = cap_join(list(dict.fromkeys(template)),  3)

    kb = "çŸ¥è¦‹: "
    parts = []
    if cluster_txt:
        parts.append(f"èªå½™: {cluster_txt}")
    if market_txt:
        parts.append(f"å¸‚å ´èª: {market_txt}")
    if sem_txt:
        parts.append(f"æ§‹é€ : {sem_txt}")
    if tmpl_txt:
        parts.append(f"éª¨å­: {tmpl_txt}")
    if tone_txt:
        parts.append(f"ãƒˆãƒ¼ãƒ³: {tone_txt}")
    if not parts:
        kb += "ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»ç”¨é€”ãƒ»å¯¾è±¡ãƒ»ã‚¹ãƒšãƒƒã‚¯ãƒ»é–¢é€£æ©Ÿç¨®åã‚’è‡ªç„¶ã«å«ã‚ã€"
    else:
        kb += " / ".join(parts) + "ã€‚"
    kb += "ç”»åƒæå†™èªã¯ä½¿ã‚ãšã€2æ–‡ä»¥å†…ã€èª­ã¿ã‚„ã™ãè‡ªç„¶ã«ã€‚"
    return kb, all_forbidden

# =========================
# 5) ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆAIï¼‰
# =========================
SYSTEM_PROMPT = (
    "ã‚ãªãŸã¯ECç”»åƒã®ALTãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œã‚‹ãƒ—ãƒ­ã®æ—¥æœ¬èªã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚"
    "ç›®çš„ã¯ã€æ¥½å¤©ã®ã‚µã‚¤ãƒˆå†…SEOã«å¼·ã„è‡ªç„¶æ–‡ã®ALTã‚’20æœ¬ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã™ã€‚"
    "ä»¥ä¸‹ã®å¿…é ˆãƒ«ãƒ¼ãƒ«ã‚’å³å®ˆã—ã¦ãã ã•ã„ï¼š\n"
    "ãƒ»ç”»åƒã‚„å†™çœŸã®æå†™èªï¼ˆä¾‹ï¼šç”»åƒã€å†™çœŸã€è¦‹ãŸç›®ã€ä¸Šã®ç”»åƒ ç­‰ï¼‰ã¯ä½¿ã‚ãªã„ã€‚\n"
    "ãƒ»ECãƒ¡ã‚¿èªï¼ˆå½“åº—ã€ãƒ¬ãƒ“ãƒ¥ãƒ¼ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã€ãƒªãƒ³ã‚¯ã€è³¼å…¥ã¯ã“ã¡ã‚‰ ç­‰ï¼‰ã¯ä½¿ã‚ãªã„ã€‚\n"
    "ãƒ»ç«¶åˆæ¯”è¼ƒã‚„ã€Œç«¶åˆå„ªä½æ€§ã€ã®ã‚ˆã†ãªãƒ¡ã‚¿è¡¨ç¾ã¯ç¦æ­¢ã€‚\n"
    f"ãƒ»å„æ–‡ã¯å…¨è§’ç´„{RAW_MIN}ã€œ{RAW_MAX}æ–‡å­—ç¨‹åº¦ã€1ã€œ2æ–‡ã§è‡ªç„¶ã«ã€‚å¿…ãšå¥ç‚¹ã€Œã€‚ã€ã§çµ‚ãˆã‚‹ã€‚\n"
    "ãƒ»ç®‡æ¡æ›¸ãã‚„ç•ªå·ï¼ˆ1. 2. ãƒ» ãªã©ï¼‰ã‚„ãƒ©ãƒ™ãƒ«ï¼ˆALT: ç­‰ï¼‰ã¯ä»˜ã‘ãªã„ã€‚\n"
    "ãƒ»å•†å“åãƒ»å¯¾å¿œæ©Ÿç¨®ãƒ»ã‚¹ãƒšãƒƒã‚¯ãƒ»æ©Ÿèƒ½ãƒ»ç”¨é€”ãƒ»å¯¾è±¡ãƒ»ãƒ™ãƒãƒ•ã‚£ãƒƒãƒˆã‚’è‡ªç„¶ã«ç¹”ã‚Šè¾¼ã‚€ï¼ˆè©°ã‚è¾¼ã¿ç¦æ­¢ï¼‰ã€‚\n"
    "ãƒ»å‡ºåŠ›ã¯20è¡Œã®ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ï¼ˆJSONã‚„è¨˜å·ãªã—ï¼‰ã€‚"
)

def build_user_prompt(product: str, knowledge_text: str, forbidden_words):
    forbid_txt = "ã€".join(sorted(set([w for w in forbidden_words if isinstance(w, str)])))
    hint = (
        "æ§‹æˆãƒ’ãƒ³ãƒˆï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ã«ã›ãšè‡ªç„¶ã«ï¼‰ï¼š"
        "å•†å“ã‚¹ãƒšãƒƒã‚¯â†’ã‚³ã‚¢ã‚³ãƒ³ãƒ”ã‚¿ãƒ³ã‚¹â†’ã©ã‚“ãªäººâ†’ã‚·ãƒ¼ãƒ³â†’ãƒ™ãƒãƒ•ã‚£ãƒƒãƒˆã€‚"
    )
    return (
        f"å•†å“å: {product}\n"
        f"{knowledge_text}\n"
        f"{hint}\n"
        f"ç¦æ­¢èªï¼ˆçµ¶å¯¾ã«ä½¿ã‚ãªã„ï¼‰: {forbid_txt}\n"
        "20è¡Œã§ã€å„è¡Œã¯ã²ã¨ã¤ã®è‡ªç„¶æ–‡ï¼ˆ1ã€œ2æ–‡å†…ï¼‰ã§æ›¸ã„ã¦ãã ã•ã„ã€‚"
    )

# =========================
# 6) OpenAI å‘¼ã³å‡ºã—
# =========================
def call_openai_20_lines(client, model, product, kb_text, forbidden_words, retry=3, wait=5):
    """
    20è¡Œã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™ã€‚å¤±æ•—æ™‚ã¯ãƒªãƒˆãƒ©ã‚¤ã€‚
    """
    user_prompt = build_user_prompt(product, kb_text, forbidden_words)

    last_err = None
    for _ in range(retry):
        try:
            res = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt},
                ],
                response_format={"type": "text"},
                max_completion_tokens=1000,
                temperature=1,
            )
            content = (res.choices[0].message.content or "").strip()
            if content:
                lines = [ln.strip() for ln in content.split("\n") if ln.strip()]
                # ã€Œ- ã€ã‚„ã€Œ1. ã€ãªã©ã®é ­ã‚’å‰¥ãŒã™
                clean = []
                for ln in lines:
                    ln2 = LEADING_ENUM_RE.sub("", ln)
                    ln2 = ln2.strip("ãƒ»-â€”â—ã€€")
                    if ln2:
                        clean.append(ln2)
                # è¡ŒãŒå¤šã™ãã‚Œã°20ã«åˆ‡ã‚‹ã€å°‘ãªã‘ã‚Œã°å¾Œã§è£œå®Œ
                return clean[:60]  # ä½™åˆ†ã«è¿”ã£ã¦ãã‚‹ã“ã¨ãŒã‚ã‚‹ã®ã§æœ€å¤§60ã¾ã§ä¿æŒï¼ˆå¾Œã§20æŠ½å‡ºï¼‰
        except Exception as e:
            last_err = e
            time.sleep(wait)
    # å…¨å¤±æ•—
    raise RuntimeError(f"OpenAIå¿œç­”ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ: {last_err}")

# =========================
# 7) ãƒ­ãƒ¼ã‚«ãƒ«æ•´å½¢
# =========================
def soft_clip_sentence(text: str, min_len=FINAL_MIN, max_len=FINAL_MAX) -> str:
    """
    ç›®æ¨™ãƒ¬ãƒ³ã‚¸ã‚’ç›®æŒ‡ã—ã¦æœ«å°¾ã®ã€Œã€‚ã€ã§è‡ªç„¶ã‚«ãƒƒãƒˆã€‚
    é•·ã™ãã‚‹: 120ã¾ã§è¨±å®¹â†’æœ€å¾Œã®ã€Œã€‚ã€ã¾ã§è©°ã‚ã¦80ã€œ110ç‹™ã„
    çŸ­ã™ãã‚‹: ãã®ã¾ã¾è¿”ã™ï¼ˆå¾Œç¶šã§åˆ¥è¡Œã¨å·®æ›¿ãˆï¼‰
    """
    t = text.strip()
    # å¥ç‚¹çµ‚æ­¢ãªã‘ã‚Œã°ä»˜ã‘ã‚‹
    if not t.endswith("ã€‚"):
        t += "ã€‚"
    # ä½™è¨ˆãªã‚¹ãƒšãƒ¼ã‚¹åœ§ç¸®
    t = WHITESPACE_RE.sub(" ", t)
    t = MULTI_COMMA_RE.sub("ã€ã€", t)
    # æ¥µç«¯ãªç®‡æ¡æ›¸ããƒ»ç•ªå·ã‚’æƒé™¤
    t = LEADING_ENUM_RE.sub("", t).strip("ãƒ»-â€”â—ã€€")

    if len(t) > 120:
        cut = t[:120]
        # ç›´è¿‘ã®å¥ç‚¹ä½ç½®
        p = cut.rfind("ã€‚")
        if p != -1:
            t = cut[:p+1]
        else:
            t = cut
    # æœ€çµ‚å®‰å…¨: ç¦å‰‡èªå‰Šé™¤ï¼ˆå®Œå…¨é™¤å»ï¼‰
    for ng in FORBIDDEN:
        if ng and ng in t:
            t = t.replace(ng, "")
    return t.strip()

def refine_20_lines(raw_lines):
    """
    20æœ¬ã«æ•´å½¢ã™ã‚‹:
      - ç„¡åŠ¹è¡Œãƒ»çŸ­ã™ãã‚‹è¡Œã‚’é™¤å»
      - å¥ç‚¹çµ‚æ­¢
      - é•·ã™ãã‚‹è¡Œã‚’è‡ªç„¶ã‚«ãƒƒãƒˆ
      - é¡ä¼¼è¡Œã®é‡è¤‡é™¤å»
      - 20æœ¬æœªæº€ã¯å¯èƒ½ãªé™ã‚Šãƒãƒ¼ã‚¸/è£œå®Œï¼ˆã“ã“ã§ã¯å˜ç´”è¤‡å†™ç¦æ­¢ï¼šé‡è¤‡ã¯è½ã¨ã™ï¼‰
    """
    # æ­£è¦åŒ–ï¼†ãƒ•ã‚£ãƒ«ã‚¿
    norm = []
    for ln in raw_lines:
        if not ln:
            continue
        # ç•°å¸¸ãªç®‡æ¡æ›¸ã/ç•ªå·ã‚’é™¤å»
        ln = LEADING_ENUM_RE.sub("", ln).strip("ãƒ»-â€”â—ã€€")
        # å¥ç‚¹çµ‚æ­¢åŒ– + é•·ã•æ•´å½¢
        ln = soft_clip_sentence(ln)
        # 1æ–‡å­—ã ã‘ãªã©ç„¡åŠ¹è¡Œã¯æ¨ã¦ã‚‹
        if len(ln) < 15:
            continue
        norm.append(ln)

    # é¡ä¼¼é™¤å»ï¼ˆç°¡æ˜“ï¼šå‰å¾Œä¸€è‡´ã‚„åŒä¸€æ–‡ã®é‡è¤‡ã‚’æ¨ã¦ã‚‹ï¼‰
    uniq = []
    seen = set()
    for ln in norm:
        key = ln
        if key not in seen:
            uniq.append(ln)
            seen.add(key)

    # 80ã€œ110å­—ã®å¯†åº¦ã«å¯„ã›ãŸã„ã®ã§ã€110è¶…ã¯æ–‡æœ«ã¾ã§è©°ã‚ã‚‹
    refined = [soft_clip_sentence(ln) for ln in uniq]

    # 20æœ¬ã¡ã‚‡ã†ã©ã«æ•´ãˆã‚‹
    # è¶³ã‚Šãªã„ã¨ãã¯ã€æ—¢å­˜æ–‡ã‚’è»½ãå¤‰å½¢ï¼ˆèªå°¾å„ªå…ˆç½®æ›ï¼‰ã—ã¦åŸ‹ã‚ã‚‹
    def light_variation(s: str) -> str:
        s2 = s
        s2 = s2.replace("ã—ã¾ã™ã€‚", "ã§ãã¾ã™ã€‚")
        s2 = s2.replace("ã§ãã¾ã™ã€‚", "ã—ã‚„ã™ã„ã§ã™ã€‚")
        s2 = s2.replace("ã§ã™ã€‚", "ã«ãªã‚Šã¾ã™ã€‚")
        # ãã‚Œã§ã‚‚åŒä¸€ãªã‚‰å¥å†…ã®ä¸€èªã‚’å¾®ä¿®æ­£ï¼ˆèª­ç‚¹è¿½åŠ ãªã©æœ€å°å¤‰åŒ–ï¼‰
        if s2 == s:
            s2 = re.sub(r"(\w{2,})", r"\1ã€", s, count=1)
            s2 = s2.replace("ã€ã€", "ã€")
            s2 = s2.replace("ã€ã€", "ã€")
            if not s2.endswith("ã€‚"):
                s2 += "ã€‚"
        return soft_clip_sentence(s2)

    i = 0
    while len(refined) < 20 and refined:
        refined.append(light_variation(refined[i % len(refined)]))
        i += 1

    # å¤šã™ãã‚‹ãªã‚‰å…ˆé ­20æœ¬ã ã‘
    return refined[:20]

# =========================
# 8) æ›¸ãå‡ºã—
# =========================
def ensure_outdir():
    os.makedirs(OUT_DIR, exist_ok=True)

def write_raw(products, all_raw):
    with open(RAW_PATH, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["å•†å“å"] + [f"ALT_raw_{i+1}" for i in range(20)])
        for p, lines in zip(products, all_raw):
            row = [p] + (lines[:20] + [""] * max(0, 20 - len(lines)))
            w.writerow(row)

def write_refined(products, all_refined):
    with open(REF_PATH, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["å•†å“å"] + [f"ALT_{i+1}" for i in range(20)])
        for p, lines in zip(products, all_refined):
            row = [p] + (lines[:20] + [""] * max(0, 20 - len(lines)))
            w.writerow(row)

def write_diff(products, all_raw, all_refined):
    with open(DIFF_PATH, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        header = ["å•†å“å"] + [f"ALT_raw_{i+1}" for i in range(20)] + [f"ALT_refined_{i+1}" for i in range(20)]
        w.writerow(header)
        for p, r, ref in zip(products, all_raw, all_refined):
            r_line = (r[:20] + [""] * max(0, 20 - len(r)))
            ref_line = (ref[:20] + [""] * max(0, 20 - len(ref)))
            w.writerow([p] + r_line + ref_line)

# =========================
# 9) ãƒ¡ã‚¤ãƒ³
# =========================
def main():
    print("ğŸŒ¸ ALTé•·æ–‡ç”Ÿæˆ v3ï¼ˆSEOï¼‹è‡ªç„¶æ–‡ï¼‹å·®åˆ†å¯è¦–åŒ–ï¼‰")
    client, model = init_env_and_client()
    ensure_outdir()

    products = load_products_from_csv(INPUT_CSV)
    print(f"âœ… å¯¾è±¡å•†å“: {len(products)}ä»¶")

    knowledge_text, forbidden_local = summarize_knowledge()
    forbidden_all = list({*FORBIDDEN, *forbidden_local})

    all_raw, all_refined = [], []

    for p in tqdm(products, desc="ğŸ§  AIç”Ÿæˆä¸­", total=len(products)):
        # 1) AIç”Ÿæˆï¼ˆ20è¡Œä»¥ä¸Šã‚’è¿”ã™ã“ã¨ã‚‚ã‚ã‚‹ï¼‰
        try:
            raw_lines = call_openai_20_lines(client, model, p, knowledge_text, forbidden_all)
        except Exception as e:
            # ã©ã†ã—ã¦ã‚‚ãƒ€ãƒ¡ãªã‚‰ã€ãƒ€ãƒŸãƒ¼20æœ¬ï¼ˆç©ºç™½ã¯é¿ã‘ã‚‹ï¼‰
            raw_lines = [f"{p} ã®ä½¿ã„å‹æ‰‹ã‚’é«˜ã‚ã‚‹è¨­è¨ˆã§ã€æ—¥å¸¸ã®ä¸ä¾¿ã‚’è§£æ¶ˆã—ã¾ã™ã€‚"] * 20

        # 2) ãƒ­ãƒ¼ã‚«ãƒ«æ•´å½¢ â†’ 20æœ¬
        refined_lines = refine_20_lines(raw_lines)

        all_raw.append(raw_lines[:20])
        all_refined.append(refined_lines)

        # è»½ã„ã‚¹ãƒ­ãƒƒãƒˆãƒªãƒ³ã‚°
        time.sleep(0.2)

    # 3) æ›¸ãå‡ºã—
    write_raw(products, all_raw)
    write_refined(products, all_refined)
    write_diff(products, all_raw, all_refined)

    # çµ±è¨ˆãƒ­ã‚°
    def avg_len(blocks):
        lens = [len(x) for lines in blocks for x in lines if x]
        return (sum(lens) / max(1, len(lens)))

    print("âœ… å‡ºåŠ›å®Œäº†:")
    print(f"   - AIç”Ÿå‡ºåŠ›: {RAW_PATH}")
    print(f"   - æ•´å½¢å¾Œ   : {REF_PATH}")
    print(f"   - å·®åˆ†æ¯”è¼ƒ : {DIFF_PATH}")
    print(f"ğŸ“ æ–‡å­—æ•°(å¹³å‡): raw={avg_len(all_raw):.1f} / refined={avg_len(all_refined):.1f}")
    print("ğŸ”’ ä»•æ§˜: ")
    print(f"   - AIã¯ç´„{RAW_MIN}ã€œ{RAW_MAX}å­—ãƒ»1ã€œ2æ–‡ã€å¥ç‚¹çµ‚æ­¢ã€ç¦å‰‡é©ç”¨ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰")
    print(f"   - ãƒ­ãƒ¼ã‚«ãƒ«æ•´å½¢ã§{FINAL_MIN}ã€œ{FINAL_MAX}å­—ã«è‡ªç„¶ã‚«ãƒƒãƒˆã€é‡è¤‡é™¤å»ãƒ»å¥ç‚¹è£œå®Œãƒ»ç¦å‰‡å†é©ç”¨")

if __name__ == "__main__":
    main()
import atlas_autosave_core
