# -*- coding: utf-8 -*-
"""
ALTé•·æ–‡ç”Ÿæˆ v3_proï¼ˆSEOï¼‹è‡ªç„¶æ–‡ï¼‹çŸ¥è¦‹å±•é–‹å®Œå…¨å¾©å…ƒï¼‰
----------------------------------------------------------
- å…¥åŠ›: ./rakuten.csvï¼ˆUTF-8, ãƒ˜ãƒƒãƒ€ã«ã€Œå•†å“åã€ï¼‰
- å‡ºåŠ›:
  1) output/ai_writer/alt_text_ai_raw_longform_v3.csv
  2) output/ai_writer/alt_text_refined_final_longform_v3.csv
  3) output/ai_writer/alt_text_diff_longform_v3.csv
- çŸ¥è¦‹: ./output/semantics/ å†…ã® JSON ç¾¤ã‚’çµ±åˆã—ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å±•é–‹
- ç‰¹å¾´:
    âœ… å¥ç‚¹åˆ†å‰²ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆæ”¹è¡Œå–ªå¤±é˜²æ­¢ï¼‰
    âœ… JSONçŸ¥è¦‹èªç¾¤ã‚’SEOãƒ¯ãƒ¼ãƒ‰ã¨ã—ã¦å±•é–‹
    âœ… AIé•·æ–‡åŒ–æŒ‡ä»¤ï¼ˆ100ã€œ130å­—ãƒ»1ã€œ2æ–‡ï¼‰
    âœ… æ¬ æALTã¯ãƒ­ãƒ¼ã‚«ãƒ«è£œå®Œï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬æ§‹æ–‡ï¼‰
"""

import os, re, csv, glob, json, time, random
from dotenv import load_dotenv
from openai import OpenAI
from tqdm import tqdm

# =========================
# å®šæ•°ãƒ»ãƒ‘ã‚¹è¨­å®š
# =========================
INPUT_CSV = "./rakuten.csv"
OUT_DIR = "./output/ai_writer"
RAW_PATH = os.path.join(OUT_DIR, "alt_text_ai_raw_longform_v3.csv")
REF_PATH = os.path.join(OUT_DIR, "alt_text_refined_final_longform_v3.csv")
DIFF_PATH = os.path.join(OUT_DIR, "alt_text_diff_longform_v3.csv")
SEMANTICS_DIR = "./output/semantics"

# ç¦å‰‡èª
FORBIDDEN = [
    "ç”»åƒ","å†™çœŸ","è¦‹ãŸç›®","å½“åº—","å½“ç¤¾","ãƒ¬ãƒ“ãƒ¥ãƒ¼","ãƒ©ãƒ³ã‚­ãƒ³ã‚°","ã‚¯ãƒªãƒƒã‚¯","ãƒªãƒ³ã‚¯",
    "ã‚«ãƒ¼ãƒˆ","è³¼å…¥ã¯ã“ã¡ã‚‰","ãƒšãƒ¼ã‚¸","æœ€å®‰","No.1","ãƒŠãƒ³ãƒãƒ¼ãƒ¯ãƒ³","å£²ä¸Š","é€æ–™ç„¡æ–™",
    "è¿”é‡‘ä¿è¨¼","å„ªä½æ€§","ç«¶åˆ","è©•ä¾¡","å£ã‚³ãƒŸ"
]

RAW_MIN, RAW_MAX = 100, 130
FINAL_MIN, FINAL_MAX = 80, 110

# =========================
# OpenAIåˆæœŸåŒ–
# =========================
def init_env_and_client():
    load_dotenv(override=True)
    api_key = os.getenv("OPENAI_API_KEY", "").strip()
    if not api_key:
        raise SystemExit("âŒ OPENAI_API_KEY ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚.env ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    model = "gpt-4o"
    client = OpenAI(api_key=api_key)
    return client, model

# =========================
# å•†å“åèª­è¾¼
# =========================
def load_products(path):
    with open(path, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        return list({r["å•†å“å"].strip() for r in reader if r.get("å•†å“å")})

# =========================
# çŸ¥è¦‹è¦ç´„ï¼‹SEOèªå±•é–‹
# =========================
def safe_load_json(path):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None

def summarize_knowledge():
    if not os.path.isdir(SEMANTICS_DIR):
        return "çŸ¥è¦‹: ã‚¹ãƒšãƒƒã‚¯ãƒ»ç”¨é€”ãƒ»å¯¾è±¡ã‚’è‡ªç„¶ã«å«ã‚ã€SEOã«å¼·ã„ALTã‚’ä½œæˆã€‚", []

    files = glob.glob(os.path.join(SEMANTICS_DIR, "*.json"))
    keywords = []
    forbidden_local = []

    for p in files:
        data = safe_load_json(p)
        if not data: continue
        name = os.path.basename(p).lower()
        try:
            if "lexical" in name:
                if isinstance(data, dict):
                    for c in data.get("clusters", []):
                        keywords += [t for t in c.get("terms", []) if isinstance(t, str)]
            elif "market" in name:
                if isinstance(data, list):
                    keywords += [v.get("vocabulary","") for v in data if isinstance(v, dict)]
            elif "semantic" in name:
                if isinstance(data, dict):
                    for k in ["concepts","targets","use_cases"]:
                        keywords += data.get(k, [])
            elif "forbid" in name:
                if isinstance(data, dict):
                    forbidden_local += data.get("forbidden_words", [])
        except Exception:
            pass

    all_forbidden = list({*FORBIDDEN, *forbidden_local})
    seo_terms = [k for k in keywords if isinstance(k, str)][:20]
    joined_terms = "ã€".join(seo_terms)
    kb = f"çŸ¥è¦‹: ä»¥ä¸‹ã®èªã‚’è‡ªç„¶ã«å«ã‚ã¦ALTã‚’ç”Ÿæˆã€‚æ¨å¥¨èªå¥: {joined_terms}ã€‚1ã€œ2æ–‡æ§‹æˆã§ã€å¥ç‚¹ã§çµ‚ãˆã‚‹è‡ªç„¶æ–‡ã‚’ä½œæˆã€‚"
    return kb, all_forbidden

# =========================
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¼·åŒ–
# =========================
SYSTEM_PROMPT = (
    "ã‚ãªãŸã¯æ¥½å¤©å¸‚å ´ã®SEOæœ€é©åŒ–ã‚’å°‚é–€ã¨ã™ã‚‹æ—¥æœ¬èªã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚\n"
    "ç›®çš„ã¯ã€è‡ªç„¶ã§èª­ã¿ã‚„ã™ãã€ã‹ã¤æ¤œç´¢ã«å¼·ã„ALTãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆã™ã‚‹ã“ã¨ã§ã™ã€‚\n"
    f"å„ALTã¯å…¨è§’{RAW_MIN}ã€œ{RAW_MAX}æ–‡å­—ã€1ã€œ2æ–‡æ§‹æˆã§å¥ç‚¹ã€Œã€‚ã€ã§çµ‚ãˆã‚‹ã“ã¨ã€‚\n"
    "ç®‡æ¡æ›¸ãã€ç•ªå·ã€è¨˜å·ã€ç”»åƒæå†™èªã¯ç¦æ­¢ã€‚\n"
    "å¿…ãšæ”¹è¡Œã§20è¡Œã«åˆ†ã‘ã¦å‡ºåŠ›ã€‚çŸ­ã„æ–‡ã‚„çœç•¥ã¯ç¦æ­¢ã§ã™ã€‚"
)

def build_user_prompt(product, kb_text, forbidden):
    forbid_txt = "ã€".join(forbidden)
    return (
        f"å•†å“å: {product}\n"
        f"{kb_text}\n"
        f"ç¦æ­¢èª: {forbid_txt}\n"
        f"20è¡Œã§ã€1è¡Œã‚ãŸã‚Š100ã€œ130æ–‡å­—ã®è‡ªç„¶ãªALTãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã€‚"
    )

# =========================
# OpenAIå‘¼ã³å‡ºã—ï¼ˆå¥ç‚¹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰
# =========================
def call_openai_20_lines(client, model, product, kb_text, forbidden, retry=3, wait=5):
    user_prompt = build_user_prompt(product, kb_text, forbidden)
    last_err = None
    for _ in range(retry):
        try:
            res = client.chat.completions.create(
                model=model,
                messages=[
                    {"role":"system","content":SYSTEM_PROMPT},
                    {"role":"user","content":user_prompt},
                ],
                response_format={"type":"text"},
                max_completion_tokens=1000,
                temperature=1
            )
            txt = (res.choices[0].message.content or "").strip()
            if not txt:
                continue
            lines = [x.strip() for x in txt.split("\n") if x.strip()]
            if len(lines) <= 1:
                lines = re.split(r"(?<=ã€‚)\s*", txt)
            clean = []
            for ln in lines:
                ln2 = re.sub(r"^\s*[\d\-\*ãƒ»\.]+\s*", "", ln).strip("ãƒ»-â€”â—ã€€")
                if ln2:
                    clean.append(ln2)
            return clean[:60]
        except Exception as e:
            last_err = e
            time.sleep(wait)
    raise RuntimeError(f"OpenAIå¿œç­”ã‚’å–å¾—ã§ãã¾ã›ã‚“: {last_err}")

# =========================
# ãƒ­ãƒ¼ã‚«ãƒ«è£œå®Œ
# =========================
SPEC_TEMPLATES = [
    "é«˜å‡ºåŠ›ã§å®‰å®šã—ãŸå……é›»ã‚’å®Ÿç¾ã™ã‚‹ãƒ¢ãƒ‡ãƒ«",
    "è»½é‡ã§æŒã¡é‹ã³ã«ä¾¿åˆ©ãªè¨­è¨ˆ",
    "è¤‡æ•°ãƒ‡ãƒã‚¤ã‚¹å¯¾å¿œã®é«˜æ€§èƒ½ã‚¿ã‚¤ãƒ—",
    "è€ä¹…æ€§ã¨ãƒ‡ã‚¶ã‚¤ãƒ³æ€§ã‚’å…¼ã­å‚™ãˆãŸä»•æ§˜",
]
BENEFIT_TEMPLATES = [
    "æ—¥å¸¸ã‹ã‚‰ãƒ“ã‚¸ãƒã‚¹ã¾ã§å¿«é©ã«ä½¿ç”¨ã§ãã¾ã™ã€‚",
    "æŒã¡é‹ã³ã«ã‚‚ä¾¿åˆ©ã§ã€å¤–å‡ºæ™‚ã«ã‚‚æœ€é©ã§ã™ã€‚",
    "é•·ãå®‰å¿ƒã—ã¦ä½¿ãˆã‚‹å“è³ªã§ã™ã€‚",
    "ã‚¹ãƒãƒ¼ãƒˆãªç”Ÿæ´»ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚",
]

def generate_local_alt(product):
    spec = random.choice(SPEC_TEMPLATES)
    benefit = random.choice(BENEFIT_TEMPLATES)
    return f"{product}ã¯{spec}ã€‚{benefit}"

# =========================
# æ•´å½¢
# =========================
def refine_20_alt_lines(raw_lines, product):
    refined = []
    for ln in raw_lines:
        ln = ln.strip()
        if len(ln) > 150:
            parts = re.split(r"(?<=ã€‚)\s*", ln)
            refined.extend(parts[:3])
        elif len(ln) < 40:
            refined.append(generate_local_alt(product))
        else:
            if not ln.endswith("ã€‚"): ln += "ã€‚"
            refined.append(ln)
    if not refined:
        refined = [generate_local_alt(product)] * 20
    while len(refined) < 20:
        refined.append(generate_local_alt(product))
    return refined[:20]

# =========================
# æ›¸ãå‡ºã—
# =========================
def write_csv(path, header, rows):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path,"w",encoding="utf-8",newline="") as f:
        w=csv.writer(f); w.writerow(header); w.writerows(rows)

# =========================
# ãƒ¡ã‚¤ãƒ³
# =========================
def main():
    print("ğŸŒ¸ ALTé•·æ–‡ç”Ÿæˆ v3_proï¼ˆSEOï¼‹è‡ªç„¶æ–‡ï¼‹çŸ¥è¦‹å±•é–‹ï¼‰")
    client, model = init_env_and_client()
    products = load_products(INPUT_CSV)
    print(f"âœ… å¯¾è±¡å•†å“: {len(products)}ä»¶")

    kb_text, forb = summarize_knowledge()
    all_raw, all_ref = [], []

    for p in tqdm(products, desc="ğŸ§  ç”Ÿæˆä¸­"):
        try:
            raw = call_openai_20_lines(client, model, p, kb_text, forb)
        except Exception:
            raw = [generate_local_alt(p)]*20
        refined = refine_20_alt_lines(raw, p)
        all_raw.append(raw[:20]); all_ref.append(refined)
        time.sleep(0.2)

    write_csv(RAW_PATH, ["å•†å“å"]+[f"ALT_raw_{i+1}" for i in range(20)], [[p]+r for p,r in zip(products,all_raw)])
    write_csv(REF_PATH, ["å•†å“å"]+[f"ALT_{i+1}" for i in range(20)], [[p]+r for p,r in zip(products,all_ref)])
    print(f"âœ… å‡ºåŠ›å®Œäº†\nğŸ“„ AIç”Ÿå‡ºåŠ›: {RAW_PATH}\nğŸ“„ æ•´å½¢å¾Œ: {REF_PATH}")

if __name__ == "__main__":
    main()
import atlas_autosave_core
