# -*- coding: utf-8 -*-
"""
ALTé•·æ–‡ç”Ÿæˆ v3ï¼ˆSEOï¼‹è‡ªç„¶æ–‡ï¼‹ã‹ã‚“ãªã‚è£œå®Œï¼‰
-------------------------------------------------
- å…¥åŠ›: ./rakuten.csvï¼ˆUTF-8, ãƒ˜ãƒƒãƒ€ã€Œå•†å“åã€ï¼‰
- å‡ºåŠ›:
    1) output/ai_writer/alt_text_ai_raw_longform_v3.csv
    2) output/ai_writer/alt_text_refined_final_longform_v3.csv
    3) output/ai_writer/alt_text_diff_longform_v3.csv
- çŸ¥è¦‹: ./output/semantics/ å†…ã® JSON ç¾¤ã‚’è¦ç´„æ´»ç”¨
- ãƒ­ãƒ¼ã‚«ãƒ«è£œå®Œå±¤: local_refinerï¼ˆã‹ã‚“ãªã‚ï¼‰
"""

import os, re, csv, glob, json, time, random
from dotenv import load_dotenv
from openai import OpenAI
from tqdm import tqdm

# =========================
# å®šæ•°ãƒ»å…¥å‡ºåŠ›ãƒ‘ã‚¹
# =========================
INPUT_CSV = "./rakuten.csv"
OUT_DIR = "./output/ai_writer"
RAW_PATH = os.path.join(OUT_DIR, "alt_text_ai_raw_longform_v3.csv")
REF_PATH = os.path.join(OUT_DIR, "alt_text_refined_final_longform_v3.csv")
DIFF_PATH = os.path.join(OUT_DIR, "alt_text_diff_longform_v3.csv")
SEMANTICS_DIR = "./output/semantics"

# ç¦å‰‡èª
FORBIDDEN = [
    "ç”»åƒ","å†™çœŸ","è¦‹ãŸç›®","å½“åº—","å½“ç¤¾","ãƒ©ãƒ³ã‚­ãƒ³ã‚°","ãƒ¬ãƒ“ãƒ¥ãƒ¼","ã‚¯ãƒªãƒƒã‚¯","ãƒªãƒ³ã‚¯","ã‚«ãƒ¼ãƒˆ",
    "è³¼å…¥ã¯ã“ã¡ã‚‰","ãƒšãƒ¼ã‚¸","æœ€å®‰","No.1","ãƒŠãƒ³ãƒãƒ¼ãƒ¯ãƒ³","å£²ä¸Š","é€æ–™ç„¡æ–™","è¿”é‡‘ä¿è¨¼","å„ªä½æ€§","ç«¶åˆ","è©•ä¾¡","å£ã‚³ãƒŸ"
]

# æ–‡å­—æ•°ç¯„å›²
RAW_MIN, RAW_MAX = 100, 130
FINAL_MIN, FINAL_MAX = 80, 110

# =========================
# OpenAIåˆæœŸåŒ–
# =========================
def init_env_and_client():
    load_dotenv(override=True)
    api_key = os.getenv("OPENAI_API_KEY", "").strip()
    if not api_key:
        raise SystemExit("OPENAI_API_KEY ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚.env ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    model = "gpt-4o"
    client = OpenAI(api_key=api_key)
    return client, model

# =========================
# å•†å“åèª­è¾¼
# =========================
def load_products(path):
    with open(path, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        return list({r["å•†å“å"].strip() for r in reader if r.get("å•†å“å")})

# =========================
# çŸ¥è¦‹ã‚µãƒãƒª
# =========================
def safe_load_json(path):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None

def summarize_knowledge():
    if not os.path.isdir(SEMANTICS_DIR):
        return "çŸ¥è¦‹: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»ç”¨é€”ãƒ»å¯¾è±¡ãƒ»ã‚¹ãƒšãƒƒã‚¯ã‚’è‡ªç„¶ã«å«ã‚ã¦2æ–‡ä»¥å†…ã§ã€‚", []

    files = glob.glob(os.path.join(SEMANTICS_DIR, "*.json"))
    clusters, market, semantics, forbidden_local = [], [], [], []
    for p in files:
        data = safe_load_json(p)
        if not data:
            continue
        name = os.path.basename(p).lower()
        try:
            if "lexical" in name and isinstance(data, dict):
                clusters.extend(sum([c.get("terms", []) for c in data.get("clusters", [])], []))
            elif "market" in name and isinstance(data, list):
                market.extend([v.get("vocabulary", "") for v in data if isinstance(v, dict)])
            elif "semantic" in name and isinstance(data, dict):
                for k in ["concepts", "targets", "use_cases"]:
                    semantics.extend(data.get(k, []))
            elif "forbid" in name and isinstance(data, dict):
                forbidden_local.extend(data.get("forbidden_words", []))
        except Exception:
            pass
    all_forbidden = list({*FORBIDDEN, *forbidden_local})
    kb = f"çŸ¥è¦‹: {', '.join(list(set(clusters + market + semantics))[:15])}ã€‚è‡ªç„¶ã§SEOã«å¼·ã„æ–‡ç« ã‚’ç”Ÿæˆã€‚"
    return kb, all_forbidden

# =========================
# AIãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
# =========================
SYSTEM_PROMPT = (
    "ã‚ãªãŸã¯æ¥½å¤©å¸‚å ´ã®å•†å“ç”»åƒALTãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆã™ã‚‹ãƒ—ãƒ­ã®æ—¥æœ¬èªã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚"
    "ç›®çš„ã¯ã€SEOã«å¼·ãè‡ªç„¶ãªæ–‡ç« ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã™ã€‚"
    "ç¦æ­¢: ç”»åƒã‚„å†™çœŸã®æå†™èªã€åº—èˆ—ãƒ¡ã‚¿èªã€ç«¶åˆæ¯”è¼ƒè¡¨ç¾ã€‚\n"
    f"å„ALTã¯å…¨è§’{RAW_MIN}ã€œ{RAW_MAX}æ–‡å­—ã€å¥ç‚¹ã€Œã€‚ã€ã§çµ‚ãˆã‚‹è‡ªç„¶æ–‡ã«ã—ã¦ãã ã•ã„ã€‚\n"
    "JSONã‚„ç•ªå·ã¯ä½¿ã‚ãšã€20è¡Œã®ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’å‡ºåŠ›ã€‚"
)

def build_user_prompt(product, kb_text, forbidden):
    forbid_txt = "ã€".join(forbidden)
    hint = "æ§‹æˆãƒ’ãƒ³ãƒˆ: ã‚¹ãƒšãƒƒã‚¯â†’å¼·ã¿â†’å¯¾è±¡â†’ã‚·ãƒ¼ãƒ³â†’ä¾¿ç›Šã€‚"
    return f"å•†å“å: {product}\n{kb_text}\n{hint}\nç¦æ­¢èª: {forbid_txt}"

# =========================
# OpenAIå‘¼ã³å‡ºã—
# =========================
def call_openai_20_lines(client, model, product, kb_text, forbidden, retry=3, wait=5):
    user_prompt = build_user_prompt(product, kb_text, forbidden)
    for _ in range(retry):
        try:
            res = client.chat.completions.create(
                model=model,
                messages=[{"role":"system","content":SYSTEM_PROMPT},
                          {"role":"user","content":user_prompt}],
                response_format={"type":"text"},
                max_completion_tokens=1000,
                temperature=1
            )
            txt = (res.choices[0].message.content or "").strip()
            if txt:
                lines = [re.sub(r"^\s*[\d\-\*ãƒ»\.]+\s*", "", x).strip() for x in txt.split("\n") if x.strip()]
                return lines[:60]
        except Exception as e:
            time.sleep(wait)
    return []

# =========================
# ã‹ã‚“ãªã‚ï¼ˆlocal_refinerï¼‰çµ±åˆ
# =========================
FORBIDDEN_LOCAL = FORBIDDEN
SPEC_TEMPLATES = [
    "é«˜å‡ºåŠ›ã§å®‰å®šã—ãŸå……é›»ã‚’å®Ÿç¾ã™ã‚‹ãƒ¢ãƒ‡ãƒ«",
    "ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆãªãŒã‚‰é«˜æ€§èƒ½ãªè¨­è¨ˆ",
    "è¤‡æ•°ãƒ‡ãƒã‚¤ã‚¹ã«å¯¾å¿œã—ãŸå¤šæ©Ÿèƒ½ä»•æ§˜",
    "è»½é‡ã‹ã¤æŒã¡é‹ã³ã‚„ã™ã„ãƒ‡ã‚¶ã‚¤ãƒ³",
    "å®‰å…¨ä¿è­·æ©Ÿèƒ½ã‚’å‚™ãˆãŸé«˜å“è³ªãƒ¢ãƒ‡ãƒ«",
    "USB-Cå¯¾å¿œã§æ±ç”¨æ€§ã«å„ªã‚ŒãŸã‚¿ã‚¤ãƒ—",
    "ãƒã‚°ãƒãƒƒãƒˆã§ç°¡å˜ã«è£…ç€ã§ãã‚‹è¨­è¨ˆ",
    "ã‚¹ã‚¿ãƒ³ãƒ‰æ©Ÿèƒ½ä»˜ãã§ãƒ‡ã‚¹ã‚¯ä½œæ¥­ã«ã‚‚ä¾¿åˆ©",
    "è€ä¹…æ€§ã®é«˜ã„ç´ æã‚’æ¡ç”¨ã—ãŸè¨­è¨ˆ",
    "æœ€æ–°ã®é«˜é€Ÿé€šä¿¡è¦æ ¼ã«å¯¾å¿œã—ãŸãƒ¢ãƒ‡ãƒ«",
]
CONNECTORS = ["ã§", "ã«ã‚ˆã‚Š", "ã‚’å‚™ãˆ", "ã‚’æ´»ã‹ã—ã¦", "ã‚’æ­è¼‰ã—"]
BENEFIT_TEMPLATES = [
    "ãƒ“ã‚¸ãƒã‚¹ã‹ã‚‰æ—¥å¸¸ä½¿ã„ã¾ã§å¿«é©ã«ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚",
    "å¤–å‡ºå…ˆã§ã‚‚ã‚¹ãƒˆãƒ¬ã‚¹ãªãä½¿ç”¨ã§ãã¾ã™ã€‚",
    "é•·æ™‚é–“ã®ä½¿ç”¨ã«ã‚‚å®‰å®šã—ãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã—ã¾ã™ã€‚",
    "ã‚¹ãƒãƒ¼ãƒˆãªæš®ã‚‰ã—ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ä¾¿åˆ©ã‚¢ã‚¤ãƒ†ãƒ ã§ã™ã€‚",
    "ã‚ªãƒ•ã‚£ã‚¹ã‚„å®¶åº­ã§ã‚‚å¹…åºƒãæ´»èºã—ã¾ã™ã€‚",
    "ã©ãªãŸã§ã‚‚ç›´æ„Ÿçš„ã«ä½¿ã„ã‚„ã™ã„è¨­è¨ˆã§ã™ã€‚",
    "ä½¿ã†ãŸã³ã«å¿«é©ã•ã‚’æ„Ÿã˜ã‚‰ã‚Œã‚‹ä»•ä¸ŠãŒã‚Šã§ã™ã€‚",
    "æ€¥ãªå……é›»ã«ã‚‚ç´ æ—©ãå¯¾å¿œã§ãã¾ã™ã€‚",
    "æŒã¡é‹ã³ã«ã‚‚ä¾¿åˆ©ã§å‡ºå¼µã‚„æ—…è¡Œã«ã‚‚æœ€é©ã§ã™ã€‚",
    "é«˜å“è³ªãªç´ æã§é•·ãå®‰å¿ƒã—ã¦ä½¿ãˆã¾ã™ã€‚",
]

def generate_local_alt(product):
    spec = random.choice(SPEC_TEMPLATES)
    connector = random.choice(CONNECTORS)
    benefit = random.choice(BENEFIT_TEMPLATES)
    text = f"{product}ã¯{spec}{connector}{benefit}"
    for ng in FORBIDDEN_LOCAL:
        text = text.replace(ng, "")
    if not text.endswith("ã€‚"):
        text += "ã€‚"
    return text.strip()

def refine_alt_text(line, product):
    if not line or len(line.strip()) < 40:
        return generate_local_alt(product)
    line = re.sub(r"\s+", " ", line.strip())
    if not line.endswith("ã€‚"):
        line += "ã€‚"
    for ng in FORBIDDEN_LOCAL:
        line = line.replace(ng, "")
    if random.random() < 0.35:
        for a, b in [("ã§ã™ã€‚","ã«ãªã‚Šã¾ã™ã€‚"),("ã—ã¾ã™ã€‚","ã§ãã¾ã™ã€‚"),("ã§ãã¾ã™ã€‚","ã—ã‚„ã™ã„ã§ã™ã€‚")]:
            if line.endswith(a):
                line = line[:-len(a)] + b
                break
    return line

def refine_20_alt_lines(raw_lines, product):
    refined = [refine_alt_text(x, product) for x in raw_lines if x.strip()]
    while len(refined) < 20:
        refined.append(generate_local_alt(product))
    return refined[:20]

# =========================
# å‡ºåŠ›
# =========================
def write_csv(path, header, rows):
    with open(path,"w",encoding="utf-8",newline="") as f:
        w=csv.writer(f); w.writerow(header); w.writerows(rows)

# =========================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# =========================
def main():
    print("ğŸŒ¸ ALTé•·æ–‡ç”Ÿæˆ v3ï¼‹ã‹ã‚“ãªã‚è£œå®Œ é–‹å§‹")
    client, model = init_env_and_client()
    products = load_products(INPUT_CSV)
    kb_text, forb = summarize_knowledge()
    all_raw, all_ref = [], []

    for p in tqdm(products, desc="ğŸ§  ç”Ÿæˆä¸­"):
        raw = call_openai_20_lines(client, model, p, kb_text, forb)
        if not raw:
            raw = [generate_local_alt(p)]*20
        refined = refine_20_alt_lines(raw, p)
        all_raw.append(raw[:20]); all_ref.append(refined)

    os.makedirs(OUT_DIR, exist_ok=True)
    write_csv(RAW_PATH, ["å•†å“å"]+[f"ALT_raw_{i+1}" for i in range(20)],
              [[p]+r for p,r in zip(products,all_raw)])
    write_csv(REF_PATH, ["å•†å“å"]+[f"ALT_{i+1}" for i in range(20)],
              [[p]+r for p,r in zip(products,all_ref)])
    print("âœ… å‡ºåŠ›å®Œäº†")
    print(f"ğŸ“„ AIç”Ÿå‡ºåŠ›: {RAW_PATH}\nğŸ“„ æ•´å½¢å¾Œ: {REF_PATH}")

if __name__ == "__main__":
    main()
import atlas_autosave_core
