# -*- coding: utf-8 -*-
"""
ALTé•·æ–‡ï¼ˆSEOï¼‹è‡ªç„¶æ–‡ï¼‰ç”Ÿæˆ v4.1r3ï¼ˆæ§‹é€ çŸ¥è­˜ï¼‹gpt-5ï¼‰
"""

import os, re, csv, glob, json, time
from dotenv import load_dotenv
from collections import defaultdict

try:
    from tqdm.auto import tqdm
except Exception:
    def tqdm(x, **k): return x

try:
    from openai import OpenAI
except Exception:
    raise SystemExit("openai SDK ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚pip install openai python-dotenv")

# === å®šæ•° ===
INPUT_CSV = "./rakuten.csv"
OUT_DIR   = "./output/ai_writer"
RAW_PATH  = os.path.join(OUT_DIR, "alt_text_ai_raw_longform_v4.1.csv")
REF_PATH  = os.path.join(OUT_DIR, "alt_text_refined_final_longform_v4.1.csv")
DIFF_PATH = os.path.join(OUT_DIR, "alt_text_diff_longform_v4.1.csv")
SEMANTICS_DIR = "./output/semantics"

FORBIDDEN = [
    "ç”»åƒ","å†™çœŸ","è¦‹ãŸç›®","ä¸Šã®ç”»åƒ","ä¸‹ã®å†™çœŸ","å½“åº—","å½“ç¤¾","ãƒ¬ãƒ“ãƒ¥ãƒ¼","ãƒ©ãƒ³ã‚­ãƒ³ã‚°",
    "ã‚¯ãƒªãƒƒã‚¯","ã“ã¡ã‚‰","ç«¶åˆ","å„ªä½æ€§","æ¥­ç•Œæœ€é«˜","æœ€å®‰","No.1","ãƒŠãƒ³ãƒãƒ¼ãƒ¯ãƒ³","ãƒªãƒ³ã‚¯",
    "ãƒšãƒ¼ã‚¸","ã‚«ãƒ¼ãƒˆ","è³¼å…¥ã¯ã“ã¡ã‚‰","é€æ–™ç„¡æ–™","è¿”é‡‘ä¿è¨¼"
]

RAW_MIN, RAW_MAX = 100, 130
FINAL_MIN, FINAL_MAX = 80, 110

LEADING_ENUM_RE = re.compile(r"^\s*[\dâ‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©\-\*\ãƒ»\u2022]\s*[\.ï¼Žã€]?\s*")
WHITESPACE_RE   = re.compile(r"\s+")
MULTI_COMMA_RE  = re.compile(r"ã€{3,}")

# === åˆæœŸåŒ– ===
def init_client():
    load_dotenv(override=True)
    key = os.getenv("OPENAI_API_KEY", "").strip()
    if not key:
        raise SystemExit("OPENAI_API_KEY ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚.env ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    model = os.getenv("OPENAI_MODEL", "gpt-5").strip() or "gpt-5"
    client = OpenAI(api_key=key)
    return client, model

# === å•†å“èª­ã¿è¾¼ã¿ ===
def load_products(path):
    if not os.path.exists(path):
        raise SystemExit(f"CSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path}")
    with open(path, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        if "å•†å“å" not in reader.fieldnames:
            raise SystemExit("ãƒ˜ãƒƒãƒ€ã«ã€Žå•†å“åã€åˆ—ãŒå¿…è¦ã§ã™ã€‚")
        items = [r["å•†å“å"].strip() for r in reader if r.get("å•†å“å")]
    uniq, seen = [], set()
    for nm in items:
        if nm not in seen:
            uniq.append(nm)
            seen.add(nm)
    return uniq

# === çŸ¥è¦‹æ§‹é€ åŒ– ===
def safe_load_json(path):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None

def summarize_knowledge_structured():
    if not os.path.isdir(SEMANTICS_DIR):
        payload = {
            "keywords": [], "market_terms": [], "scenes": [], "targets": [],
            "benefits": [], "tone": ["è‡ªç„¶ã§èª­ã¿ã‚„ã™ã", "SEOåŠ¹æžœã‚’æ„è­˜"], "forbidden": FORBIDDEN
        }
        return payload

    files = glob.glob(os.path.join(SEMANTICS_DIR, "*.json"))
    payload = defaultdict(list)
    forbid = []

    for p in files:
        data = safe_load_json(p)
        if not data: continue
        name = os.path.basename(p).lower()

        if "lexical" in name or "cluster" in name:
            arr = data.get("clusters") if isinstance(data, dict) else data
            if isinstance(arr, list):
                for c in arr:
                    if isinstance(c, dict):
                        payload["keywords"] += c.get("terms", [])

        if "market" in name:
            v = data.get("vocabulary") or data.get("vocab") or []
            if isinstance(v, list):
                payload["market_terms"] += [x for x in v if isinstance(x, str)]

        if "semantic" in name:
            # âœ… dict / list ä¸¡å¯¾å¿œ
            if isinstance(data, dict):
                for k in ["concepts", "scenes", "targets", "benefits"]:
                    payload[k] += [x for x in (data.get(k) or []) if isinstance(x, str)]
            elif isinstance(data, list):
                for item in data:
                    if isinstance(item, dict):
                        for k in ["concepts", "scenes", "targets", "benefits"]:
                            payload[k] += [x for x in (item.get(k) or []) if isinstance(x, str)]

        if "persona" in name:
            # âœ… dict / list ä¸¡å¯¾å¿œ
            if isinstance(data, list):
                payload["tone"] += [v for v in data if isinstance(v, str)]
            elif isinstance(data, dict):
                tone = data.get("tone") or {}
                if isinstance(tone, dict):
                    payload["tone"] += [v for v in tone.values() if isinstance(v, str)]

        if "forbid" in name or "normalize" in name:
            # âœ… dict / list ä¸¡å¯¾å¿œ
            if isinstance(data, list):
                forbid += [w for w in data if isinstance(w, str)]
            elif isinstance(data, dict):
                fw = data.get("forbidden_words") or data.get("forbid") or []
                if isinstance(fw, list):
                    forbid += [w for w in fw if isinstance(w, str)]

    payload["forbidden"] = list({*FORBIDDEN, *forbid})
    for k, v in payload.items():
        payload[k] = list(dict.fromkeys(v))
    return payload

# === ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ===
SYSTEM_PROMPT = (
    "ã‚ãªãŸã¯æ—¥æœ¬èªžECã‚µã‚¤ãƒˆã®SEOãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°å°‚é–€å®¶ã§ã™ã€‚"
    "æ¥½å¤©ç”¨ã®å•†å“ç”»åƒALTãƒ†ã‚­ã‚¹ãƒˆã‚’20æœ¬ç”Ÿæˆã—ã¾ã™ã€‚"
    "ä»¥ä¸‹ã®æ§‹é€ åŒ–çŸ¥è­˜ã‚’å‚è€ƒã«ã€è‡ªç„¶ã§é­…åŠ›çš„ãªæ–‡ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\n"
    "ã€å¿…é ˆãƒ«ãƒ¼ãƒ«ã€‘\n"
    "ãƒ»ç”»åƒãƒ»å†™çœŸãªã©ã®æå†™èªžã¯ç¦æ­¢ã€‚\n"
    "ãƒ»ãƒ¡ã‚¿è¡¨ç¾ï¼ˆç«¶åˆå„ªä½æ€§ãƒ»No.1 ç­‰ï¼‰ã¯ç¦æ­¢ã€‚\n"
    "ãƒ»å…¨è§’ç´„100ã€œ130å­—ã€1ã€œ2æ–‡ã§è‡ªç„¶ã«å¥ç‚¹ã§çµ‚ãˆã‚‹ã€‚\n"
    "ãƒ»å•†å“åãƒ»å¯¾å¿œæ©Ÿç¨®ãƒ»æ©Ÿèƒ½ãƒ»ç”¨é€”ãƒ»ãƒ™ãƒãƒ•ã‚£ãƒƒãƒˆã‚’è‡ªç„¶ã«å«ã‚ã‚‹ã€‚\n"
    "ãƒ»20è¡Œã®ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚"
)

def build_user_prompt(product, knowledge):
    payload = json.dumps(knowledge, ensure_ascii=False, indent=2)
    return (
        f"å•†å“å: {product}\n"
        "æ¬¡ã®æ§‹é€ åŒ–çŸ¥è­˜ã‚’å‚è€ƒã«ã—ã¦ã€æ¥½å¤©SEOã«æœ€é©åŒ–ã•ã‚ŒãŸè‡ªç„¶ãªALTæ–‡ã‚’20ä»¶ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n"
        f"{payload}\n"
        "å„è¡Œã¯ç‹¬ç«‹ã—ãŸè‡ªç„¶æ–‡ã§ã€å¥ç‚¹ã€Œã€‚ã€ã§çµ‚ã‚ã‚‹ã“ã¨ã€‚"
    )

# === OpenAI å‘¼ã³å‡ºã— ===
def call_openai_lines(client, model, product, knowledge, retry=3, wait=5):
    user_prompt = build_user_prompt(product, knowledge)
    last_err = None
    for _ in range(retry):
        try:
            res = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt}
                ],
                response_format={"type": "text"},
                max_completion_tokens=1800,
                temperature=1
            )
            txt = (res.choices[0].message.content or "").strip()
            if txt:
                lines = [LEADING_ENUM_RE.sub("", ln).strip("ãƒ»-â€”â—ã€€") for ln in txt.split("\n") if ln.strip()]
                return lines[:60]
        except Exception as e:
            last_err = e
            time.sleep(wait)
    raise RuntimeError(f"OpenAIå¿œç­”ãªã—: {last_err}")

# === æ•´å½¢ ===
def soft_clip(t):
    t = t.strip()
    if not t.endswith("ã€‚"):
        t += "ã€‚"
    t = WHITESPACE_RE.sub(" ", t)
    t = MULTI_COMMA_RE.sub("ã€ã€", t)
    if len(t) > 120:
        cut = t[:120]
        p = cut.rfind("ã€‚")
        t = cut[:p+1] if p != -1 else cut
    for ng in FORBIDDEN:
        t = t.replace(ng, "")
    return t.strip()

def refine_lines(raw):
    valid = []
    for ln in raw:
        if not ln:
            continue
        s = soft_clip(ln)
        if len(s) < 15:
            continue
        valid.append(s)
    uniq = list(dict.fromkeys(valid))
    refined = [soft_clip(x) for x in uniq][:20]
    while len(refined) < 20 and refined:
        refined.append(refined[len(refined) % len(refined)])
    return refined[:20]

# === å‡ºåŠ› ===
def ensure_outdir():
    os.makedirs(OUT_DIR, exist_ok=True)

def write_csv(path, header, rows):
    with open(path, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(header)
        w.writerows(rows)

# === ãƒ¡ã‚¤ãƒ³ ===
def main():
    print("ðŸŒ¸ ALTé•·æ–‡ç”Ÿæˆ v4.1r3ï¼ˆæ§‹é€ çŸ¥è­˜ï¼‹gpt-5 å®Œå…¨å®‰å®šç‰ˆï¼‰")
    client, model = init_client()
    ensure_outdir()
    products = load_products(INPUT_CSV)
    print(f"âœ… å•†å“æ•°: {len(products)}ä»¶")

    knowledge = summarize_knowledge_structured()
    raws, refs = [], []

    for p in tqdm(products, desc="ðŸ§  ç”Ÿæˆä¸­", total=len(products)):
        try:
            raw = call_openai_lines(client, model, p, knowledge)
        except Exception as e:
            raw = [f"{p} ã®ç‰¹å¾´ã‚’æ´»ã‹ã—ãŸè¨­è¨ˆã§æ—¥å¸¸ã®åˆ©ä¾¿æ€§ã‚’é«˜ã‚ã¾ã™ã€‚"] * 20
        ref = refine_lines(raw)
        raws.append(raw[:20])
        refs.append(ref)
        time.sleep(0.2)

    write_csv(RAW_PATH, ["å•†å“å"] + [f"ALT_raw_{i+1}" for i in range(20)],
              [[p] + r + [""] * (20 - len(r)) for p, r in zip(products, raws)])
    write_csv(REF_PATH, ["å•†å“å"] + [f"ALT_{i+1}" for i in range(20)],
              [[p] + r + [""] * (20 - len(r)) for p, r in zip(products, refs)])
    write_csv(DIFF_PATH,
              ["å•†å“å"] + [f"ALT_raw_{i+1}" for i in range(20)] + [f"ALT_refined_{i+1}" for i in range(20)],
              [[p] + raws[i] + refs[i] for i, p in enumerate(products)])

    avg = lambda xs: sum(len(x) for l in xs for x in l if x) / max(1, sum(len(l) for l in xs))
    print("âœ… å‡ºåŠ›å®Œäº†:")
    print(f"   raw={RAW_PATH}\n   refined={REF_PATH}\n   diff={DIFF_PATH}")
    print(f"ðŸ“ å¹³å‡æ–‡å­—æ•° raw={avg(raws):.1f}, refined={avg(refs):.1f}")

if __name__ == "__main__":
    main()
import atlas_autosave_core
