# -*- coding: utf-8 -*-
"""
v3.3 ALTé•·æ–‡ç”Ÿæˆï¼ˆSEOï¼‹è‡ªç„¶æ–‡ï¼‹gpt-5æœ€é©åŒ–ç‰ˆï¼‰
- å…¥åŠ›: ./rakuten.csvï¼ˆUTF-8, ãƒ˜ãƒƒãƒ€ã«ã€Œå•†å“åã€ï¼‰
- å‡ºåŠ›:
    1. output/ai_writer/alt_text_ai_raw_longform_v4.csv
    2. output/ai_writer/alt_text_refined_final_longform_v4.csv
    3. output/ai_writer/alt_text_diff_longform_v4.csv
- ãƒ¢ãƒ‡ãƒ«: gpt-5ï¼ˆ.envã§å›ºå®šï¼‰
"""

import os
import re
import csv
import time
import json
import glob
from dotenv import load_dotenv
from collections import defaultdict

try:
    from openai import OpenAI
except ImportError:
    raise SystemExit("openai SDKãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚pip install openai python-dotenv ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

try:
    from tqdm.auto import tqdm
except Exception:
    def tqdm(x, **k): return x

# ====== åŸºæœ¬è¨­å®š ======
INPUT_CSV = "./rakuten.csv"
OUT_DIR = "./output/ai_writer"
RAW_PATH = os.path.join(OUT_DIR, "alt_text_ai_raw_longform_v4.csv")
REF_PATH = os.path.join(OUT_DIR, "alt_text_refined_final_longform_v4.csv")
DIFF_PATH = os.path.join(OUT_DIR, "alt_text_diff_longform_v4.csv")
SEMANTICS_DIR = "./output/semantics"

FORBIDDEN = [
    "ç”»åƒ", "å†™çœŸ", "è¦‹ãŸç›®", "ä¸Šã®ç”»åƒ", "ä¸‹ã®å†™çœŸ", "å½“åº—", "å½“ç¤¾", "ãƒ¬ãƒ“ãƒ¥ãƒ¼", "ãƒ©ãƒ³ã‚­ãƒ³ã‚°",
    "ã‚¯ãƒªãƒƒã‚¯", "ã“ã¡ã‚‰", "ç«¶åˆ", "å„ªä½æ€§", "æ¥­ç•Œæœ€é«˜", "æœ€å®‰", "No.1", "ãƒŠãƒ³ãƒãƒ¼ãƒ¯ãƒ³", "å£²ä¸ŠNo1",
    "ãƒªãƒ³ã‚¯", "ãƒšãƒ¼ã‚¸", "ã‚«ãƒ¼ãƒˆ", "è³¼å…¥ã¯ã“ã¡ã‚‰", "ã‚¯ãƒªãƒƒã‚¯ã—ã¦", "é€æ–™ç„¡æ–™", "è¿”é‡‘ä¿è¨¼"
]

RAW_MIN, RAW_MAX = 100, 130
FINAL_MIN, FINAL_MAX = 80, 110

LEADING_ENUM_RE = re.compile(r"^\s*[\dâ‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©\-\*\ãƒ»]\s*[\.ï¼Žã€]?\s*")
WHITESPACE_RE = re.compile(r"\s+")
MULTI_COMMA_RE = re.compile(r"ã€{3,}")

# ====== ç’°å¢ƒåˆæœŸåŒ– ======
def init_env():
    load_dotenv(override=True)
    api_key = os.getenv("OPENAI_API_KEY", "").strip()
    if not api_key:
        raise SystemExit("âŒ OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.envã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    model = os.getenv("OPENAI_MODEL", "gpt-5")
    temperature = float(os.getenv("OPENAI_TEMPERATURE", "1.2"))
    max_tokens = int(os.getenv("OPENAI_MAX_TOKENS", "1800"))
    client = OpenAI(api_key=api_key)
    return client, model, temperature, max_tokens

# ====== å…¥åŠ›CSVèª­è¾¼ ======
def load_products(path):
    with open(path, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        if "å•†å“å" not in reader.fieldnames:
            raise SystemExit("âŒ ã€Žå•†å“åã€åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        prods = [r["å•†å“å"].strip() for r in reader if r.get("å•†å“å")]
    uniq = list(dict.fromkeys(prods))
    return uniq

# ====== çŸ¥è¦‹ã‚µãƒžãƒª ======
def safe_load_json(path):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None

def summarize_knowledge():
    if not os.path.isdir(SEMANTICS_DIR):
        return "ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»ç”¨é€”ãƒ»å¯¾è±¡ãƒ»ã‚¹ãƒšãƒƒã‚¯ã‚’è‡ªç„¶ã«å«ã‚ã¦ã€‚", FORBIDDEN

    files = glob.glob(os.path.join(SEMANTICS_DIR, "*.json"))
    vocab = []
    forbid_local = []
    for p in files:
        data = safe_load_json(p)
        if not data:
            continue
        if isinstance(data, dict):
            if "forbidden_words" in data:
                forbid_local += data["forbidden_words"]
            if "clusters" in data:
                for c in data["clusters"]:
                    vocab += c.get("terms", [])
    kb = "çŸ¥è¦‹: " + "ã€".join(list(dict.fromkeys(vocab[:12]))) + "ã€‚"
    all_forbidden = list({*FORBIDDEN, *forbid_local})
    return kb, all_forbidden

# ====== ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ======
SYSTEM_PROMPT = (
    "ã‚ãªãŸã¯æ¥½å¤©å¸‚å ´ã®å•†å“ç”»åƒALTãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œã‚‹ãƒ—ãƒ­ã®ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚\n"
    "ç›®çš„ã¯SEOæœ€é©åŒ–ã•ã‚ŒãŸè‡ªç„¶æ–‡ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã™ã€‚\n"
    "ã€ç¦æ­¢äº‹é …ã€‘ç”»åƒã‚„å†™çœŸã®æå†™ã€ç«¶åˆæ¯”è¼ƒã€åº—èˆ—ãƒ¡ã‚¿èªžã€ç¯„å›²è¡¨è¨˜ï¼ˆä¾‹ï¼šiPhone12ã€œ16ï¼‰ã¯ç¦æ­¢ã€‚\n"
    "ã€å¿…é ˆæ§‹æˆã€‘å•†å“ã‚¹ãƒšãƒƒã‚¯â†’å¼·ã¿â†’å¯¾è±¡â†’åˆ©ç”¨ã‚·ãƒ¼ãƒ³â†’ãƒ™ãƒãƒ•ã‚£ãƒƒãƒˆã®é †ã§è‡ªç„¶ã«å«ã‚ã‚‹ã€‚\n"
    f"å„ALTã¯å…¨è§’{RAW_MIN}ã€œ{RAW_MAX}æ–‡å­—ã§ã€å¥ç‚¹ã€Œã€‚ã€ã§çµ‚ãˆã‚‹ã€‚\n"
    "20è¡Œã§ã€å„è¡Œã¯1ã€œ2æ–‡ã®è‡ªç„¶æ–‡ã€‚JSONä¸è¦ã€‚"
)

def build_user_prompt(product, kb, forbidden):
    ftxt = "ã€".join(sorted(set(forbidden)))
    return (
        f"å•†å“å: {product}\n"
        f"{kb}\n"
        "ç¦æ­¢èªž: " + ftxt + "\n"
        "å•†å“åãƒ»æ©Ÿç¨®ãƒ»ç”¨é€”ãƒ»æ©Ÿèƒ½ãƒ»ã‚¹ãƒšãƒƒã‚¯ãƒ»åˆ©ç‚¹ã‚’è‡ªç„¶ã«ç¹”ã‚Šè¾¼ã¿ãªãŒã‚‰20æ–‡æ›¸ã„ã¦ãã ã•ã„ã€‚"
    )

# ====== OpenAIå‘¼ã³å‡ºã— ======
def call_openai_lines(client, model, temp, max_tokens, prod, kb, forbid):
    user_prompt = build_user_prompt(prod, kb, forbid)
    res = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt},
        ],
        response_format={"type": "text"},
        max_completion_tokens=max_tokens,
        temperature=temp,
    )
    content = (res.choices[0].message.content or "").strip()
    lines = [LEADING_ENUM_RE.sub("", ln).strip() for ln in content.split("\n") if ln.strip()]
    return lines[:40]

# ====== æ•´å½¢ ======
def soft_clip(text):
    t = text.strip()
    if not t.endswith("ã€‚"):
        t += "ã€‚"
    t = WHITESPACE_RE.sub(" ", t)
    if len(t) > 120:
        cut = t[:120]
        p = cut.rfind("ã€‚")
        if p != -1:
            t = cut[:p+1]
        else:
            t = cut
    for ng in FORBIDDEN:
        t = t.replace(ng, "")
    return t.strip()

def refine_lines(raw):
    refined = []
    for ln in raw:
        if not ln:
            continue
        ln = soft_clip(ln)
        if len(ln) < 15:
            continue
        refined.append(ln)
    # 20è¡Œã«èª¿æ•´
    while len(refined) < 20 and refined:
        refined.append(refined[-1])
    return refined[:20]

# ====== æ›¸ãå‡ºã— ======
def write_csv(products, raws, refs):
    os.makedirs(OUT_DIR, exist_ok=True)
    with open(RAW_PATH, "w", encoding="utf-8", newline="") as f:
        w = csv.writer(f)
        w.writerow(["å•†å“å"] + [f"ALT_raw_{i+1}" for i in range(20)])
        for p, r in zip(products, raws):
            w.writerow([p] + r[:20])

    with open(REF_PATH, "w", encoding="utf-8", newline="") as f:
        w = csv.writer(f)
        w.writerow(["å•†å“å"] + [f"ALT_{i+1}" for i in range(20)])
        for p, r in zip(products, refs):
            w.writerow([p] + r[:20])

# ====== ãƒ¡ã‚¤ãƒ³ ======
def main():
    print("ðŸŒ¸ ALTé•·æ–‡ç”Ÿæˆ v4ï¼ˆgpt-5å®‰å®šï¼‹SEOè‡ªç„¶æ–‡ï¼‰")
    client, model, temp, max_tokens = init_env()
    products = load_products(INPUT_CSV)
    print(f"âœ… å¯¾è±¡å•†å“: {len(products)}ä»¶")
    kb, forbid = summarize_knowledge()

    raws, refs = [], []
    for p in tqdm(products, desc="ðŸ§  ç”Ÿæˆä¸­", total=len(products)):
        try:
            raw = call_openai_lines(client, model, temp, max_tokens, p, kb, forbid)
        except Exception as e:
            raw = [f"{p} ã®ç‰¹é•·ã‚’æ´»ã‹ã—ã€ä½¿ã„ã‚„ã™ã•ã‚’é«˜ã‚ãŸè¨­è¨ˆã§ã™ã€‚"] * 20
        ref = refine_lines(raw)
        raws.append(raw)
        refs.append(ref)
        time.sleep(0.3)

    write_csv(products, raws, refs)
    print("âœ… å‡ºåŠ›å®Œäº†:")
    print("   - RAW:", RAW_PATH)
    print("   - REF:", REF_PATH)
    print("ðŸ“ gpt-5æœ€é©åŒ–å®Œäº†ï¼ˆå¥ç‚¹è£œå®Œãƒ»SEOå¼·åŒ–ãƒ»è‡ªç„¶æ–‡æ§‹æˆï¼‰")

if __name__ == "__main__":
    main()
import atlas_autosave_core
