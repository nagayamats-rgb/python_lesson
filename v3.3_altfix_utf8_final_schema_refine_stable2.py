# -*- coding: utf-8 -*-
"""
v3.3_altfix_utf8_final_schema_refine_stable2.py
ALTï¼ˆæ¥½å¤©å…±é€šALT20ï¼‰ã‚’â€œé•·æ–‡â†’ãƒ­ãƒ¼ã‚«ãƒ«æ•´å½¢â€ã§å®‰å®šç”Ÿæˆã™ã‚‹å°‚ç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚
- å…¥åŠ›: /Users/tsuyoshi/Desktop/python_lesson/rakuten.csvï¼ˆUTF-8, å…ˆé ­è¡Œãƒ˜ãƒƒãƒ€,ã€Œå•†å“åã€åˆ—ï¼‰
- å‡ºåŠ›: /Users/tsuyoshi/Desktop/python_lesson/output/ai_writer/alt_text_refined_final_stable.csv
- OpenAI: .env ã‚’è‡ªå‹•èª­è¾¼ï¼ˆOPENAI_API_KEY / OPENAI_MODEL ä»»æ„ï¼‰
- å¿œç­”å½¢å¼: response_format="text"ï¼ˆJSONã¯ä½¿ã‚ãªã„ï¼‰
- ç”Ÿæˆ: 100ã€œ130å­—ã‚’AIã«æ›¸ã‹ã›ã€ãƒ­ãƒ¼ã‚«ãƒ«ã§80ã€œ110å­—ã«è‡ªç„¶æ•´å½¢ï¼ˆå¥ç‚¹ã ã‘ç¦æ­¢ï¼‰
- 20ä»¶/å•†å“ã‚’ä¿è¨¼ï¼ˆå°‘ãªã„å ´åˆã¯ãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ›ã§è£œå®Œï¼‰
"""

import os
import re
import csv
import json
import time
import glob
from typing import List, Tuple, Dict, Any

# ========= 0) .env ãƒ­ãƒ¼ãƒ€ï¼ˆä¾å­˜ãªã—ï¼‰ =========
def load_env_from_local_env():
    """
    ã‚·ã‚§ãƒ«ã®æ–°ã—ã„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§ OPENAI_API_KEY ãŒæ¶ˆãˆã‚‹å•é¡Œã«å‚™ãˆã€
    ã‚«ãƒ¬ãƒ³ãƒˆé…ä¸‹ or ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç›´ä¸‹ã® .env ã‚’æ‰‹å‹•ã§èª­ã‚€ã€‚
    """
    candidates = [
        ".env",
        "/Users/tsuyoshi/Desktop/python_lesson/.env",
    ]
    for p in candidates:
        try:
            if os.path.isfile(p):
                with open(p, "r", encoding="utf-8") as f:
                    for line in f:
                        if "=" in line and not line.strip().startswith("#"):
                            k, v = line.strip().split("=", 1)
                            k, v = k.strip(), v.strip().strip('"').strip("'")
                            if k and v and k not in os.environ:
                                os.environ[k] = v
        except Exception:
            pass

load_env_from_local_env()

# ========= 1) OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ =========
try:
    from openai import OpenAI
except Exception as e:
    raise SystemExit("openai ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚`pip install openai` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚") from e

if not os.environ.get("OPENAI_API_KEY"):
    raise SystemExit("OPENAI_API_KEY ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚.env ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

client = OpenAI()  # APIã‚­ãƒ¼ã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰

# ========= 2) å…¥å‡ºåŠ›ãƒ‘ã‚¹ =========
INPUT_RAKUTEN = "/Users/tsuyoshi/Desktop/python_lesson/rakuten.csv"  # UTF-8
OUT_DIR = "/Users/tsuyoshi/Desktop/python_lesson/output/ai_writer"
os.makedirs(OUT_DIR, exist_ok=True)
OUTPUT_CSV = os.path.join(OUT_DIR, "alt_text_refined_final_stable.csv")

# ========= 3) ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒ‘ãƒ©ãƒ¡ã‚¿ =========
MODEL = os.environ.get("OPENAI_MODEL", "gpt-4o-mini")
# â€» ä¸€éƒ¨ãƒ¢ãƒ‡ãƒ«ã¯éå¯¾å¿œãƒ‘ãƒ©ãƒ¡ã‚¿ãŒã‚ã‚‹ãŸã‚ã€æ¸©åº¦ãªã©ã¯â€œæœªæŒ‡å®šâ€ã§å‘¼ã¶ï¼ˆå®‰å…¨ï¼‰
MAX_COMPLETION_TOKENS = 1000  # é•·æ–‡è¨±å®¹
RETRY = 3
RETRY_WAIT = 3

# ========= 4) ãƒ­ãƒ¼ã‚«ãƒ«çŸ¥è¦‹ã®èª­ã¿è¾¼ã¿ï¼ˆä»»æ„ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°å¸ã„ä¸Šã’ï¼‰ =========
SEMANTICS_DIR = "/Users/tsuyoshi/Desktop/python_lesson/output/semantics"

def safe_load_json(path: str) -> Any:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None

def collect_local_knowledge() -> Tuple[str, List[str], Dict[str, str]]:
    """
    å¯èƒ½ãªé™ã‚Š /output/semantics é…ä¸‹ã‹ã‚‰çŸ¥è¦‹ã‚’è¦ç´„ã€‚
    - forbidden_words: ç¦æ­¢èªé›†ç´„
    - synonyms_map: ç½®æ›ã—ã¦å·®åˆ†ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½œã‚‹ç°¡æ˜“èªå½™
    - knowledge_text: AIãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸ‹ã‚è¾¼ã‚€ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆ
    """
    forbidden: List[str] = []
    synonyms_map: Dict[str, str] = {}

    if os.path.isdir(SEMANTICS_DIR):
        for fp in glob.glob(os.path.join(SEMANTICS_DIR, "*.json")):
            data = safe_load_json(fp)
            if not data:
                continue
            # ä»£è¡¨çš„ãªæ§‹é€ ã‚’æƒ³å®šã—ã¦æŠ½å‡º
            if isinstance(data, dict):
                if "forbidden_words" in data and isinstance(data["forbidden_words"], list):
                    forbidden.extend([str(x) for x in data["forbidden_words"] if isinstance(x, (str, int))])
                if "synonyms" in data and isinstance(data["synonyms"], dict):
                    for k, v in data["synonyms"].items():
                        if isinstance(k, str) and isinstance(v, str):
                            synonyms_map[k] = v
            # ãƒªã‚¹ãƒˆå½¢å¼ã‚‚è¨±å®¹ï¼ˆå„è¦ç´ ãŒ dict ã®æƒ³å®šï¼‰
            if isinstance(data, list):
                for row in data:
                    if isinstance(row, dict):
                        if "forbidden_words" in row and isinstance(row["forbidden_words"], list):
                            forbidden.extend([str(x) for x in row["forbidden_words"] if isinstance(x, (str, int))])

    # ç”»åƒæå†™NGãƒ»ãƒ¡ã‚¿è¡¨ç¾NGãªã©æœ€ä½é™
    base_forbidden = [
        "ç”»åƒ", "å†™çœŸ", "ã‚¤ãƒ¡ãƒ¼ã‚¸", "è¦‹ãŸç›®", "ã“ã¡ã‚‰", "å½“åº—", "ç«¶åˆ", "ç«¶åˆå„ªä½æ€§",
        "å£²ä¸ŠNo.1", "No.1", "ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä¸Šä½", "ã‚¯ãƒªãƒƒã‚¯", "ãƒªãƒ³ã‚¯"
    ]
    forbidden.extend(base_forbidden)
    # æ­£è¦åŒ–ãƒ»ä¸€æ„åŒ–
    forb = sorted(set([str(x).strip() for x in forbidden if str(x).strip()]))

    # çŸ¥è¦‹ãƒ†ã‚­ã‚¹ãƒˆï¼ˆç°¡æ½”ã«ï¼‰
    knowledge_text = (
        "ãƒ»ç”»åƒæå†™ã¯ç¦æ­¢ã€‚å•†å“ã‚¹ãƒšãƒƒã‚¯ï¼ã‚³ã‚¢ã‚³ãƒ³ãƒ”ã‚¿ãƒ³ã‚¹ï¼æƒ³å®šãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼ä½¿ç”¨ã‚·ãƒ¼ãƒ³ï¼ãƒ™ãƒãƒ•ã‚£ãƒƒãƒˆã‚’ç„¡ç†ãªãå«ã‚ã‚‹ã€‚"
        "ãƒ»ç«¶åˆæ¯”è¼ƒã‚„â€œç«¶åˆå„ªä½æ€§â€ãªã©ã®ãƒ¡ã‚¿è¡¨ç¾ã¯ç¦æ­¢ã€‚"
        "ãƒ»å¥ç‚¹ã‚„èª­ç‚¹ã‚’æ­£ã—ãç”¨ã„ã€è‡ªç„¶ãªæ–‡ã§100ã€œ130æ–‡å­—ã‚’ç›®å®‰ã«ã€‚"
    )

    return knowledge_text, forb, synonyms_map

KNOWLEDGE_TEXT, FORBIDDEN_WORDS, SYN_MAP = collect_local_knowledge()

# ========= 5) ãƒ†ã‚­ã‚¹ãƒˆæ•´å½¢ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =========
JP_SPACES = re.compile(r"[ \t\u3000]+")
MULTI_PUNCTS = re.compile(r"[ã€‚\.]{2,}")
TRAILING_QUOTES = re.compile(r"[\"'â€™â€ï¼‰)\]]+$")
LEADING_QUOTES = re.compile(r"^[\"'â€˜â€œï¼ˆ(\[]+")

def normalize_text(s: str) -> str:
    s = s.replace("\r", "").replace("\n", " ").strip()
    s = JP_SPACES.sub(" ", s)
    s = LEADING_QUOTES.sub("", s)
    s = TRAILING_QUOTES.sub("", s)
    s = s.replace("..", "ã€‚")
    s = MULTI_PUNCTS.sub("ã€‚", s)
    return s.strip()

def is_punct_only(s: str) -> bool:
    if not s:
        return True
    t = s.strip()
    return all(ch in "ã€‚ã€.ï¼Œ,ï¼!ï¼Ÿ?ãƒ»" for ch in t)

def ends_with_terminal(s: str) -> bool:
    return s.endswith(("ã€‚","ï¼","ï¼Ÿ","!","?"))

def finalize_sentence(s: str) -> str:
    s = normalize_text(s)
    if not s or is_punct_only(s):
        return ""  # å¥ç‚¹ã ã‘ã¯ç¦æ­¢ï¼šç„¡åŠ¹åŒ–
    if not ends_with_terminal(s):
        s = s + "ã€‚"
    return s

def remove_forbidden(s: str, forbidden: List[str]) -> str:
    out = s
    for w in forbidden:
        if w and w in out:
            out = out.replace(w, "")  # å˜ç´”é™¤å»ï¼ˆæ„å›³ã›ã¬èªå°¾æ¬ æã¯å¾Œæ®µã§æ•´ãˆã‚‹ï¼‰
    return normalize_text(out)

def natural_trim(s: str, min_len=80, target_max=110, hard_max=130) -> str:
    """
    - ã¾ãš hard_maxï¼ˆ130ï¼‰ã§ç²—ãæŠ‘ãˆã‚‹
    - ã§ãã‚‹é™ã‚Šå¥ç‚¹ï¼ˆã€‚ï¼‰ã€èª­ç‚¹ï¼ˆã€ï¼‰ã€ã‚¹ãƒšãƒ¼ã‚¹ã§è‡ªç„¶ã‚«ãƒƒãƒˆ
    - æœ€å¾Œã« 80ã€œ110 ã«åã‚ã‚‹åŠªåŠ›ã‚’ã™ã‚‹
    """
    s = normalize_text(s)
    if len(s) > hard_max:
        s = s[:hard_max]
    # æœ«å°¾ã‚’è‡ªç„¶ã«è½ã¨ã™
    cut_points = [i for i, ch in enumerate(s) if ch in ("ã€‚", "ã€", " ")]
    if cut_points:
        # target_max ã‚’è¶…ãˆã‚‹å ´åˆã¯ã€target_maxä»¥ä¸‹ã§ä¸€ç•ªå¾Œã‚ã®åŒºåˆ‡ã‚Šã§åˆ‡ã‚‹
        if len(s) > target_max:
            candidates = [i for i in cut_points if i <= target_max]
            if candidates:
                s = s[:candidates[-1]+1]
    s = s.strip()
    s = finalize_sentence(s)
    # ã¾ã é•·ã„ãªã‚‰ã‚‚ã†ä¸€æ®µè½ã¨ã™
    if len(s) > target_max:
        # æœ€å¾Œã® "ã€‚" ã§åˆ‡ã‚‹
        last = s.rfind("ã€‚")
        if last != -1 and last+1 >= min_len:
            s = s[:last+1]
        elif len(s) > target_max:
            s = s[:target_max].rstrip("ã€ï¼Œ,")
            s = finalize_sentence(s)
    return s

def clean_sentences(lines: List[str], forbidden: List[str]) -> List[str]:
    out = []
    seen = set()
    for raw in lines:
        s = normalize_text(raw)
        if not s or is_punct_only(s):
            continue
        s = remove_forbidden(s, forbidden)
        s = finalize_sentence(s)
        if not s:
            continue
        if s not in seen:
            seen.add(s)
            out.append(s)
    return out

def diversify(sent: str, syn_map: Dict[str,str]) -> str:
    """
    åŒä¸€æ–‡ã®è»½å¾®å·®åˆ†ï¼ˆALTä¸è¶³æ™‚ã®è£œå®Œç”¨ï¼‰
    """
    s = sent
    for k, v in syn_map.items():
        if k in s:
            s = s.replace(k, v)
            break
    if s == sent:
        # ç½®æ›ãŒãªã‘ã‚Œã°è»½ã„è¨€ã„å›ã—å¤‰æ›´
        s = s.replace("ä¾¿åˆ©", "ä½¿ã„ã‚„ã™ã„").replace("æœ€é©", "ã¡ã‚‡ã†ã©è‰¯ã„")
    return finalize_sentence(s)

# ========= 6) OpenAI å‘¼ã³å‡ºã—ï¼ˆtext å¿œç­”ï¼‰ =========
def call_openai_text(product_name: str, knowledge: str) -> str:
    """
    å¯èƒ½ãªé™ã‚Šâ€œãƒ†ã‚­ã‚¹ãƒˆé•·æ–‡â€ã§20ä»¶ä»¥ä¸Šã®å€™è£œã‚’1ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã§å–å¾—ã€‚
    è§£æã¯ãƒ­ãƒ¼ã‚«ãƒ«ã§å®‰å®šåŒ–ã•ã›ã‚‹ã€‚
    """
    sys = (
        "ã‚ãªãŸã¯æ—¥æœ¬èªã®ãƒ—ãƒ­ã®ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚"
        "æ¥½å¤©å‘ã‘ALTãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚"
        "ç”»åƒæå†™ã‚„ãƒ¡ã‚¿è¡¨ç¾ã¯ç¦æ­¢ã€‚1æ–‡ï¼è‡ªç„¶ãªæ—¥æœ¬èªã§å‡ºåŠ›ã€‚"
        "è¦‹å‡ºã—ã‚„ç•ªå·ã¯ä¸è¦ã€‚è¡Œã”ã¨ã«1å€™è£œã€‚"
    )
    usr = (
        f"å•†å“åï¼š{product_name}\n"
        f"{knowledge}\n"
        "å‡ºåŠ›è¦ä»¶ï¼š\n"
        "ãƒ»1è¡Œã«ã¤ã1æ–‡ã®ALTå€™è£œã€‚\n"
        "ãƒ»å„æ–‡ã¯ãŠãŠã‚ˆã100ã€œ130å­—ã§ã€æ–‡æœ«ã¯å¥ç‚¹ã§è‡ªç„¶ã«çµ‚ãˆã‚‹ã€‚\n"
        "ãƒ»20ä»¶ä»¥ä¸Šï¼ˆ25ä»¶ç¨‹åº¦ï¼‰æ›¸ãå‡ºã™ã€‚\n"
        "ãƒ»ç”»åƒãƒ»å†™çœŸã®è¨˜è¿°ã¯å…¥ã‚Œãªã„ã€‚\n"
        "ãƒ»ç«¶åˆæ¯”è¼ƒã‚„â€œç«¶åˆå„ªä½æ€§â€ã®èªã¯ä½¿ã‚ãªã„ã€‚\n"
        "ãƒ»æ”¹è¡Œã§åŒºåˆ‡ã£ã¦åˆ—æŒ™ã€‚"
    )

    last_err = None
    for _ in range(RETRY):
        try:
            res = client.chat.completions.create(
                model=MODEL,
                messages=[
                    {"role": "system", "content": sys},
                    {"role": "user", "content": usr},
                ],
                response_format="text",          # â† JSONã¯ä½¿ã‚ãªã„
                max_completion_tokens=MAX_COMPLETION_TOKENS,
                # temperature ã¯ä¸€éƒ¨ãƒ¢ãƒ‡ãƒ«éå¯¾å¿œãŒã‚ã‚Šå¾—ã‚‹ã®ã§æœªæŒ‡å®šï¼ˆå†ç¾æ€§å„ªå…ˆãªã‚‰ 0.3 æŒ‡å®šå¯ï¼‰
                stream=False,
            )
            txt = res.choices[0].message.content if res.choices else ""
            if txt and txt.strip():
                return txt
        except Exception as e:
            last_err = e
            time.sleep(RETRY_WAIT)
    # ã™ã¹ã¦å¤±æ•—æ™‚
    raise RuntimeError(f"OpenAIå‘¼ã³å‡ºã—ã«å¤±æ•—ã—ã¾ã—ãŸ: {last_err}")

def parse_lines_from_text(block: str) -> List[str]:
    """
    ç®‡æ¡æ›¸ãã§ã‚‚ãƒ—ãƒ¬ãƒ¼ãƒ³ã§ã‚‚å—ã‘å–ã‚Œã‚‹ã‚ˆã†ã«ã€è¡Œå˜ä½ã«å‰¥ãŒã™ã€‚
    """
    if not block:
        return []
    # ãƒã‚¤ãƒ•ãƒ³/ç•ªå·ç®‡æ¡æ›¸ãã‚’æƒ³å®šã—ã¦åˆ†è§£
    raw_lines = re.split(r"[\r\n]+", block)
    out = []
    for ln in raw_lines:
        ln = ln.strip()
        if not ln:
            continue
        # ç®‡æ¡æ›¸ãè¨˜å·ã‚’å‰¥ã
        ln = re.sub(r"^[-ãƒ»*â—â—‹\d\.\)ï¼‰]+\s*", "", ln)
        out.append(ln)
    return out

# ========= 7) ALTç”Ÿæˆ + ãƒªãƒ•ã‚¡ã‚¤ãƒ³ =========
def ai_generate_alt(product_name: str) -> List[str]:
    """
    1) AIã§25æ–‡ç¨‹åº¦å–å¾—ï¼ˆ100ã€œ130å­—æƒ³å®šï¼‰
    2) ãƒ­ãƒ¼ã‚«ãƒ«ã§ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚° â†’ 80ã€œ110å­—ã¸è‡ªç„¶æ•´å½¢
    3) 20ä»¶ã«æ•´ãˆã‚‹ï¼ˆä¸è¶³ã¯è»½å¾®å·®åˆ†ã§è£œå®Œï¼‰
    """
    raw_text = call_openai_text(product_name, KNOWLEDGE_TEXT)
    lines = parse_lines_from_text(raw_text)

    # 1st pass: ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
    cleaned = clean_sentences(lines, FORBIDDEN_WORDS)

    # 2nd pass: é•·ã•æ•´å½¢ï¼ˆè‡ªç„¶ãƒˆãƒªãƒ ï¼‰
    shaped = [natural_trim(s, min_len=80, target_max=110, hard_max=130) for s in cleaned]
    shaped = clean_sentences(shaped, FORBIDDEN_WORDS)  # ã‚‚ã†ä¸€åº¦æ•´åˆ—ï¼ˆç©ºã‚’é™¤å¤–ï¼‰

    # 20ä»¶ã«æº€ãŸãªã„å ´åˆã¯è£œå®Œ
    while len(shaped) < 20 and shaped:
        shaped.append(diversify(shaped[len(shaped) % max(1, len(shaped)) - 1], SYN_MAP))

    # ã¾ã è¶³ã‚Šãªã„ï¼ˆæ¥µç«¯ã«ç©ºï¼‰ã®å ´åˆã®ä¿é™º
    if not shaped:
        shaped = [finalize_sentence(f"{product_name} ã®ç‰¹é•·ã‚’æ´»ã‹ã—ã€æ—¥å¸¸ã®ä¸ä¾¿ã‚’æ¸›ã‚‰ã™å®Ÿç”¨çš„ãªè¨­è¨ˆã€‚æŒã¡é‹ã³ã‚„ã™ã•ã¨è€ä¹…æ€§ã«é…æ…®ã—ã€æ¯æ—¥ã®ã‚·ãƒ¼ãƒ³ã§å®‰å¿ƒã—ã¦ä½¿ãˆã‚‹ã€‚")] * 20

    # 20ä»¶ã«æƒãˆã‚‹ï¼è¶…éã¯ä¸Šä½20ä»¶ã¸
    shaped = shaped[:20]
    return shaped

# ========= 8) å…¥åŠ›CSV èª­è¾¼ï¼ˆUTF-8ï¼‰ =========
def read_products_from_csv(path: str) -> List[str]:
    if not os.path.isfile(path):
        raise FileNotFoundError(f"å…¥åŠ›CSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path}")
    names = []
    with open(path, "r", encoding="utf-8", newline="") as f:
        reader = csv.DictReader(f)
        if "å•†å“å" not in (reader.fieldnames or []):
            raise ValueError("CSVã«ã€å•†å“åã€åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        for r in reader:
            nm = (r.get("å•†å“å") or "").strip()
            if nm:
                names.append(nm)
    # ä¸€æ„åŒ–
    uniq = []
    seen = set()
    for nm in names:
        if nm not in seen:
            seen.add(nm)
            uniq.append(nm)
    return uniq

# ========= 9) CSVå‡ºåŠ› =========
def write_alt_csv(path: str, items: List[Tuple[str, List[str]]]):
    fieldnames = ["å•†å“å"] + [f"ALT{i}" for i in range(1, 21)]
    with open(path, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for name, alts in items:
            row = {"å•†å“å": name}
            for i in range(20):
                row[f"ALT{i+1}"] = alts[i] if i < len(alts) else ""
            writer.writerow(row)

# ========= 10) é€²æ—ï¼ˆtqdmä»»æ„ï¼‰ =========
def progress_iter(seq, desc=""):
    try:
        from tqdm import tqdm
        return tqdm(seq, desc=desc)
    except Exception:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        total = len(seq)
        for i, x in enumerate(seq, 1):
            if i == 1 or i == total or i % max(1, total // 10) == 0:
                print(f"ğŸ§  ALTç”Ÿæˆä¸­: {i}/{total} ({int(i/total*100)}%)")
            yield x

# ========= 11) main =========
def main():
    print("ğŸŒ¸ v3.3_altfix_utf8_final_schema_refine_stable2 å®Ÿè¡Œé–‹å§‹ï¼ˆALTé•·æ–‡â†’ãƒ­ãƒ¼ã‚«ãƒ«æ•´å½¢ãƒ»ç¦å‰‡é©ç”¨ï¼‰")
    products = read_products_from_csv(INPUT_RAKUTEN)
    print(f"âœ… å•†å“åæŠ½å‡º: {len(products)}ä»¶ï¼ˆé‡è¤‡é™¤å»æ¸ˆï¼‰")

    results: List[Tuple[str, List[str]]] = []
    for name in progress_iter(products, desc="ğŸ§  ALTç”Ÿæˆä¸­"):
        try:
            alts = ai_generate_alt(name)
        except Exception as e:
            # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ï¼šæœ€ä½é™1ä»¶ã‹ã‚‰è¤‡è£½
            fallback = finalize_sentence(f"{name} ã¯ã€æ—¥å¸¸ã®ä¸ä¾¿ã‚’æ¸›ã‚‰ã™å®Ÿç”¨çš„ãªè¨­è¨ˆã€‚æºå¸¯æ€§ã¨è€ä¹…æ€§ã«é…æ…®ã—ã€å¹…åºƒã„ã‚·ãƒ¼ãƒ³ã§å®‰å¿ƒã—ã¦ä½¿ãˆã‚‹ã€‚")
            alts = [fallback] * 20
            print(f"âš ï¸ ç”Ÿæˆã‚¨ãƒ©ãƒ¼ã‚’å›é¿ã—ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ã‚’é©ç”¨: {e}")
        # æ–‡å­—æ•°çµ±è¨ˆ
        lens = [len(x) for x in alts if x]
        avg_len = sum(lens)/len(lens) if lens else 0
        print(f"   â”œ avg_len: {avg_len:.1f}å­— / æœ‰åŠ¹: {len(lens)}ä»¶")
        results.append((name, alts))

    write_alt_csv(OUTPUT_CSV, results)
    print(f"âœ… å‡ºåŠ›å®Œäº†: {OUTPUT_CSV}")
    print("âœ… ä»•æ§˜: ALTã¯AIã§100ã€œ130å­—â†’ãƒ­ãƒ¼ã‚«ãƒ«ã§80ã€œ110å­—ã«æ•´å½¢ã€‚å¥ç‚¹ã ã‘è¡Œã¯é™¤å¤–ãƒ»è£œå®Œæ¸ˆã€‚ç”»åƒæå†™èªãƒ»ãƒ¡ã‚¿èªã¯ç¦æ­¢ã€‚")

if __name__ == "__main__":
    main()
import atlas_autosave_core
