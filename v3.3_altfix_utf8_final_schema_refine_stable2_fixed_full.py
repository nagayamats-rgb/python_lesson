# -*- coding: utf-8 -*-
"""
v3.3_altfix_utf8_final_schema_refine_stable2_fixed_full.py
------------------------------------------------------------
ALTé•·æ–‡ç”Ÿæˆ â†’ ãƒ­ãƒ¼ã‚«ãƒ«æ•´å½¢ â†’ ç¦å‰‡é©ç”¨ ã®å®‰å®šç‰ˆ
OpenAIå‘¼ã³å‡ºã—æ§‹æ–‡ã‚’ç¾è¡Œä»•æ§˜(gpt-4o)ã«æº–æ‹ ã€‚
ãƒ­ã‚¸ãƒƒã‚¯é †åºãƒ»é–¢æ•°åãƒ»å‡ºåŠ›ä»•æ§˜ã¯396è¡Œç‰ˆã‚’å®Œå…¨ç¶­æŒã€‚
"""

import os
import re
import csv
import json
import glob
import time
from typing import List, Tuple, Dict, Any

# ========= 0. .env ãƒ­ãƒ¼ãƒ‰ =========
def load_env_file():
    candidates = [
        ".env",
        "/Users/tsuyoshi/Desktop/python_lesson/.env"
    ]
    for path in candidates:
        if not os.path.exists(path):
            continue
        with open(path, encoding="utf-8") as f:
            for line in f:
                if "=" in line and not line.strip().startswith("#"):
                    k, v = line.strip().split("=", 1)
                    os.environ[k.strip()] = v.strip().strip('"').strip("'")
load_env_file()

# ========= 1. OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ– =========
try:
    from openai import OpenAI
except Exception as e:
    raise SystemExit("âŒ openai ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚`pip install openai` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

if not os.environ.get("OPENAI_API_KEY"):
    raise SystemExit("âŒ OPENAI_API_KEY ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚.env ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])
MODEL = os.environ.get("OPENAI_MODEL", "gpt-4o")

# ========= 2. åŸºæœ¬è¨­å®š =========
INPUT_RAKUTEN = "/Users/tsuyoshi/Desktop/python_lesson/rakuten.csv"
OUT_DIR = "/Users/tsuyoshi/Desktop/python_lesson/output/ai_writer"
os.makedirs(OUT_DIR, exist_ok=True)
OUTPUT_CSV = os.path.join(OUT_DIR, "alt_text_refined_final_stable2_fixed.csv")

MAX_COMPLETION_TOKENS = 1000
RETRY = 3
RETRY_WAIT = 3

SEMANTICS_DIR = "/Users/tsuyoshi/Desktop/python_lesson/output/semantics"

# ========= 3. ãƒ­ãƒ¼ã‚«ãƒ«JSONçŸ¥è¦‹ãƒ­ãƒ¼ãƒ‰ =========
def safe_load_json(path: str):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None

def collect_local_knowledge() -> Tuple[str, List[str]]:
    forbidden = []
    if os.path.isdir(SEMANTICS_DIR):
        for fp in glob.glob(os.path.join(SEMANTICS_DIR, "*.json")):
            data = safe_load_json(fp)
            if isinstance(data, dict) and "forbidden_words" in data:
                forbidden.extend(data["forbidden_words"])
            elif isinstance(data, list):
                for d in data:
                    if isinstance(d, dict) and "forbidden_words" in d:
                        forbidden.extend(d["forbidden_words"])
    base_forbidden = [
        "ç”»åƒ", "å†™çœŸ", "ã‚¤ãƒ¡ãƒ¼ã‚¸", "ã“ã¡ã‚‰", "å½“åº—",
        "ç«¶åˆ", "ç«¶åˆå„ªä½æ€§", "å£²ä¸ŠNo.1", "No.1", "ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä¸Šä½",
        "ãƒªãƒ³ã‚¯", "ã‚¯ãƒªãƒƒã‚¯", "ãƒšãƒ¼ã‚¸"
    ]
    forbidden.extend(base_forbidden)
    forbidden = sorted(set([x.strip() for x in forbidden if x.strip()]))

    knowledge_text = (
        "ãƒ»ç”»åƒæå†™ã¯ç¦æ­¢ã€‚\n"
        "ãƒ»å•†å“ã‚¹ãƒšãƒƒã‚¯ï¼ã‚³ã‚¢ã‚³ãƒ³ãƒ”ã‚¿ãƒ³ã‚¹ï¼æƒ³å®šãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼ä½¿ç”¨ã‚·ãƒ¼ãƒ³ï¼ãƒ™ãƒãƒ•ã‚£ãƒƒãƒˆã‚’è‡ªç„¶ã«å«ã‚ã‚‹ã€‚\n"
        "ãƒ»ç«¶åˆæ¯”è¼ƒã‚„â€œç«¶åˆå„ªä½æ€§â€ãªã©ã®ãƒ¡ã‚¿è¡¨ç¾ã¯ç¦æ­¢ã€‚\n"
        "ãƒ»å¥èª­ç‚¹ã‚„åŠ©è©ã‚’æ­£ã—ãä½¿ã„ã€è‡ªç„¶ãªæ—¥æœ¬èªã§100ã€œ130æ–‡å­—ç¨‹åº¦ã®æ–‡ã‚’ç”Ÿæˆã€‚\n"
        "ãƒ»æ–‡æœ«ã¯å¥ç‚¹ã§è‡ªç„¶ã«çµ‚ãˆã‚‹ã€‚"
    )
    return knowledge_text, forbidden

KNOWLEDGE_TEXT, FORBIDDEN_WORDS = collect_local_knowledge()

# ========= 4. ãƒ†ã‚­ã‚¹ãƒˆæ•´å½¢é–¢é€£ =========
def normalize_text(s: str) -> str:
    s = s.replace("\r", "").replace("\n", " ").strip()
    s = re.sub(r"[ \t\u3000]+", " ", s)
    s = re.sub(r"[\"'â€˜â€œâ€ï¼ˆ()ï¼‰\[\]]", "", s)
    s = re.sub(r"[ã€‚\.]{2,}", "ã€‚", s)
    return s.strip()

def finalize_sentence(s: str) -> str:
    s = normalize_text(s)
    if not s:
        return ""
    if not s.endswith(("ã€‚", "ï¼", "ï¼Ÿ", "!", "?")):
        s += "ã€‚"
    return s

def natural_trim(s: str, min_len=80, target_max=110, hard_max=130):
    s = normalize_text(s)
    if len(s) > hard_max:
        s = s[:hard_max]
    if len(s) > target_max:
        last = s.rfind("ã€‚", 0, target_max)
        if last > min_len:
            s = s[:last + 1]
    return finalize_sentence(s)

def apply_forbidden_filters(s: str, forbidden: List[str]) -> str:
    text = s
    for word in forbidden:
        text = re.sub(word, "", text)
    return normalize_text(text)

# ========= 5. OpenAIå‘¼ã³å‡ºã—ï¼ˆå®‰å…¨ä¿®æ­£ç‰ˆï¼‰ =========
def call_openai_text(product_name: str) -> str:
    system_prompt = (
        "ã‚ãªãŸã¯æ—¥æœ¬èªã®ãƒ—ãƒ­ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚"
        "æ¥½å¤©ã®å•†å“ç”»åƒALTãƒ†ã‚­ã‚¹ãƒˆã‚’è‡ªç„¶ãªæ—¥æœ¬èªã§ä½œæˆã—ã¦ãã ã•ã„ã€‚"
        "å¥èª­ç‚¹ã‚’æ­£ã—ãä½¿ã„ã€æ–‡æ³•çš„ã«æ­£ã—ã„æ—¥æœ¬èªã‚’ç”Ÿæˆã€‚"
        "ç”»åƒã‚„å†™çœŸã®èª¬æ˜ã¯ç¦æ­¢ã€‚"
    )

    user_prompt = (
        f"å•†å“å: {product_name}\n\n"
        f"{KNOWLEDGE_TEXT}\n\n"
        "å‡ºåŠ›æ¡ä»¶:\n"
        "ãƒ»æ”¹è¡Œã§åŒºåˆ‡ã£ãŸ25æ–‡ã‚’ç”Ÿæˆã€‚\n"
        "ãƒ»å„æ–‡ã¯100ã€œ130æ–‡å­—ç¨‹åº¦ã€‚\n"
        "ãƒ»å¥ç‚¹ã§è‡ªç„¶ã«çµ‚ãˆã‚‹ã€‚\n"
        "ãƒ»ç«¶åˆå„ªä½æ€§ã‚„ãƒ¡ã‚¿è¡¨ç¾ã¯ç¦æ­¢ã€‚\n"
        "ãƒ»ç”»åƒãƒ»å†™çœŸãƒ»è¦‹ãŸç›®ãªã©ã®èªå¥ã¯å«ã‚ãªã„ã€‚\n"
        "ãƒ»è‡ªç„¶ã§SEOã«å¼·ã„æ—¥æœ¬èªã‚’ä½¿ç”¨ã€‚"
    )

    last_err = None
    for attempt in range(RETRY):
        try:
            res = client.chat.completions.create(
                model=MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=0.4,
                max_completion_tokens=MAX_COMPLETION_TOKENS,
            )
            content = res.choices[0].message.content
            if content and content.strip():
                return content
        except Exception as e:
            last_err = e
            print(f"âš ï¸ OpenAIå‘¼ã³å‡ºã—å¤±æ•—({attempt+1}/{RETRY}): {e}")
            time.sleep(RETRY_WAIT)
    raise RuntimeError(f"OpenAIå‘¼ã³å‡ºã—å¤±æ•—: {last_err}")
# ========= 6. ALTç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯ =========
def ai_generate_alt(product_name: str) -> List[str]:
    """
    ALTãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã€‚
    100ã€œ130å­—ã®æ–‡ã‚’AIã«ç”Ÿæˆã•ã›ã€ãƒ­ãƒ¼ã‚«ãƒ«ã§æ•´å½¢ã€‚
    ç¦å‰‡èªãƒ»å¥èª­ç‚¹è£œæ­£ãƒ»æ–‡å­—æ•°ãƒˆãƒªãƒ ã‚’é©ç”¨ã€‚
    """
    try:
        raw = call_openai_text(product_name)
    except Exception as e:
        print(f"âš ï¸ {product_name[:25]}... ã§AIå‘¼ã³å‡ºã—å¤±æ•— â†’ fallbacké©ç”¨ ({e})")
        fallback = finalize_sentence(f"{product_name} ã®é­…åŠ›ã‚’å¼•ãç«‹ã¦ã€æ¯æ—¥ã®ç”Ÿæ´»ã‚’å¿«é©ã«ã™ã‚‹è¨­è¨ˆã€‚æ©Ÿèƒ½æ€§ã¨ãƒ‡ã‚¶ã‚¤ãƒ³æ€§ã‚’å…¼ã­å‚™ãˆã¦ã„ã¾ã™ã€‚")
        return [fallback] * 20

    # æ”¹è¡Œå˜ä½ã§æ•´å½¢
    lines = [ln.strip("-ãƒ»â—* \t") for ln in raw.splitlines() if ln.strip()]
    alts = []
    for ln in lines:
        ln = apply_forbidden_filters(ln, FORBIDDEN_WORDS)
        ln = natural_trim(ln)
        if 80 <= len(ln) <= 130 and "ã€‚" in ln:
            alts.append(finalize_sentence(ln))
    # ä¸è¶³æ™‚ã¯fallback
    if not alts:
        fallback = finalize_sentence(f"{product_name} ã®ç‰¹é•·ã‚’æ´»ã‹ã—ã€ä½¿ã„ã‚„ã™ã•ã¨å¿«é©ã•ã‚’ä¸¡ç«‹ã—ãŸé«˜å“è³ªè¨­è¨ˆã€‚")
        alts = [fallback] * 20

    # ALTã‚’20ä»¶ã«èª¿æ•´
    if len(alts) < 20:
        alts += [alts[-1]] * (20 - len(alts))
    return alts[:20]


# ========= 7. CSV I/O =========
def read_products(path: str) -> List[str]:
    names = []
    with open(path, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            nm = (row.get("å•†å“å") or "").strip()
            if nm:
                names.append(nm)
    uniq = list(dict.fromkeys(names))
    return uniq


def write_alt_csv(path: str, data: List[Tuple[str, List[str]]]):
    fields = ["å•†å“å"] + [f"ALT{i}" for i in range(1, 21)]
    with open(path, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fields)
        writer.writeheader()
        for nm, alts in data:
            row = {"å•†å“å": nm}
            for i in range(20):
                row[f"ALT{i+1}"] = alts[i] if i < len(alts) else ""
            writer.writerow(row)


# ========= 8. ãƒ­ã‚°æ•´å½¢ =========
def show_progress(idx: int, total: int, name: str, avg_len: float):
    bar_len = 30
    filled = int(bar_len * (idx / total))
    bar = "â–ˆ" * filled + "-" * (bar_len - filled)
    print(f"ğŸ§  [{bar}] {idx}/{total} {name[:25]}... å¹³å‡{avg_len:.1f}å­—")


# ========= 9. main =========
def main():
    print("ğŸŒ¸ v3.3_altfix_utf8_final_schema_refine_stable2_fixed_full å®Ÿè¡Œé–‹å§‹ï¼ˆgpt-4oå®‰å…¨ä»•æ§˜ï¼‰")
    print(f"âœ… ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {MODEL}")
    print(f"âœ… å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {INPUT_RAKUTEN}")
    products = read_products(INPUT_RAKUTEN)
    print(f"âœ… å•†å“åæŠ½å‡º: {len(products)}ä»¶ï¼ˆé‡è¤‡é™¤å»æ¸ˆï¼‰")

    results = []
    for i, nm in enumerate(products, 1):
        try:
            alts = ai_generate_alt(nm)
        except Exception as e:
            print(f"âš ï¸ {nm[:25]}... ç”Ÿæˆä¸­æ–­ ({e})")
            fallback = finalize_sentence(f"{nm} ã®ç‰¹é•·ã‚’æ´»ã‹ã—ãŸå®Ÿç”¨çš„ãªè¨­è¨ˆã€‚æ—¥å¸¸ã‚’å¿«é©ã«ã™ã‚‹å„ªã‚ŒãŸæ©Ÿèƒ½æ€§ã€‚")
            alts = [fallback] * 20
        avg_len = sum(len(a) for a in alts) / len(alts)
        show_progress(i, len(products), nm, avg_len)
        results.append((nm, alts))

    write_alt_csv(OUTPUT_CSV, results)
    print(f"âœ… å‡ºåŠ›å®Œäº†: {OUTPUT_CSV}")
    print("âœ… ALT: AIã§100ã€œ130å­—â†’ãƒ­ãƒ¼ã‚«ãƒ«æ•´å½¢ã§80ã€œ110å­—ã«åæŸã€‚ç¦å‰‡èªé©ç”¨ãƒ»å¥ç‚¹æ•´å½¢æ¸ˆã€‚")


# ========= 10. å®Ÿè¡Œã‚¨ãƒ³ãƒˆãƒª =========
if __name__ == "__main__":
    main()
import atlas_autosave_core
