# -*- coding: utf-8 -*-
"""
v4.2_persona_template_fusion.py
ALTé•·æ–‡ï¼ˆSEOï¼‹è‡ªç„¶æ–‡ï¼‰ç”Ÿæˆï¼šãƒšãƒ«ã‚½ãƒŠÃ—ãƒ†ãƒ³ãƒ—ãƒ¬Ã—ãƒ­ãƒ¼ã‚«ãƒ«çŸ¥è¦‹ã®èåˆç‰ˆ

- å…¥åŠ›: ./rakuten.csvï¼ˆUTF-8, ãƒ˜ãƒƒãƒ€ã«ã€Œå•†å“åã€ï¼‰
- å‚ç…§: ./output/semantics/ å†…ã® JSON ç¾¤ï¼ˆå­˜åœ¨ã™ã‚Œã°è‡ªå‹•çµ±åˆã€‚ç„¡ã‘ã‚Œã°æ—¢å®šã®çŸ¥è¦‹æ–‡ã‚’ä½¿ç”¨ï¼‰
- å‡ºåŠ›:
  1) output/ai_writer/alt_text_ai_raw_longform_v4.2.csv        â€¦ AIã®ç”Ÿå‡ºåŠ›ï¼ˆ20æœ¬/å•†å“ï¼‰
  2) output/ai_writer/alt_text_refined_final_longform_v4.2.csv â€¦ ãƒ­ãƒ¼ã‚«ãƒ«æ•´å½¢å¾Œï¼ˆ80ã€œ110å­—ï¼‰
  3) output/ai_writer/alt_text_diff_longform_v4.2.csv          â€¦ raw/refined ã®æ¨ªä¸¦ã³æ¯”è¼ƒ

- OpenAI:
    model                 = .envå›ºå®šï¼ˆgpt-5ã‚’æ¨å¥¨ï¼‰
    response_format       = {"type":"text"}
    max_completion_tokens = 1000
    temperature           = 1
- ãƒãƒªã‚·ãƒ¼:
    * ç”»åƒæå†™èªãƒ»ECãƒ¡ã‚¿èªãƒ»èª‡å¼µè¡¨ç¾NGï¼ˆFORBIDDENï¼‹normalized.jsonã®ç¦æ­¢èªã‚’çµ±åˆï¼‰
    * ç®‡æ¡æ›¸ã/ç•ªå·ï¼ˆ1. ãƒ» ãªã©ï¼‰ç¦æ­¢ã€å¿…ãšå¥ç‚¹ã€Œã€‚ã€ã§çµ‚ãˆã‚‹
    * AIã¯100ã€œ130å­—/è¡Œã‚’ç›®æ¨™ã€ãƒ­ãƒ¼ã‚«ãƒ«ã§80ã€œ110å­—ã«è‡ªç„¶ã‚«ãƒƒãƒˆ
"""

import os
import re
import csv
import glob
import json
import time
from typing import List, Tuple, Dict, Any

from dotenv import load_dotenv

# tqdmï¼ˆç„¡ã‘ã‚Œã°ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
try:
    from tqdm.auto import tqdm
except Exception:
    def tqdm(x, **k): return x

# ========= OpenAI SDK =========
try:
    from openai import OpenAI
except Exception:
    raise SystemExit("openai SDK ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚`pip install openai python-dotenv` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

# =========================
# å®šæ•°ãƒ»ãƒ‘ã‚¹
# =========================
INPUT_CSV = "./rakuten.csv"        # UTF-8, ãƒ˜ãƒƒãƒ€ã€Œå•†å“åã€
SEMANTICS_DIR = "./output/semantics"

OUT_DIR   = "./output/ai_writer"
RAW_PATH  = os.path.join(OUT_DIR, "alt_text_ai_raw_longform_v4.2.csv")
REF_PATH  = os.path.join(OUT_DIR, "alt_text_refined_final_longform_v4.2.csv")
DIFF_PATH = os.path.join(OUT_DIR, "alt_text_diff_longform_v4.2.csv")

# æ–‡å­—æ•°ãƒ¬ãƒ³ã‚¸
RAW_MIN, RAW_MAX = 100, 130    # AIç”Ÿæˆã®ç›®æ¨™é•·
FINAL_MIN, FINAL_MAX = 80, 110 # ãƒ­ãƒ¼ã‚«ãƒ«æ•´å½¢ã®ç›®æ¨™é•·

# ç¦å‰‡èªï¼ˆåˆæœŸï¼‰
FORBIDDEN_BASE = [
    # ç”»åƒæå†™ãƒ»æŒ‡ç¤ºèª
    "ç”»åƒ", "å†™çœŸ", "è¦‹ãŸç›®", "ä¸Šã®ç”»åƒ", "ä¸‹ã®å†™çœŸ",
    # ECãƒ¡ã‚¿/ãƒªãƒ³ã‚¯ç³»
    "å½“åº—", "å½“ç¤¾", "ãƒ¬ãƒ“ãƒ¥ãƒ¼", "ãƒ©ãƒ³ã‚­ãƒ³ã‚°", "ã‚¯ãƒªãƒƒã‚¯", "ã“ã¡ã‚‰", "ãƒªãƒ³ã‚¯", "ã‚«ãƒ¼ãƒˆ", "è³¼å…¥ã¯ã“ã¡ã‚‰",
    # èª‡å¼µ/ãƒ¡ã‚¿ç«¶åˆ
    "ç«¶åˆ", "å„ªä½æ€§", "æ¥­ç•Œæœ€é«˜", "æœ€å®‰", "No.1", "ãƒŠãƒ³ãƒãƒ¼ãƒ¯ãƒ³", "å£²ä¸ŠNo1",
    # ä¿è¨¼ç³»ã®èª¤è§£æ‹›ãå¼·è¡¨ç¾
    "é€æ–™ç„¡æ–™ï¼ˆç¢ºç´„ï¼‰", "è¿”é‡‘ä¿è¨¼",
]

# ãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ–ç”¨
LEADING_ENUM_RE = re.compile(r"^\s*[\dâ‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©\-\*\ãƒ»\u2022]\s*[\.ï¼ã€]?\s*")
WHITESPACE_RE   = re.compile(r"\s+")
MULTI_COMMA_RE  = re.compile(r"ã€{3,}")

# =========================
# ç’°å¢ƒåˆæœŸåŒ–
# =========================
def init_env_and_client():
    load_dotenv(override=True)
    api_key = os.getenv("OPENAI_API_KEY", "").strip()
    if not api_key:
        raise SystemExit("OPENAI_API_KEY ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚.env ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    # ãƒ¢ãƒ‡ãƒ«ã¯ .env ã‚’å°Šé‡ï¼ˆgpt-5 ã‚’æƒ³å®šï¼‰ã€‚ç©ºãªã‚‰ gpt-5 ã‚’æ—¢å®šã€‚
    model_env = os.getenv("OPENAI_MODEL", "").strip() or "gpt-5"
    client = OpenAI(api_key=api_key)
    return client, model_env

# =========================
# å…¥åŠ›ï¼ˆå•†å“åï¼‰
# =========================
def load_products(path: str) -> List[str]:
    if not os.path.exists(path):
        raise SystemExit(f"å…¥åŠ›CSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path}")
    items = []
    with open(path, "r", encoding="utf-8", newline="") as f:
        r = csv.DictReader(f)
        if not r.fieldnames or "å•†å“å" not in r.fieldnames:
            raise SystemExit("å…¥åŠ›CSVã«ã€å•†å“åã€ãƒ˜ãƒƒãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        for row in r:
            nm = (row.get("å•†å“å") or "").strip()
            if nm:
                items.append(nm)
    # é‡è¤‡é™¤å»ï¼ˆé †åºä¿æŒï¼‰
    seen, uniq = set(), []
    for nm in items:
        if nm not in seen:
            seen.add(nm)
            uniq.append(nm)
    return uniq

# =========================
# çŸ¥è¦‹ãƒ­ãƒ¼ãƒ‰ï¼ˆJSONï¼‰
# =========================
def safe_json_load(path: str):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None

def listify(x):
    if isinstance(x, list): return x
    if isinstance(x, dict): return [x]
    return []

def summarize_knowledge_fusion() -> Tuple[str, List[str]]:
    """
    /output/semantics/ é…ä¸‹ã®JSONã‚’ã‚†ã‚‹ãå¸åã—ã€AIã«æ¸¡ã™ã€Œæ—¥æœ¬èªçŸ¥è¦‹æ–‡ã€ã‚’ç”Ÿæˆã€‚
    ã¾ãŸã€ç¦æ­¢èªã‚’FORBIDDEN_BASEã¨çµ±åˆã—ã¦è¿”ã™ã€‚
    """
    keywords, scenes, targets, specs, market, templates, tones, extra_forbidden = [], [], [], [], [], [], [], []

    if os.path.isdir(SEMANTICS_DIR):
        for p in glob.glob(os.path.join(SEMANTICS_DIR, "*.json")):
            name = os.path.basename(p).lower()
            data = safe_json_load(p)
            if data is None:
                continue

            try:
                # lexical / cluster ç³»: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
                if "lexical" in name or "cluster" in name:
                    arr = data.get("clusters") if isinstance(data, dict) else data
                    for c in listify(arr):
                        terms = c.get("terms") if isinstance(c, dict) else None
                        if isinstance(terms, list):
                            keywords.extend([t for t in terms if isinstance(t, str)])

                # structured_semantics ç³»: æ§‹é€ æƒ…å ±
                if "semantic" in name or "structured" in name:
                    if isinstance(data, dict):
                        for k in ("scenes", "targets", "specs", "concepts", "use_cases"):
                            for v in (data.get(k) or []):
                                if not isinstance(v, str): continue
                                if k == "scenes":   scenes.append(v)
                                elif k == "targets": targets.append(v)
                                elif k == "specs":  specs.append(v)
                                else:               keywords.append(v)

                # market èªå½™
                if "market" in name or "vocab" in name:
                    if isinstance(data, list):
                        for v in data:
                            if isinstance(v, dict) and isinstance(v.get("vocabulary"), str):
                                market.append(v["vocabulary"])
                            elif isinstance(v, str):
                                market.append(v)
                    elif isinstance(data, dict):
                        vocab = data.get("vocabulary") or data.get("vocab") or []
                        market.extend([x for x in vocab if isinstance(x, str)])

                # persona / style
                if "persona" in name or "style" in name:
                    if isinstance(data, dict):
                        t = data.get("tone") or {}
                        if isinstance(t, dict):
                            for v in t.values():
                                if isinstance(v, str):
                                    tones.append(v)
                        # æ–‡å­—åˆ—ã®é…åˆ—ã§ tone ã‚’æŒã¤å½¢å¼ã«ã‚‚å¯¾å¿œ
                        if isinstance(data.get("tone"), list):
                            for v in data["tone"]:
                                if isinstance(v, str):
                                    tones.append(v)

                # template composer / hints
                if "template" in name or "composer" in name:
                    if isinstance(data, dict):
                        for key in ("hints", "templates"):
                            for v in (data.get(key) or []):
                                if isinstance(v, str):
                                    templates.append(v)

                # normalized / forbid
                if "normalized" in name or "forbid" in name:
                    if isinstance(data, dict):
                        for v in (data.get("forbidden_words") or []):
                            if isinstance(v, str):
                                extra_forbidden.append(v)

            except Exception:
                # å½¢å¼ãŒãƒãƒ©ã¤ããƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå …ç‰¢è¨­è¨ˆï¼‰
                continue

    # ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–
    def uniq(lst): return list(dict.fromkeys([x for x in lst if isinstance(x, str) and x.strip()]))

    keywords = uniq(keywords)
    scenes   = uniq(scenes)
    targets  = uniq(targets)
    specs    = uniq(specs)
    market   = uniq(market)
    templates = uniq(templates)
    tones     = uniq(tones)
    extra_forbidden = uniq(extra_forbidden)

    # æ—¥æœ¬èªçŸ¥è¦‹æ–‡ã‚’åˆæˆï¼ˆå†—é•·ã«ãªã‚‰ãªã„ã‚ˆã†ä¸Šé™ã‚’ã‹ã‘ã‚‹ï¼‰
    def cap_join(xs, n): return "ã€".join(xs[:n]) if xs else ""

    blocks = []
    if keywords: blocks.append(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¾‹: {cap_join(keywords, 12)}")
    if specs:    blocks.append(f"ä»•æ§˜ãƒ»æ©Ÿèƒ½ä¾‹: {cap_join(specs, 10)}")
    if scenes:   blocks.append(f"åˆ©ç”¨ã‚·ãƒ¼ãƒ³ä¾‹: {cap_join(scenes, 8)}")
    if targets:  blocks.append(f"æƒ³å®šãƒ¦ãƒ¼ã‚¶ãƒ¼ä¾‹: {cap_join(targets, 8)}")
    if market:   blocks.append(f"å¸‚å ´èªå½™: {cap_join(market, 12)}")
    if templates: blocks.append(f"æ§‹æˆãƒ’ãƒ³ãƒˆä¾‹: {cap_join(templates, 3)}")
    if tones:    blocks.append(f"æ–‡ä½“ã‚¬ã‚¤ãƒ‰ä¾‹: {cap_join(tones, 3)}")

    if blocks:
        knowledge = "çŸ¥è¦‹ã¾ã¨ã‚: " + " / ".join(blocks) + "ã€‚"
    else:
        knowledge = "çŸ¥è¦‹ã¾ã¨ã‚: å¯¾å¿œæ©Ÿç¨®åãƒ»ä¸»è¦ã‚¹ãƒšãƒƒã‚¯ãƒ»ç”¨é€”ãƒ»å°å…¥ãƒ¡ãƒªãƒƒãƒˆã‚’è‡ªç„¶ã«ç¹”ã‚Šè¾¼ã¿ã€2æ–‡ä»¥å†…ã§ç°¡æ½”ã«ã€‚"

    # ç¦å‰‡èªã®çµ±åˆ
    forbidden_all = uniq(FORBIDDEN_BASE + extra_forbidden)
    return knowledge, forbidden_all

# =========================
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
# =========================
def build_system_prompt(knowledge_text: str, tone_hint: str) -> str:
    # tone_hint ã¯ç©ºã§ã‚‚OKã€‚ã‚ã‚Œã°æ´»ã‹ã™ã€‚
    tone_line = f"ã€æ–‡ä½“ã‚¬ã‚¤ãƒ‰ã€‘{tone_hint}\n" if tone_hint else ""
    sys = (
        "ã‚ãªãŸã¯ECç”»åƒã®ALTãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œã‚‹æ—¥æœ¬èªã®ãƒ—ãƒ­ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚\n"
        "ç›®çš„ã¯ã€æ¥½å¤©ã®ã‚µã‚¤ãƒˆå†…SEOã«å¼·ã„è‡ªç„¶æ–‡ALTã‚’20æœ¬ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã™ã€‚\n"
        + tone_line +
        "ã€å³å®ˆãƒ«ãƒ¼ãƒ«ã€‘\n"
        "ãƒ»ç”»åƒã‚„å†™çœŸã®æå†™èªï¼ˆä¾‹ï¼šç”»åƒã€å†™çœŸã€è¦‹ãŸç›®ã€ä¸Šã®ç”»åƒ ç­‰ï¼‰ã¯ä½¿ã‚ãªã„ã€‚\n"
        "ãƒ»ECãƒ¡ã‚¿èªï¼ˆå½“åº—ã€ãƒ¬ãƒ“ãƒ¥ãƒ¼ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã€ãƒªãƒ³ã‚¯ã€è³¼å…¥ã¯ã“ã¡ã‚‰ ç­‰ï¼‰ã¯ä½¿ã‚ãªã„ã€‚\n"
        "ãƒ»ç«¶åˆæ¯”è¼ƒã‚„â€œç«¶åˆå„ªä½æ€§â€ã®ã‚ˆã†ãªãƒ¡ã‚¿è¡¨ç¾ã¯ç¦æ­¢ã€‚\n"
        f"ãƒ»å„è¡Œã¯å…¨è§’ãŠã‚ˆã{RAW_MIN}ã€œ{RAW_MAX}æ–‡å­—ã€1ã€œ2æ–‡ã§è‡ªç„¶ã«ã€‚å¿…ãšå¥ç‚¹ã€Œã€‚ã€ã§çµ‚ãˆã‚‹ã€‚\n"
        "ãƒ»ç®‡æ¡æ›¸ãã‚„ç•ªå·ï¼ˆ1. 2. ãƒ» ãªã©ï¼‰ã‚„ãƒ©ãƒ™ãƒ«ï¼ˆALT: ç­‰ï¼‰ã¯ä»˜ã‘ãªã„ã€‚\n"
        "ãƒ»å•†å“åãƒ»å¯¾å¿œæ©Ÿç¨®ãƒ»ã‚¹ãƒšãƒƒã‚¯ãƒ»æ©Ÿèƒ½ãƒ»ç”¨é€”ãƒ»å¯¾è±¡ãƒ»ãƒ™ãƒãƒ•ã‚£ãƒƒãƒˆã‚’è‡ªç„¶ã«ç¹”ã‚Šè¾¼ã‚€ï¼ˆè©°ã‚è¾¼ã¿ç¦æ­¢ï¼‰ã€‚\n"
        "ãƒ»å‡ºåŠ›ã¯20è¡Œã®ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ï¼ˆJSONã‚„è¨˜å·ãªã—ï¼‰ã€‚\n"
        "\n"
        f"{knowledge_text}\n"
    )
    return sys

def build_user_prompt(product: str, forbidden_words: List[str]) -> str:
    forbid_txt = "ã€".join(sorted(set([w for w in forbidden_words if isinstance(w, str)])))
    hint = "æ§‹æˆãƒ’ãƒ³ãƒˆï¼ˆè‡ªç„¶ã«ä½¿ã†ï¼‰ï¼šå•†å“ã‚¹ãƒšãƒƒã‚¯â†’ã‚³ã‚¢ã‚³ãƒ³ãƒ”ã‚¿ãƒ³ã‚¹â†’ã©ã‚“ãªäººâ†’ã‚·ãƒ¼ãƒ³â†’ãƒ™ãƒãƒ•ã‚£ãƒƒãƒˆã€‚"
    return (
        f"å•†å“å: {product}\n"
        f"{hint}\n"
        f"ç¦æ­¢èªï¼ˆçµ¶å¯¾ã«ä½¿ã‚ãªã„ï¼‰: {forbid_txt}\n"
        "20è¡Œã§ã€å„è¡Œã¯ã²ã¨ã¤ã®è‡ªç„¶æ–‡ï¼ˆ1ã€œ2æ–‡å†…ï¼‰ã€‚å¥ç‚¹ã§çµ‚ãˆã‚‹ã“ã¨ã€‚"
    )

# =========================
# OpenAI å‘¼ã³å‡ºã—
# =========================
def call_openai_alt20(client: OpenAI, model: str, system_prompt: str, user_prompt: str,
                      retry: int = 3, wait: int = 6) -> List[str]:
    last_err = None
    for _ in range(retry):
        try:
            res = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                response_format={"type": "text"},
                max_completion_tokens=1000,
                temperature=1,
            )
            content = (res.choices[0].message.content or "").strip()
            if not content:
                raise RuntimeError("Empty content")

            # è¡Œå˜ä½ã«ã—ã€ç®‡æ¡æ›¸ã/ç•ªå·ã‚’å‰¥ãŒã™
            lines = [ln.strip() for ln in content.split("\n") if ln.strip()]
            clean = []
            for ln in lines:
                ln2 = LEADING_ENUM_RE.sub("", ln)
                ln2 = ln2.strip("ãƒ»-â€”â—ã€€")
                if ln2:
                    clean.append(ln2)

            # 20è¡Œä»¥ä¸Šè¿”ã‚‹ãƒ¢ãƒ‡ãƒ«æŒ™å‹•ãŒã‚ã‚‹ãŸã‚ã€ä¸€æ—¦æœ€å¤§60è¡Œã¾ã§å—ã‘å–ã‚Šå¾Œå·¥ç¨‹ã§20æœ¬ã«æ•´å½¢
            return clean[:60]
        except Exception as e:
            last_err = e
            time.sleep(wait)

    raise RuntimeError(f"OpenAIå¿œç­”ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ: {last_err}")

# =========================
# ãƒ­ãƒ¼ã‚«ãƒ«æ•´å½¢
# =========================
def soft_clip_sentence(text: str, min_len=FINAL_MIN, max_len=FINAL_MAX) -> str:
    t = (text or "").strip()
    if not t:
        return t
    # å¥ç‚¹çµ‚æ­¢
    if not t.endswith("ã€‚"):
        t += "ã€‚"
    # ç©ºç™½åœ§ç¸®/èª­ç‚¹æ•´å½¢
    t = WHITESPACE_RE.sub(" ", t)
    t = MULTI_COMMA_RE.sub("ã€ã€", t)
    t = LEADING_ENUM_RE.sub("", t).strip("ãƒ»-â€”â—ã€€")

    # é•·ã™ãã‚‹å ´åˆã¯120ã¾ã§è¨±å®¹ã—ã€ç›´è¿‘ã®å¥ç‚¹ã§è‡ªç„¶ã‚«ãƒƒãƒˆ
    if len(t) > 120:
        cut = t[:120]
        p = cut.rfind("ã€‚")
        if p != -1:
            t = cut[:p+1]
        else:
            t = cut

    # ç¦å‰‡èªã¯å®Œå…¨é™¤å»
    for ng in FORBIDDEN_BASE:
        if ng and ng in t:
            t = t.replace(ng, "")

    return t.strip()

def refine_20_lines(raw_lines: List[str]) -> List[str]:
    # æ­£è¦åŒ–â†’ãƒ•ã‚£ãƒ«ã‚¿
    norm = []
    for ln in raw_lines:
        ln = (ln or "").strip()
        if not ln:
            continue
        ln = LEADING_ENUM_RE.sub("", ln).strip("ãƒ»-â€”â—ã€€")
        ln = soft_clip_sentence(ln)
        if len(ln) < 15:
            continue
        norm.append(ln)

    # é‡è¤‡é™¤å»
    uniq, seen = [], set()
    for ln in norm:
        if ln not in seen:
            uniq.append(ln)
            seen.add(ln)

    # é•·ã•ãƒ¬ãƒ³ã‚¸ã«å¯„ã›ã‚‹æœ€çµ‚èª¿æ•´
    refined = [soft_clip_sentence(ln) for ln in uniq]

    # è¶³ã‚Šãªã„å ´åˆã¯è»½ã„ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã§è£œå®Œï¼ˆåŒç¾©èªã®ã‚ˆã†ãªæœ€å°å¤‰å½¢ï¼‰
    def light_var(s: str) -> str:
        s2 = s
        s2 = s2.replace("ã—ã¾ã™ã€‚", "ã§ãã¾ã™ã€‚")
        s2 = s2.replace("ã§ãã¾ã™ã€‚", "ã—ã‚„ã™ã„ã§ã™ã€‚")
        s2 = s2.replace("ã§ã™ã€‚", "ã«ãªã‚Šã¾ã™ã€‚")
        if s2 == s:
            s2 = re.sub(r"(\S{2,})", r"\1ã€", s, count=1)
            s2 = s2.replace("ã€ã€", "ã€")
            if not s2.endswith("ã€‚"):
                s2 += "ã€‚"
        return soft_clip_sentence(s2)

    i = 0
    while len(refined) < 20 and refined:
        refined.append(light_var(refined[i % len(refined)]))
        i += 1

    return refined[:20]

# =========================
# æ›¸ãå‡ºã—
# =========================
def ensure_outdir():
    os.makedirs(OUT_DIR, exist_ok=True)

def write_raw(products: List[str], all_raw: List[List[str]]):
    with open(RAW_PATH, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["å•†å“å"] + [f"ALT_raw_{i+1}" for i in range(20)])
        for p, lines in zip(products, all_raw):
            row = [p] + (lines[:20] + [""] * max(0, 20 - len(lines)))
            w.writerow(row)

def write_refined(products: List[str], all_refined: List[List[str]]):
    with open(REF_PATH, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["å•†å“å"] + [f"ALT_{i+1}" for i in range(20)])
        for p, lines in zip(products, all_refined):
            row = [p] + (lines[:20] + [""] * max(0, 20 - len(lines)))
            w.writerow(row)

def write_diff(products: List[str], all_raw: List[List[str]], all_refined: List[List[str]]):
    with open(DIFF_PATH, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        header = ["å•†å“å"] + [f"ALT_raw_{i+1}" for i in range(20)] + [f"ALT_refined_{i+1}" for i in range(20)]
        w.writerow(header)
        for p, r, ref in zip(products, all_raw, all_refined):
            r_line = (r[:20] + [""] * max(0, 20 - len(r)))
            ref_line = (ref[:20] + [""] * max(0, 20 - len(ref)))
            w.writerow([p] + r_line + ref_line)

# =========================
# ãƒ¡ã‚¤ãƒ³
# =========================
def main():
    print("ğŸŒ¸ v4.2_persona_template_fusionï¼šALTç”Ÿæˆï¼ˆãƒšãƒ«ã‚½ãƒŠÃ—ãƒ†ãƒ³ãƒ—ãƒ¬Ã—çŸ¥è¦‹çµ±åˆï¼‰")
    client, model = init_env_and_client()
    ensure_outdir()

    products = load_products(INPUT_CSV)
    print(f"âœ… å¯¾è±¡å•†å“: {len(products)}ä»¶")

    knowledge_text, forbidden_all = summarize_knowledge_fusion()

    # æ–‡ä½“ãƒ’ãƒ³ãƒˆï¼ˆknowledge_textå†…ã®â€œæ–‡ä½“ã‚¬ã‚¤ãƒ‰ä¾‹:â€ã‚’æ‹¾ã£ã¦ä½¿ã†ç¨‹åº¦ï¼‰
    tone_hint = ""
    m = re.search(r"æ–‡ä½“ã‚¬ã‚¤ãƒ‰ä¾‹:\s*(.+?)(?:ã€‚|$)", knowledge_text)
    if m:
        tone_hint = m.group(1).strip()

    system_prompt = build_system_prompt(knowledge_text, tone_hint)

    all_raw, all_refined = [], []

    for p in tqdm(products, desc="ğŸ§  AIç”Ÿæˆä¸­", total=len(products)):
        user_prompt = build_user_prompt(p, forbidden_all)
        try:
            raw_lines = call_openai_alt20(client, model, system_prompt, user_prompt)
        except Exception:
            # ã©ã†ã—ã¦ã‚‚å–å¾—ã§ããªã„å ´åˆã¯æœ€ä½é™ã®ãƒ€ãƒŸãƒ¼20æœ¬ï¼ˆç©ºã¯é¿ã‘ã‚‹ï¼‰
            raw_lines = [f"{p} ã®ä½¿ã„å‹æ‰‹ã‚’é«˜ã‚ã‚‹è¨­è¨ˆã§ã€æ—¥å¸¸ã®ä¸ä¾¿ã‚’æ¸›ã‚‰ã—ã¾ã™ã€‚"] * 20

        refined_lines = refine_20_lines(raw_lines)

        all_raw.append(raw_lines[:20])
        all_refined.append(refined_lines)

        # è»½ã„ã‚¹ãƒ­ãƒƒãƒˆãƒªãƒ³ã‚°ï¼ˆAPIå®‰å®šï¼‰
        time.sleep(0.2)

    write_raw(products, all_raw)
    write_refined(products, all_refined)
    write_diff(products, all_raw, all_refined)

    # çµ±è¨ˆè¡¨ç¤º
    def avg_len(blocks):
        lens = [len(x) for lines in blocks for x in lines if x]
        return (sum(lens) / max(1, len(lens)))

    print("âœ… å‡ºåŠ›å®Œäº†:")
    print(f"   - AIç”Ÿå‡ºåŠ› : {RAW_PATH}")
    print(f"   - æ•´å½¢å¾Œ   : {REF_PATH}")
    print(f"   - å·®åˆ†æ¯”è¼ƒ : {DIFF_PATH}")
    print(f"ğŸ“ æ–‡å­—æ•°(å¹³å‡): raw={avg_len(all_raw):.1f} / refined={avg_len(all_refined):.1f}")
    print("ğŸ”’ ä»•æ§˜ã¾ã¨ã‚:")
    print(f"   - AI: ç´„{RAW_MIN}ã€œ{RAW_MAX}å­—ãƒ»1ã€œ2æ–‡ãƒ»å¥ç‚¹çµ‚æ­¢ãƒ»ç¦å‰‡é©ç”¨ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰")
    print(f"   - ãƒ­ãƒ¼ã‚«ãƒ«: {FINAL_MIN}ã€œ{FINAL_MAX}å­—ã«è‡ªç„¶ã‚«ãƒƒãƒˆã€é‡è¤‡é™¤å»ãƒ»å¥ç‚¹è£œå®Œãƒ»ç¦å‰‡å†é©ç”¨")
    print("   - çŸ¥è¦‹: /output/semantics ã®JSONç¾¤ï¼ˆlexical/semantic/market/persona/template/normalizedï¼‰ã‚’è‡ªå‹•çµ±åˆ")

if __name__ == "__main__":
    main()
import atlas_autosave_core
