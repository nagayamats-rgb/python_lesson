# -*- coding: utf-8 -*-
"""
v4.3_natural_language_reinforce.py
ALTé•·æ–‡ï¼ˆæ¥½å¤©å°‚ç”¨ / è‡ªç„¶æ–‡å¼·åŒ–ãƒ»ä½“è¨€æ­¢ã‚è¨±å®¹ãƒ»çŸ¥è¦‹ã‚†ã‚‹çµåˆãƒ»raw/refined/diff å‡ºåŠ›ï¼‰

å…¥åŠ›:
  - ./rakuten.csv  ï¼ˆUTF-8 / ãƒ˜ãƒƒãƒ€ã«ã€Œå•†å“åã€å¿…é ˆï¼‰

çŸ¥è¦‹ï¼ˆä»»æ„ãƒ»ã‚ã‚Œã°æ´»ç”¨ï¼‰:
  - ./output/semantics/*.json   ï¼ˆå½¢å¼ãƒãƒ©ãƒãƒ©OKã€ã‚†ã‚‹çµåˆã§è¦ç´„ï¼‰
    ä¾‹: lexical_clusters_*.json / structured_semantics_*.json / market_vocab_*.json /
        styled_persona_*.json / normalized_*.json / template_composer.json ãªã©

å‡ºåŠ›:
  - ./output/ai_writer/alt_text_ai_raw_longform_v4.3.csv
  - ./output/ai_writer/alt_text_refined_final_longform_v4.3.csv
  - ./output/ai_writer/alt_text_diff_longform_v4.3.csv

OpenAI:
  - .env ã«ã¦å›ºå®šï¼ˆä¾‹ï¼‰
      OPENAI_API_KEY="..."
      OPENAI_MODEL="gpt-5"
      OPENAI_MODE="chat"
      OPENAI_TEMPERATURE="1"
      OPENAI_MAX_TOKENS="1000"

å‘¼ã³å‡ºã—å›ºå®š:
  - response_format={"type": "text"}
  - temperature ã¯ .env å€¤ãŒã‚ã£ã¦ã‚‚ 1 ã‚’å¼·åˆ¶ï¼ˆå®‰å®šæœ€å„ªå…ˆï¼‰
  - max_completion_tokens ã¯ .env å€¤ãŒã‚ã£ã¦ã‚‚ 1000 ã‚’å¼·åˆ¶
  - ç¦æ­¢: ç”»åƒæå†™èª / åº—èˆ—ãƒ¡ã‚¿èª / ç«¶åˆå„ªä½ãƒ¡ã‚¿ / ã‚¯ãƒªãƒƒã‚¯èª˜å° ç­‰

ä»•æ§˜ãƒã‚¤ãƒ³ãƒˆ:
  - ã¾ãšAIã§ 100ã€œ130å­—ãƒ»1ã€œ2æ–‡ãƒ»å¥ç‚¹çµ‚æ­¢ãƒ»è‡ªç„¶æ–‡ï¼ˆä½“è¨€æ­¢ã‚è¨±å®¹ï¼‰ã§20è¡Œç”Ÿæˆï¼ˆrawï¼‰
  - ãƒ­ãƒ¼ã‚«ãƒ«æ•´å½¢ã§ 80ã€œ110å­—ã¸è‡ªç„¶ã‚«ãƒƒãƒˆã€å¥ç‚¹è£œå®Œã€ç¦å‰‡å†é©ç”¨ã€é‡è¤‡/çŸ­æ–‡/åè©ç¾…åˆ—ã‚±ã‚¢ï¼ˆrefinedï¼‰
  - raw ã¨ refined ã®æ¨ªä¸¦ã³ diff ã‚‚ä¿å­˜ï¼ˆQAç”¨ï¼‰
  - ã€Œè¦ï¼ˆã‹ã‚“ãªã‚ï¼‰ã€åŸ‹ã‚è¾¼ã¿ï¼ˆKANNAME_BANNERï¼‰
"""

import os
import re
import csv
import glob
import json
import time
import random
from collections import defaultdict

from dotenv import load_dotenv

# tqdmï¼ˆç„¡ãã¦ã‚‚å‹•ããƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
try:
    from tqdm.auto import tqdm
except Exception:
    def tqdm(x, **k): return x

# =========================
# ã‹ã‚“ãªã‚ï¼ˆè¦ï¼‰
# =========================
KANNAME_BANNER = "ã€è¦ã€‘KOTOHA-ALT v4.3 / Natural Language Reinforce / Rakutenå°‚ç”¨"

# =========================
# OpenAI SDK
# =========================
try:
    from openai import OpenAI
except Exception:
    raise SystemExit("openai SDK ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚`pip install openai python-dotenv` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

# =========================
# å®šæ•°
# =========================
INPUT_CSV  = "./rakuten.csv"
OUT_DIR    = "./output/ai_writer"
RAW_PATH   = os.path.join(OUT_DIR, "alt_text_ai_raw_longform_v4.3.csv")
REF_PATH   = os.path.join(OUT_DIR, "alt_text_refined_final_longform_v4.3.csv")
DIFF_PATH  = os.path.join(OUT_DIR, "alt_text_diff_longform_v4.3.csv")

SEMANTICS_DIR = "./output/semantics"

# ã¾ãšAIã§ç›®æŒ‡ã™é•·ã• â†’ ãƒ­ãƒ¼ã‚«ãƒ«ã§æœ€çµ‚æ•´å½¢
RAW_MIN, RAW_MAX     = 100, 130    # AI ç›®æ¨™
FINAL_MIN, FINAL_MAX = 80, 110     # ãƒ­ãƒ¼ã‚«ãƒ«æ•´å½¢ç›®æ¨™

# æ¥½å¤©ALTå°‚ç”¨ ç¦å‰‡èªï¼ˆç”»åƒæå†™ãƒ»åº—èˆ—/èª˜å°ãƒ¡ã‚¿ãƒ»ç«¶åˆãƒ¡ã‚¿ ç­‰ï¼‰
FORBIDDEN_BASE = [
    "ç”»åƒ", "å†™çœŸ", "è¦‹ãŸç›®", "ä¸Šã®ç”»åƒ", "ä¸‹ã®å†™çœŸ", "å›³", "ã‚¤ãƒ©ã‚¹ãƒˆ",
    "å½“åº—", "å½“ç¤¾", "ã‚·ãƒ§ãƒƒãƒ—", "è²©å£²åº—", "ãƒ¬ãƒ“ãƒ¥ãƒ¼", "ãƒ©ãƒ³ã‚­ãƒ³ã‚°", "å£ã‚³ãƒŸ",
    "ã‚¯ãƒªãƒƒã‚¯", "ã“ã¡ã‚‰", "ãƒšãƒ¼ã‚¸", "ãƒªãƒ³ã‚¯", "ã‚«ãƒ¼ãƒˆ", "è³¼å…¥ã¯ã“ã¡ã‚‰", "ä»Šã™ã", "é™å®š", "æœ€å®‰",
    "No.1", "ãƒŠãƒ³ãƒãƒ¼ãƒ¯ãƒ³", "ä¸–ç•Œä¸€", "å„ªä½æ€§", "ç«¶åˆ", "ä»–ç¤¾",
    "é€æ–™ç„¡æ–™ï¼ˆç¢ºç´„ï¼‰", "è¿”é‡‘ä¿è¨¼", "å‰²å¼•", "SALE", "ã‚»ãƒ¼ãƒ«", "ãƒã‚¤ãƒ³ãƒˆé‚„å…ƒ",
]

# æ­£è¦è¡¨ç¾
LEADING_ENUM_RE  = re.compile(r"^\s*[\dâ‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©\-\*\ãƒ»\u2022]\s*[\.ï¼ã€]?\s*")
MULTI_COMMA_RE   = re.compile(r"ã€{3,}")
WHITESPACE_RE    = re.compile(r"\s+")
PARENS_TRIM_RE   = re.compile(r"[ï¼ˆ(]\s*[)ï¼‰]\s*")  # ç©ºæ‹¬å¼§æ¶ˆã—
LATIN_LISTY_RE   = re.compile(r"[A-Za-z0-9]+(?:\s*[ï¼/ãƒ»,]\s*[A-Za-z0-9]+){2,}")  # ãƒ©ãƒ†ãƒ³è¨˜å·åˆ—æŒ™æ¤œçŸ¥

# åè©ç¾…åˆ—ã®ç°¡æ˜“æ¤œçŸ¥ï¼ˆé›‘ã ãŒå®Ÿç”¨é‡è¦–ï¼‰
JAGGED_LISTY_RE  = re.compile(r"(?:[^\u3000-\u303F\u3040-\u30FF\u4E00-\u9FFF]{2,}|ãƒ»|ï¼|/|,){3,}")

# =========================
# ç’°å¢ƒ & ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
# =========================
def init_env_and_client():
    load_dotenv(override=True)
    api_key = os.getenv("OPENAI_API_KEY", "").strip()
    if not api_key:
        raise SystemExit("OPENAI_API_KEY ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚.env ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    # .env æ¨å¥¨: OPENAI_MODEL="gpt-5" / OPENAI_MODE="chat" ã ãŒã€å®Ÿè¡Œæ™‚ã¯å›ºå®šã§å®‰å®šåŒ–
    model = os.getenv("OPENAI_MODEL", "gpt-5").strip() or "gpt-5"
    client = OpenAI(api_key=api_key)
    return client, model

# =========================
# å…¥åŠ›ï¼ˆå•†å“åï¼‰
# =========================
def load_products_from_csv(path: str):
    if not os.path.exists(path):
        raise SystemExit(f"å…¥åŠ›CSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path}")
    products = []
    with open(path, "r", encoding="utf-8", newline="") as f:
        reader = csv.DictReader(f)
        if "å•†å“å" not in reader.fieldnames:
            raise SystemExit("å…¥åŠ›CSVã«ã€å•†å“åã€ãƒ˜ãƒƒãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        for r in reader:
            nm = (r.get("å•†å“å") or "").strip()
            if nm:
                products.append(nm)
    # é‡è¤‡é™¤å»ï¼ˆé †åºç¶­æŒï¼‰
    seen, uniq = set(), []
    for nm in products:
        if nm not in seen:
            uniq.append(nm)
            seen.add(nm)
    return uniq

# =========================
# çŸ¥è¦‹ï¼ˆã‚†ã‚‹çµåˆã§è¦ç´„ï¼‰
# =========================
def safe_load_json(path):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None

def summarize_knowledge_lite():
    """
    ./output/semantics/*.json ã‚’ã‚†ã‚‹ãé›†ç´„ â†’ ãƒ†ã‚­ã‚¹ãƒˆåŒ–
      - ç”¨èªç¾¤ï¼ˆclusters, vocabulary ç­‰ï¼‰
      - æ§‹é€ ï¼ˆscenes/targets/use_cases ç­‰ï¼‰
      - ãƒ†ãƒ³ãƒ—ãƒ¬/éª¨å­ï¼ˆhints/templatesï¼‰
      - ãƒˆãƒ¼ãƒ³ï¼ˆtoneï¼‰
      - ç¦å‰‡ï¼ˆforbidden_wordsï¼‰
    * å½¢å¼ã¯æ§˜ã€…ã§ã‚‚ã€Œæ‹¾ãˆã‚‹ã ã‘æ‹¾ã†ã€å§¿å‹¢ã§å …ç‰¢ã«
    """
    clusters, market, semantics, templates, tones, forbid_local = [], [], [], [], [], []
    if os.path.isdir(SEMANTICS_DIR):
        for p in glob.glob(os.path.join(SEMANTICS_DIR, "*.json")):
            data = safe_load_json(p)
            if data is None:
                continue
            try:
                if isinstance(data, list):
                    # list å‹ã¯èªå½™ã¨è¦‹ãªã—ã¦å›å
                    for v in data:
                        if isinstance(v, dict):
                            if "terms" in v and isinstance(v["terms"], list):
                                clusters.extend([t for t in v["terms"] if isinstance(t, str)])
                            if "vocabulary" in v and isinstance(v["vocabulary"], str):
                                market.append(v["vocabulary"])
                        elif isinstance(v, str):
                            clusters.append(v)
                elif isinstance(data, dict):
                    # clusters
                    arr = data.get("clusters") or data.get("lexical") or []
                    if isinstance(arr, list):
                        for c in arr:
                            if isinstance(c, dict) and isinstance(c.get("terms"), list):
                                clusters.extend([t for t in c["terms"] if isinstance(t, str)])
                    # market vocab
                    mv = data.get("market_vocab") or data.get("market") or []
                    if isinstance(mv, list):
                        for x in mv:
                            if isinstance(x, dict) and isinstance(x.get("vocabulary"), str):
                                market.append(x["vocabulary"])
                            elif isinstance(x, str):
                                market.append(x)
                    elif isinstance(mv, dict):
                        vv = mv.get("vocabulary") or mv.get("vocab") or []
                        if isinstance(vv, list):
                            market.extend([x for x in vv if isinstance(x, str)])
                    # semantics
                    for k in ["concepts", "scenes", "targets", "use_cases"]:
                        arr2 = data.get(k) or []
                        if isinstance(arr2, list):
                            semantics.extend([x for x in arr2 if isinstance(x, str)])
                    # templates
                    for k in ["hints", "templates"]:
                        arr3 = data.get(k) or []
                        if isinstance(arr3, list):
                            templates.extend([x for x in arr3 if isinstance(x, str)])
                    # tones
                    tone = data.get("tone") or {}
                    if isinstance(tone, dict):
                        for v in tone.values():
                            if isinstance(v, str):
                                tones.append(v)
                    # forbidden
                    fw = data.get("forbidden_words") or []
                    if isinstance(fw, list):
                        forbid_local.extend([w for w in fw if isinstance(w, str)])
            except Exception:
                # å½¢å¼ãƒãƒ©ã¤ãã¯æ¡ã‚Šã¤ã¶ã—ã¦ç¶™ç¶š
                pass

    # ç¦å‰‡ãƒãƒ¼ã‚¸
    forbidden_all = list(dict.fromkeys(FORBIDDEN_BASE + forbid_local))

    def cap_join(xs, n):
        xs = [x for x in xs if isinstance(x, str) and x.strip()]
        return "ã€".join(list(dict.fromkeys(xs))[:n])

    cluster_txt = cap_join(clusters, 12)
    market_txt  = cap_join(market,   12)
    sem_txt     = cap_join(semantics, 8)
    tmpl_txt    = cap_join(templates, 3)
    tone_txt    = cap_join(tones,     4)

    kb = "çŸ¥è¦‹: "
    parts = []
    if cluster_txt:
        parts.append(f"èªå½™:{cluster_txt}")
    if market_txt:
        parts.append(f"å¸‚å ´èª:{market_txt}")
    if sem_txt:
        parts.append(f"æ§‹é€ :{sem_txt}")
    if tmpl_txt:
        parts.append(f"éª¨å­:{tmpl_txt}")
    if tone_txt:
        parts.append(f"ãƒˆãƒ¼ãƒ³:{tone_txt}")
    kb += " / ".join(parts) + ("ã€‚" if parts else "")
    kb += "ç”»åƒæå†™èªã‚„è²©ä¿ƒãƒ¡ã‚¿ã¯ä½¿ã‚ãšã€è‡ªç„¶ãªæ—¥æœ¬èªã®1ã€œ2æ–‡ã§ã€‚"
    return kb, forbidden_all

# =========================
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
# =========================
SYSTEM_PROMPT = (
    "ã‚ãªãŸã¯æ¥½å¤©å¸‚å ´ã®SEOã«æœ€é©åŒ–ã•ã‚ŒãŸç”»åƒALTãƒ†ã‚­ã‚¹ãƒˆã‚’æ›¸ããƒ—ãƒ­ã®æ—¥æœ¬èªã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚"
    "å„ALTæ–‡ã¯è‡ªç„¶ãªæ—¥æœ¬èªã®1ã€œ2æ–‡ã§æ§‹æˆã—ã¾ã™ã€‚åè©ã®ç¾…åˆ—ã¯ç¦æ­¢ã€‚"
    "åŸºæœ¬ã¯ã€Œã€œã§ã™ã€ã€Œã€œã™ã‚‹ã€ãªã©ç”¨è¨€çµ‚æ­¢ã§ã™ãŒã€ä½“è¨€æ­¢ã‚ï¼ˆåè©ã§çµ‚ãˆã‚‹ï¼‰ã‚‚è‡ªç„¶ãªã‚‰è¨±å¯ã€‚"
    "å¥ç‚¹ã€Œã€‚ã€ã§å¿…ãšçµ‚ãˆã‚‹ã€‚èª­ç‚¹ã€Œã€ã€ã¯1è¡Œã«ã¤ãæœ€å¤§2å›ã¾ã§ã€‚"
    "ç”»åƒã‚„å†™çœŸã®æå†™èªã€åº—èˆ—ãƒ¡ã‚¿èªã€ç«¶åˆå„ªä½ãƒ¡ã‚¿ã€ã‚¯ãƒªãƒƒã‚¯èª˜å°ã¯ä¸€åˆ‡ä½¿ã‚ãªã„ã€‚"
    "å¯¾å¿œæ©Ÿç¨®ãƒ»ã‚¹ãƒšãƒƒã‚¯ãƒ»æ©Ÿèƒ½ãƒ»ç”¨é€”ãƒ»å¯¾è±¡ãƒ»ä¾¿ç›Šã‚’è‡ªç„¶ã«ç¹”ã‚Šè¾¼ã‚€ã€‚"
    "å‡ºåŠ›ã¯ALTæ–‡20è¡Œã®ã¿ï¼ˆJSON/ç•ªå·/è¨˜å·ãªã—ï¼‰ã€‚"
)

def build_user_prompt(product: str, knowledge_text: str, forbidden_words):
    forbid_txt = "ã€".join(sorted(set([w for w in forbidden_words if isinstance(w, str)])))
    hint = (
        "æ§‹æˆãƒ’ãƒ³ãƒˆï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ã§ã¯ãªãè‡ªç„¶æ–‡ã§ï¼‰:"
        "ã€ã‚¹ãƒšãƒƒã‚¯â†’å¼·ã¿â†’èª°ã«â†’ã©ã‚“ãªã‚·ãƒ¼ãƒ³â†’ãƒ™ãƒãƒ•ã‚£ãƒƒãƒˆã€ã‚’1ã€œ2æ–‡ã§è‡ªç„¶ã«ã¤ãªãã€‚"
        "åè©åˆ—æŒ™ã¯ç¦æ­¢ã€‚åŠ©è©ã§ã¤ãªãã€ä½“è¨€æ­¢ã‚ã‚‚è‡ªç„¶ãªã‚‰OKã€‚"
    )
    return (
        f"{KANNAME_BANNER}\n"
        f"å•†å“å: {product}\n"
        f"{knowledge_text}\n"
        f"{hint}\n"
        f"ç¦æ­¢èª: {forbid_txt}\n"
        "å‡ºåŠ›: ALTæ–‡ã‚’20è¡Œã€‚å„è¡Œã¯è‡ªç„¶ãª1ã€œ2æ–‡ã€å¥ç‚¹ã€Œã€‚ã€ã§çµ‚ãˆã‚‹ã€‚JSONã‚„ç•ªå·ã¯ä¸è¦ã€‚"
    )

# =========================
# OpenAI å‘¼ã³å‡ºã—
# =========================
def call_openai_20_lines(client, model, product, knowledge_text, forbidden_words, retry=3, wait=6):
    user_prompt = build_user_prompt(product, knowledge_text, forbidden_words)
    last_err = None
    for _ in range(retry):
        try:
            res = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt},
                ],
                response_format={"type": "text"},
                max_completion_tokens=1000,  # å®‰å®šå›ºå®š
                temperature=1,               # å®‰å®šå›ºå®š
            )
            content = (res.choices[0].message.content or "").strip()
            if not content:
                raise RuntimeError("Empty content")
            lines = [LEADING_ENUM_RE.sub("", ln).strip("ãƒ»-â€”â—ã€€").strip()
                     for ln in content.split("\n") if ln.strip()]
            # é•·æ–‡è¡Œã‚„ä½™åˆ†ãªèª¬æ˜ãŒæ··ã–ã‚‹ã“ã¨ãŒã‚ã‚‹ã®ã§ä¸Šé™60ã¾ã§ä¿æŒï¼ˆå¾Œã§20æŠ½å‡ºï¼‰
            return [ln for ln in lines if ln][:60]
        except Exception as e:
            last_err = e
            time.sleep(wait)
    raise RuntimeError(f"OpenAIå¿œç­”å–å¾—å¤±æ•—: {last_err}")

# =========================
# ãƒ­ãƒ¼ã‚«ãƒ«æ•´å½¢ï¼ˆè‡ªç„¶æ–‡ã‚²ãƒ¼ãƒˆï¼‰
# =========================
TAIGEN_OK_RATIO = 0.35  # ä½“è¨€æ­¢ã‚è¨±å®¹ç‡ï¼ˆæœ€çµ‚20æœ¬ã®ã†ã¡ãŠã‚ˆã35%ã¾ã§ï¼‰

def is_taigen_stop(s: str) -> bool:
    # æœ«å°¾ãŒã€Œã§ã™ã€‚ã€ã€Œã¾ã™ã€‚ã€ç­‰ã§ãªã‘ã‚Œã°åè©çµ‚æ­¢ã®å¯èƒ½æ€§ â†’ å¥ç‚¹ã¯å‰æ
    if not s.endswith("ã€‚"):
        return False
    tail = s[:-1].strip()
    # æ˜ç¤ºã®ç”¨è¨€çµ‚æ­¢ã‚’æ’é™¤
    for yogen in ("ã§ã™", "ã¾ã™", "ã§ã—ãŸ", "ã§ã—ãŸã€‚", "ã—ã¾ã™", "ã§ãã¾ã™", "ã¨ãªã‚Šã¾ã™", "ã«ãªã‚Šã¾ã™"):
        if tail.endswith(yogen):
            return False
    # ã€Œã€œå¯¾å¿œã€ã€Œã€œä»•æ§˜ã€ã€Œã€œè¨­è¨ˆã€ã€Œã€œæ§‹é€ ã€ãªã©ã¯ä½“è¨€æ‰±ã„å¯
    return True

def hard_forbid(text: str, forbids):
    t = text
    for ng in forbids:
        if ng and ng in t:
            t = t.replace(ng, "")
    return t

def normalize_sentence_core(s: str):
    t = s.strip()
    t = PARENS_TRIM_RE.sub("", t)
    t = WHITESPACE_RE.sub(" ", t)
    t = MULTI_COMMA_RE.sub("ã€ã€", t)
    t = t.strip("ãƒ»-â€”â—ã€€")
    # å¥ç‚¹ä»˜ä¸
    if not t.endswith("ã€‚"):
        t += "ã€‚"
    return t

def soft_clip_sentence(s: str, forbids):
    """
    ä¸Šé™120å­—ã¾ã§ã‚’ç›®å®‰ã«ã€æœ€å¾Œã®ã€Œã€‚ã€ã§è‡ªç„¶ã‚«ãƒƒãƒˆ â†’ ç¦å‰‡å†é©ç”¨
    """
    t = normalize_sentence_core(s)
    if len(t) > 120:
        cut = t[:120]
        p = cut.rfind("ã€‚")
        if p != -1:
            t = cut[:p+1]
        else:
            t = cut
            if not t.endswith("ã€‚"):
                t += "ã€‚"
    t = hard_forbid(t, forbids)
    return t.strip()

def looks_like_listy(s: str) -> bool:
    # ãƒ©ãƒ†ãƒ³è¨˜å·åˆ—æŒ™ã‚„è¨˜å·ã¾ã¿ã‚Œã®ç¾…åˆ—ã‚’å«Œã†
    if LATIN_LISTY_RE.search(s):
        return True
    if JAGGED_LISTY_RE.search(s):
        return True
    # èª­ç‚¹ãŒ4ã¤ä»¥ä¸Š â†’ ç¾…åˆ—ã£ã½ã„
    if s.count("ã€") >= 4:
        return True
    return False

def naturalize_short(s: str) -> str:
    """
    çŸ­ã™ãæ–‡ï¼ˆã€œ20å­—å°ï¼‰ã¸ã®è»½è£œå®Œã€‚åè©â†’ç”¨è¨€/ä½“è¨€ã«è¿‘ã¥ã‘ã‚‹ã€‚
    """
    t = s.strip("ã€‚").strip()
    if not t:
        return ""
    # ã”ãè»½ã„è£œåŠ©å¥
    addons = [
        "ã®è¨­è¨ˆã§ã™", "ã«å¯¾å¿œã—ã¾ã™", "ãŒé­…åŠ›", "ã‚’å®Ÿç¾", "ã‚’ã‚µãƒãƒ¼ãƒˆ", "ã«æœ€é©", "ã§å®‰å¿ƒ"
    ]
    t2 = t
    if not t2.endswith(("ã§ã™", "ã¾ã™", "æœ€é©", "é­…åŠ›", "è¨­è¨ˆ", "ä»•æ§˜", "å¯¾å¿œ")):
        t2 = t2 + random.choice(addons)
    return t2 + "ã€‚"

def refine_20_lines(raw_lines, forbids):
    """
    1) æ­£è¦åŒ–ãƒ»ç¦å‰‡ãƒ»åè©ç¾…åˆ—/çŸ­æ–‡é™¤å»
    2) 120å­—ã¾ã§ã§è‡ªç„¶ã‚«ãƒƒãƒˆ â†’ æœ€çµ‚80ã€œ110å­—ãƒ¬ãƒ³ã‚¸ç‹™ã„
    3) é¡ä¼¼/é‡è¤‡é™¤å»
    4) 20æœ¬æˆå½¢ï¼ˆä½“è¨€æ­¢ã‚æ¯”ç‡ã‚’35%ç¨‹åº¦ã«ï¼‰
    """
    norm = []
    for ln in raw_lines:
        if not ln:
            continue
        ln = LEADING_ENUM_RE.sub("", ln).strip("ãƒ»-â€”â—ã€€").strip()
        ln = normalize_sentence_core(ln)

        if looks_like_listy(ln):
            # åè©ç¾…åˆ—ãã•ã„ â†’ è»½è£œæ­£
            ln = ln.replace("ãƒ»", "ã€").replace("/", "ã€").replace("ï¼", "ã€")
            ln = re.sub(r"\s*[,ã€]\s*", "ã€", ln)
            ln = re.sub(r"(ã€){3,}", "ã€ã€", ln)

        # çŸ­ã™ãã‚‹ã¨ãè»½è£œå®Œ
        if len(ln) < 25:
            ln = naturalize_short(ln)

        # 120å­—ã«æŸ”ã‚‰ã‹ãåˆ‡ã£ã¦ç¦å‰‡å†é©ç”¨
        ln = soft_clip_sentence(ln, forbids)

        # æœ€çµ‚: æ¥µç«¯çŸ­æ–‡ã®æ¨ã¦
        if len(ln) < 15:
            continue

        norm.append(ln)

    # é‡è¤‡é™¤å»
    uniq, seen = [], set()
    for ln in norm:
        if ln not in seen:
            uniq.append(ln); seen.add(ln)

    # ä½“è¨€æ­¢ã‚æ¯”ç‡ã‚’ã–ã£ãã‚Šåˆ¶å¾¡ï¼ˆå¤šã™ãã‚‹å ´åˆã¯ä¸€éƒ¨ã‚’ç”¨è¨€åŒ–ï¼‰
    taigen = [i for i, s in enumerate(uniq) if is_taigen_stop(s)]
    limit = max(0, int(len(uniq) * TAIGEN_OK_RATIO))
    if len(taigen) > limit:
        over = taigen[limit:]
        for i in over:
            s = uniq[i]
            s = s[:-1] + "ã§ã™ã€‚"
            uniq[i] = s

    # 80ã€œ110å­—å¸¯ã¸å¯„ã›ã‚‹ï¼ˆé•·ã™ãã¯æœ«å°¾å¥ç‚¹ã¾ã§è©°ã‚ã€çŸ­ã™ãã¯è»½è£œå®Œï¼‰
    refined = []
    for s in uniq:
        t = s
        if len(t) > FINAL_MAX:
            cut = t[:FINAL_MAX]
            p = cut.rfind("ã€‚")
            if p != -1 and p >= FINAL_MIN:
                t = cut[:p+1]
        if len(t) < FINAL_MIN:
            t = naturalize_short(t)
        # èª­ç‚¹å¯†åº¦ï¼ˆæœ€å¤§2å›ï¼‰ã‚’è¶…ãˆã‚‹å ´åˆã€ä¸è¦ãªèª­ç‚¹ã‚’1ã¤è½ã¨ã™
        while t.count("ã€") > 2:
            t = t.replace("ã€", "", 1)
        refined.append(t)

    # 20æœ¬ã«æ•´å½¢ï¼ˆè¶³ã‚Šãªã„æ™‚ã¯è»½ã„è¨€ã„æ›ãˆã§è£œå®Œï¼‰
    def light_variation(s: str) -> str:
        v = s
        # è»½ã„èªå°¾ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
        repls = [("ã—ã¾ã™ã€‚", "ã§ãã¾ã™ã€‚"),
                 ("ã§ãã¾ã™ã€‚", "ã—ã‚„ã™ã„ã§ã™ã€‚"),
                 ("ã§ã™ã€‚", "ã«ãªã‚Šã¾ã™ã€‚")]
        for a, b in repls:
            if v.endswith(a):
                v = v[:-len(a)] + b
                break
        if v == s:
            # å…ˆé ­ã®ä¸€å˜èªã®å¾Œã‚ã«èª­ç‚¹ï¼ˆé‡è¤‡ã¯æŠ‘åˆ¶ï¼‰
            v = re.sub(r"^(\S{2,5})", r"\1ã€", v, count=1)
            v = v.replace("ã€ã€", "ã€")
            if not v.endswith("ã€‚"):
                v += "ã€‚"
        return soft_clip_sentence(v, forbids)

    i = 0
    while len(refined) < 20 and refined:
        refined.append(light_variation(refined[i % len(refined)]))
        i += 1

    return refined[:20]

# =========================
# æ›¸ãå‡ºã—
# =========================
def ensure_outdir():
    os.makedirs(OUT_DIR, exist_ok=True)

def write_raw(products, all_raw):
    with open(RAW_PATH, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["å•†å“å"] + [f"ALT_raw_{i+1}" for i in range(20)])
        for p, lines in zip(products, all_raw):
            row = [p] + (lines[:20] + [""] * max(0, 20 - len(lines)))
            w.writerow(row)

def write_refined(products, all_refined):
    with open(REF_PATH, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["å•†å“å"] + [f"ALT_{i+1}" for i in range(20)])
        for p, lines in zip(products, all_refined):
            row = [p] + (lines[:20] + [""] * max(0, 20 - len(lines)))
            w.writerow(row)

def write_diff(products, all_raw, all_refined):
    with open(DIFF_PATH, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        header = ["å•†å“å"] + [f"ALT_raw_{i+1}" for i in range(20)] + [f"ALT_refined_{i+1}" for i in range(20)]
        w.writerow(header)
        for p, r, ref in zip(products, all_raw, all_refined):
            r_line   = (r[:20]   + [""] * max(0, 20 - len(r)))
            ref_line = (ref[:20] + [""] * max(0, 20 - len(ref)))
            w.writerow([p] + r_line + ref_line)

# =========================
# ãƒ¡ã‚¤ãƒ³
# =========================
def main():
    print("ğŸŒ¸ ALTé•·æ–‡ v4.3ï¼ˆè‡ªç„¶æ–‡ï¼‹ä½“è¨€æ­¢ã‚è¨±å®¹ï¼‹çŸ¥è¦‹èåˆï¼‹raw/refined/diffï¼‰")
    print(KANNAME_BANNER)
    client, model = init_env_and_client()
    ensure_outdir()

    products = load_products_from_csv(INPUT_CSV)
    print(f"âœ… å¯¾è±¡å•†å“: {len(products)}ä»¶")

    knowledge_text, forbidden_all = summarize_knowledge_lite()

    all_raw, all_refined = [], []
    for p in tqdm(products, desc="ğŸ§  AIç”Ÿæˆä¸­", total=len(products)):
        # 1) AIç”Ÿæˆ
        try:
            raw_lines = call_openai_20_lines(client, model, p, knowledge_text, forbidden_all)
        except Exception:
            # ãƒ€ãƒŸãƒ¼ï¼ˆç©ºã¯é¿ã‘ã‚‹ï¼‰
            raw_lines = [f"{p} ã®ä½¿ã„å‹æ‰‹ã‚’é«˜ã‚ã€æ—¥å¸¸ã®å°ã•ãªä¸ä¾¿ã‚’è§£æ¶ˆã™ã‚‹è¨­è¨ˆã§ã™ã€‚"] * 20

        # 2) ãƒ­ãƒ¼ã‚«ãƒ«æ•´å½¢
        refined_lines = refine_20_lines(raw_lines, forbidden_all)

        all_raw.append(raw_lines[:20])
        all_refined.append(refined_lines)

        time.sleep(0.2)  # ã‚¹ãƒ­ãƒƒãƒˆãƒªãƒ³ã‚°

    # 3) æ›¸ãå‡ºã—
    write_raw(products, all_raw)
    write_refined(products, all_refined)
    write_diff(products, all_raw, all_refined)

    # 4) æ¤œè¨¼ãƒ­ã‚°
    def avg_len(blocks):
        lens = [len(x) for lines in blocks for x in lines if x]
        return (sum(lens) / max(1, len(lens)))

    # è‡ªç„¶æ–‡ç‡ï¼ˆå¥ç‚¹çµ‚æ­¢ã‹ã¤ç¾…åˆ—ã£ã½ããªã„ï¼‰
    def natural_rate(blocks):
        total = 0
        ok = 0
        for lines in blocks:
            for s in lines:
                if not s:
                    continue
                total += 1
                if s.endswith("ã€‚") and not looks_like_listy(s):
                    ok += 1
        return (ok / total * 100) if total else 0.0

    # ä½“è¨€æ­¢ã‚ç‡
    def taigen_rate(blocks):
        total = 0
        tg = 0
        for lines in blocks:
            for s in lines:
                if not s:
                    continue
                total += 1
                if is_taigen_stop(s):
                    tg += 1
        return (tg / total * 100) if total else 0.0

    print("âœ… å‡ºåŠ›å®Œäº†:")
    print(f"   - AIç”Ÿå‡ºåŠ› : {RAW_PATH}")
    print(f"   - æ•´å½¢å¾Œ    : {REF_PATH}")
    print(f"   - å·®åˆ†æ¯”è¼ƒ  : {DIFF_PATH}")
    print(f"ğŸ“ æ–‡å­—æ•°(å¹³å‡): raw={avg_len(all_raw):.1f} / refined={avg_len(all_refined):.1f}")
    print(f"ğŸ’¬ è‡ªç„¶æ–‡ç‡   : {natural_rate(all_refined):.1f}%")
    print(f"â— ä½“è¨€æ­¢ã‚ç‡ : {taigen_rate(all_refined):.1f}%ï¼ˆç›®æ¨™ ~{int(TAIGEN_OK_RATIO*100)}%ï¼‰")

if __name__ == "__main__":
    main()
import atlas_autosave_core
