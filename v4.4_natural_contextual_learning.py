# -*- coding: utf-8 -*-
"""
ALTé•·æ–‡ï¼ˆè‡ªç„¶æ–‡å­¦ç¿’ï¼‹çŸ¥è¦‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼‰ v4.4
- å…¥åŠ›: ./rakuten.csv ï¼ˆUTF-8, ãƒ˜ãƒƒãƒ€ã«ã€Œå•†å“åã€ï¼‰
- å‡ºåŠ›:
  1) output/ai_writer/alt_text_ai_raw_longform_v4.4.csv
  2) output/ai_writer/alt_text_refined_final_longform_v4.4.csv
  3) output/ai_writer/alt_text_diff_longform_v4.4.csv
- çŸ¥è¦‹: ./output/semantics é…ä¸‹ã®JSONç¾¤ã‚’â€œèƒŒæ™¯çŸ¥è­˜â€ã¨ã—ã¦ assistant ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§æ³¨å…¥
- OpenAI: .env ã§æŒ‡å®š
    OPENAI_API_KEY=...
    OPENAI_MODEL=gpt-5.1 (ä¾‹) â€»æœªæŒ‡å®šæ™‚ã¯ gpt-4o
    OPENAI_TEMPERATURE=1.0
    OPENAI_MAX_TOKENS=1000
"""

import os
import re
import csv
import glob
import json
import time
from collections import defaultdict

from dotenv import load_dotenv

try:
    from tqdm.auto import tqdm
except Exception:
    def tqdm(x, **k): return x

try:
    from openai import OpenAI
except Exception:
    raise SystemExit("openai SDK ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚`pip install openai python-dotenv` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

# ====== è¨­å®š ======
INPUT_CSV = "./rakuten.csv"
OUT_DIR   = "./output/ai_writer"
RAW_PATH  = os.path.join(OUT_DIR, "alt_text_ai_raw_longform_v4.4.csv")
REF_PATH  = os.path.join(OUT_DIR, "alt_text_refined_final_longform_v4.4.csv")
DIFF_PATH = os.path.join(OUT_DIR, "alt_text_diff_longform_v4.4.csv")

SEMANTICS_DIR = "./output/semantics"

# ç¦å‰‡èªï¼ˆç”»åƒæå†™èªãƒ»ãƒ¡ã‚¿èªï¼‰
FORBIDDEN = [
    "ç”»åƒ", "å†™çœŸ", "è¦‹ãŸç›®", "ä¸Šã®ç”»åƒ", "ä¸‹ã®å†™çœŸ",
    "å½“åº—", "å½“ç¤¾", "ãƒ¬ãƒ“ãƒ¥ãƒ¼", "ãƒ©ãƒ³ã‚­ãƒ³ã‚°", "ã‚¯ãƒªãƒƒã‚¯", "ã“ã¡ã‚‰",
    "ç«¶åˆ", "å„ªä½æ€§", "æ¥­ç•Œæœ€é«˜", "æœ€å®‰", "No.1", "ãƒŠãƒ³ãƒãƒ¼ãƒ¯ãƒ³", "å£²ä¸ŠNo1",
    "ãƒªãƒ³ã‚¯", "ãƒšãƒ¼ã‚¸", "ã‚«ãƒ¼ãƒˆ", "è³¼å…¥ã¯ã“ã¡ã‚‰", "é€æ–™ç„¡æ–™ï¼ˆç¢ºç´„ï¼‰", "è¿”é‡‘ä¿è¨¼",
]

# æ–‡å­—æ•°æ–¹é‡
RAW_MIN, RAW_MAX     = 100, 130
FINAL_MIN, FINAL_MAX =  80, 110

# æ­£è¦è¡¨ç¾
LEADING_ENUM_RE = re.compile(r"^\s*[\dâ‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©\-\*\ãƒ»\u2022]\s*[\.ï¼ã€]?\s*")
MULTI_COMMA_RE  = re.compile(r"ã€{3,}")
WS_RE           = re.compile(r"\s+")

# ====== ç’°å¢ƒã¨ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ ======
def init_env_and_client():
    load_dotenv(override=True)
    api_key = os.getenv("OPENAI_API_KEY", "").strip()
    if not api_key:
        raise SystemExit("OPENAI_API_KEY ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚.env ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    model = os.getenv("OPENAI_MODEL", "").strip() or "gpt-4o"
    try:
        temperature = float(os.getenv("OPENAI_TEMPERATURE", "1").strip())
    except:
        temperature = 1.0
    try:
        max_tokens = int(os.getenv("OPENAI_MAX_TOKENS", "1000").strip())
    except:
        max_tokens = 1000

    client = OpenAI(api_key=api_key)
    return client, model, temperature, max_tokens

# ====== å…¥åŠ›ï¼ˆå•†å“åï¼‰ ======
def load_products(path: str):
    if not os.path.exists(path):
        raise SystemExit(f"å…¥åŠ›CSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path}")
    items = []
    with open(path, "r", encoding="utf-8", newline="") as f:
        r = csv.DictReader(f)
        if "å•†å“å" not in r.fieldnames:
            raise SystemExit("å…¥åŠ›CSVã«ã€å•†å“åã€ãƒ˜ãƒƒãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        for row in r:
            nm = (row.get("å•†å“å") or "").strip()
            if nm: items.append(nm)
    # é‡è¤‡é™¤å»ï¼ˆé †åºç¶­æŒï¼‰
    seen, uniq = set(), []
    for nm in items:
        if nm not in seen:
            uniq.append(nm); seen.add(nm)
    return uniq

# ====== çŸ¥è¦‹ èª­ã¿è¾¼ã¿ãƒ»è¦ç´„ ======
def safe_load_json(path):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None

def summarize_knowledge_structured():
    """
    ./output/semantics å†…ã®JSONã‚’ã‚†ã‚‹ãé›†ç´„ã—ã€æ§‹é€ åŒ–ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã¨ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„ã‚’è¿”ã™
    """
    payload = {
        "clusters": [],      # ç”¨èªã‚¯ãƒ©ã‚¹ã‚¿
        "market_vocab": [],  # å¸‚å ´ç³»èªå½™
        "concepts": [],      # æ¦‚å¿µ/ç”¨é€”/å¯¾è±¡/ã‚·ãƒ¼ãƒ³
        "templates": [],     # è¡¨ç¾éª¨å­
        "tone": {},          # ãƒˆãƒ¼ãƒ³
        "forbidden_local": []
    }
    if not os.path.isdir(SEMANTICS_DIR):
        text = "ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»ç”¨é€”ãƒ»å¯¾è±¡ãƒ»ã‚¹ãƒšãƒƒã‚¯ãƒ»é–¢é€£æ©Ÿç¨®åã‚’è‡ªç„¶ã«å«ã‚ã‚‹ã€‚ç”»åƒæå†™èªã‚„ECãƒ¡ã‚¿èªã¯ä½¿ã‚ãªã„ã€‚"
        return payload, text, FORBIDDEN[:]

    files = glob.glob(os.path.join(SEMANTICS_DIR, "*.json"))
    for p in files:
        name = os.path.basename(p).lower()
        data = safe_load_json(p)
        if data is None: 
            continue

        # é…åˆ—/è¾æ›¸ æ··åœ¨ã‚’å¸å
        def listify(x):
            return x if isinstance(x, list) else ([x] if x else [])

        if "lexical" in name or "cluster" in name:
            # ä¾‹: {"clusters":[{"terms":[...]}]} / [{"terms":[...]}] / ["MagSafe", ...]
            for item in listify(data.get("clusters") if isinstance(data, dict) else data):
                if isinstance(item, dict) and isinstance(item.get("terms"), list):
                    payload["clusters"].extend([t for t in item["terms"] if isinstance(t, str)])
                elif isinstance(item, str):
                    payload["clusters"].append(item)

        elif "market" in name and "vocab" in name or "market_vocab" in name:
            # ä¾‹: [{"vocabulary":"MagSafe"}, "PD", ...] / {"vocabulary":[...]}
            if isinstance(data, dict) and isinstance(data.get("vocabulary"), list):
                payload["market_vocab"].extend([x for x in data["vocabulary"] if isinstance(x, str)])
            elif isinstance(data, list):
                for v in data:
                    if isinstance(v, dict) and isinstance(v.get("vocabulary"), str):
                        payload["market_vocab"].append(v["vocabulary"])
                    elif isinstance(v, str):
                        payload["market_vocab"].append(v)

        elif "structured_semantics" in name or "semantic" in name:
            # ä¾‹: {"concepts":[...], "targets":[...], "use_cases":[...], "scenes":[...]}
            if isinstance(data, dict):
                for k in ["concepts", "targets", "use_cases", "scenes"]:
                    payload["concepts"] += [x for x in (data.get(k) or []) if isinstance(x, str)]
            elif isinstance(data, list):
                payload["concepts"] += [x for x in data if isinstance(x, str)]

        elif "template_composer" in name or "template" in name:
            # ä¾‹: {"hints":[...]} / {"templates":[...]} / [...]
            if isinstance(data, dict):
                payload["templates"] += [x for x in (data.get("hints") or []) if isinstance(x, str)]
                payload["templates"] += [x for x in (data.get("templates") or []) if isinstance(x, str)]
            elif isinstance(data, list):
                payload["templates"] += [x for x in data if isinstance(x, str)]

        elif "persona" in name or "styled_persona" in name or "tone" in name:
            # ä¾‹: {"tone":{"style":"ã€œ","register":"ã€œ"}}
            if isinstance(data, dict):
                t = data.get("tone") or {}
                if isinstance(t, dict):
                    for k, v in t.items():
                        if isinstance(v, str):
                            payload["tone"][k] = v

        elif "normalized" in name or "forbid" in name:
            # ä¾‹: {"forbidden_words":[...]}
            if isinstance(data, dict):
                payload["forbidden_local"] += [w for w in (data.get("forbidden_words") or []) if isinstance(w, str)]
            elif isinstance(data, list):
                payload["forbidden_local"] += [w for w in data if isinstance(w, str)]

    # ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„
    def cap(xs, n):
        xs = [x for x in xs if isinstance(x, str)]
        return "ã€".join(list(dict.fromkeys(xs))[:n])

    parts = []
    c = cap(payload["clusters"], 12)
    m = cap(payload["market_vocab"], 12)
    z = cap(payload["concepts"], 8)
    t = cap(payload["templates"], 3)
    if c: parts.append(f"èªå½™: {c}")
    if m: parts.append(f"å¸‚å ´èª: {m}")
    if z: parts.append(f"æ§‹é€ : {z}")
    if t: parts.append(f"éª¨å­: {t}")
    text = " / ".join(parts) + ("ã€‚" if parts else "")
    text += "è‡ªç„¶ã§èª­ã¿ã‚„ã™ã„æ—¥æœ¬èªã§ã€éå‰°ãªè©°ã‚è¾¼ã¿ã¯é¿ã‘ã‚‹ã€‚"
    forbid_all = list({*FORBIDDEN, *payload["forbidden_local"]})
    return payload, text, forbid_all

# ====== ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ======
SYSTEM_PROMPT = (
    "ã‚ãªãŸã¯ECç”»åƒã®ALTãƒ†ã‚­ã‚¹ãƒˆã‚’æ›¸ãæ—¥æœ¬èªã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚"
    "ç›®çš„ã¯ã€æ¥½å¤©ã®ã‚µã‚¤ãƒˆå†…SEOã«å¼·ã„ã€è‡ªç„¶ãªæ—¥æœ¬èªã®ALTã‚’20æœ¬ã§ã™ã€‚"
    "å¿…é ˆãƒ«ãƒ¼ãƒ«ï¼š\n"
    f"ãƒ»å„ALTã¯å…¨è§’{RAW_MIN}ã€œ{RAW_MAX}å­—ã‚’ç›®å®‰ã«1ã€œ2æ–‡ã€‚å¿…ãšå¥ç‚¹ã€Œã€‚ã€ã§çµ‚ãˆã‚‹ã€‚\n"
    "ãƒ»ç”»åƒã‚„å†™çœŸã®æå†™èªï¼ˆä¾‹ï¼šç”»åƒã€å†™çœŸã€è¦‹ãŸç›®ï¼‰ã‚„ECãƒ¡ã‚¿èªï¼ˆå½“åº—ã€ãƒ¬ãƒ“ãƒ¥ãƒ¼ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°ç­‰ï¼‰ã¯ä½¿ã‚ãªã„ã€‚\n"
    "ãƒ»ç«¶åˆæ¯”è¼ƒã‚„ã€Œç«¶åˆå„ªä½æ€§ã€ã®ã‚ˆã†ãªãƒ¡ã‚¿è¡¨ç¾ã¯ç¦æ­¢ã€‚\n"
    "ãƒ»å¯¾å¿œæ©Ÿç¨®ãƒ»ã‚¹ãƒšãƒƒã‚¯ãƒ»æ©Ÿèƒ½ãƒ»ç”¨é€”ãƒ»å¯¾è±¡ãƒ»ä¾¿ç›Šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯ã€ä¸è‡ªç„¶ã«è©°ã‚è¾¼ã¾ãšâ€œè‡ªç„¶ã«â€å«ã‚ã‚‹ã€‚\n"
    "ãƒ»ç®‡æ¡æ›¸ããƒ»ç•ªå·ãƒ»ãƒ©ãƒ™ãƒ«ï¼ˆALT: ç­‰ï¼‰ã¯ä»˜ã‘ãªã„ã€‚\n"
    "ãƒ»å‡ºåŠ›ã¯20è¡Œã®ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã€‚JSONã‚„è¨˜å·ã¯ä¸è¦ã€‚\n"
    "\n"
    "â–¼æ‚ªã„ä¾‹ï¼ˆç¦æ­¢ï¼‰ï¼š\n"
    "ã€Œè€ä¹…æ€§ãƒ»é˜²æ°´ãƒ»è»½é‡ãƒ»ã‚·ãƒ³ãƒ—ãƒ«ãƒ‡ã‚¶ã‚¤ãƒ³ãƒ»ä½¿ã„ã‚„ã™ã„ä»•æ§˜ã€‚ã€ï¼ˆåè©ã®ç¾…åˆ—ï¼‰\n"
    "ã€Œé«˜é€Ÿå……é›»ã€æ€¥é€Ÿå……é›»ã€PDã€USB-Cã€iPhoneã€Androidã€‚ã€ï¼ˆèª­ç‚¹ã§ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸¦ã¹ï¼‰\n"
    "\n"
    "â–¼è‰¯ã„ä¾‹ï¼ˆæ¨¡å€£ï¼‰ï¼š\n"
    "ã€ŒMagSafeå¯¾å¿œã®è–„å‹å……é›»å™¨ã€‚è»½é‡ã‹ã¤å®‰å®šã—ãŸå¸ç€ã§ã€ãƒ‡ã‚¹ã‚¯ã§ã‚‚å°±å¯æ™‚ã§ã‚‚æ‰±ã„ã‚„ã™ã„è¨­è¨ˆã§ã™ã€‚ã€\n"
    "ã€Œ9Hç¡¬åº¦ã®ã‚¬ãƒ©ã‚¹ãƒ•ã‚£ãƒ«ãƒ ã§æ“¦ã‚Šå‚·ã«å¼·ãã€æŒ‡ã™ã¹ã‚Šã‚‚æ»‘ã‚‰ã‹ã€‚è²¼ã‚Šä»˜ã‘è£œåŠ©æ ãŒä»˜å±ã—ã€èª°ã§ã‚‚ç°¡å˜ã«è²¼ã‚Œã¾ã™ã€‚ã€\n"
    "ã€ŒType-C 240Wã‚±ãƒ¼ãƒ–ãƒ«ã€‚ãƒãƒ¼ãƒˆPCã®PDé«˜é€Ÿå……é›»ã«å¯¾å¿œã—ã€è€å±ˆæ›²ãƒ¡ãƒƒã‚·ãƒ¥ã§æŒã¡é‹ã³ã«ã‚‚å®‰å¿ƒã§ã™ã€‚ã€\n"
)

def build_user_prompt(product: str, forbid_words):
    forbid_txt = "ã€".join(sorted(set([w for w in forbid_words if isinstance(w, str)])))
    hint = (
        "è‡ªç„¶æ–‡ãƒ’ãƒ³ãƒˆï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ã§ã¯ãªãè‡ªç„¶ã«ï¼‰ï¼š"
        "å•†å“ã‚¹ãƒšãƒƒã‚¯â†’ã‚³ã‚¢ã‚³ãƒ³ãƒ”ã‚¿ãƒ³ã‚¹â†’ã©ã‚“ãªäººâ†’ã‚·ãƒ¼ãƒ³â†’ãƒ™ãƒãƒ•ã‚£ãƒƒãƒˆã€‚"
    )
    return (
        f"å•†å“å: {product}\n"
        f"{hint}\n"
        f"ç¦æ­¢èªï¼ˆçµ¶å¯¾ã«ä½¿ã‚ãªã„ï¼‰: {forbid_txt}\n"
        "20è¡Œã§ã€å„è¡Œã¯åŠ©è©ã§è‡ªç„¶ã«ã¤ãªã„ã 1ã€œ2æ–‡ã®æ—¥æœ¬èªã«ã—ã¦ãã ã•ã„ã€‚"
    )

# ====== OpenAI å‘¼ã³å‡ºã— ======
def call_openai_20_lines(client, model, temperature, max_tokens,
                         product, knowledge_text, forbid_words, retry=3, wait=6):
    """
    20è¡Œä»¥ä¸Šè¿”ã£ã¦ãã‚‹ã“ã¨ãŒã‚ã‚‹ã®ã§å¾Œæ®µã§æ•´å½¢ã€‚çŸ¥è¦‹ã¯ assistant ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§ä¾›çµ¦ã€‚
    """
    user_prompt = build_user_prompt(product, forbid_words)
    last_err = None
    for _ in range(retry):
        try:
            res = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": "ä»¥ä¸‹ã¯ã“ã®å•†å“ã®èƒŒæ™¯çŸ¥è­˜ã§ã™ã€‚å‚è€ƒã«ã—ã¦ãã ã•ã„ã€‚"},
                    {"role": "assistant", "content": knowledge_text},
                    {"role": "user", "content": user_prompt},
                ],
                response_format={"type": "text"},
                max_completion_tokens=max_tokens,
                temperature=temperature,
            )
            content = (res.choices[0].message.content or "").strip()
            if content:
                # è¡Œåˆ†å‰²
                lines = [ln.strip() for ln in content.split("\n") if ln.strip()]
                # ç®‡æ¡æ›¸ãç•ªå·ã®å‰¥é›¢
                cleaned = []
                for ln in lines:
                    ln2 = LEADING_ENUM_RE.sub("", ln).strip("ãƒ»-â€”â—ã€€")
                    if ln2: cleaned.append(ln2)
                return cleaned[:80]  # å¿µã®ãŸã‚å¤šã‚ã«ä¿æŒ
        except Exception as e:
            last_err = e
            time.sleep(wait)
    raise RuntimeError(f"OpenAIå¿œç­”å¤±æ•—: {last_err}")

# ====== ãƒ­ãƒ¼ã‚«ãƒ«æ•´å½¢ ======
def soft_clip_sentence(text: str, min_len=FINAL_MIN, max_len=FINAL_MAX) -> str:
    t = (text or "").strip()
    if not t:
        return ""
    if not t.endswith("ã€‚"):
        t += "ã€‚"
    t = WS_RE.sub(" ", t)
    t = MULTI_COMMA_RE.sub("ã€ã€", t)
    t = LEADING_ENUM_RE.sub("", t).strip("ãƒ»-â€”â—ã€€")

    # é•·ã™ãã‚‹å ´åˆã¯120ã¾ã§è¨±å®¹ã—ã€æœ€å¾Œã®ã€Œã€‚ã€ã§åˆ‡ã‚‹
    if len(t) > 120:
        cut = t[:120]
        p = cut.rfind("ã€‚")
        if p != -1:
            t = cut[:p+1]
        else:
            t = cut

    # ç¦å‰‡èªã¯å®Œå…¨é™¤å»
    for ng in FORBIDDEN:
        if ng and ng in t:
            t = t.replace(ng, "")
    return t.strip()

def refine_20_lines(raw_lines):
    # æ­£è¦åŒ–
    norm = []
    for ln in raw_lines:
        if not ln: continue
        ln = LEADING_ENUM_RE.sub("", ln).strip("ãƒ»-â€”â—ã€€")
        ln = soft_clip_sentence(ln)
        if len(ln) < 18:  # ç•°å¸¸ã«çŸ­ã„ã‚‚ã®ã¯æ£„å´ï¼ˆå¾Œã§è£œå®Œï¼‰
            continue
        norm.append(ln)

    # é‡è¤‡é™¤å»
    uniq, seen = [], set()
    for ln in norm:
        if ln not in seen:
            uniq.append(ln); seen.add(ln)

    refined = [soft_clip_sentence(ln) for ln in uniq]

    # è¶³ã‚Šãªã„å ´åˆã®è£œå®Œï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬æœ€å°æ–‡ãƒ»è‡ªç„¶æ–‡ï¼‰
    def synth(product):
        core = f"{product} ã®ä½¿ã„å‹æ‰‹ã‚’é«˜ã‚ã‚‹è¨­è¨ˆã§ã€æ—¥å¸¸ã®ä¸ä¾¿ã‚’è»½æ¸›ã—ã¾ã™ã€‚"
        return soft_clip_sentence(core)

    i = 0
    while len(refined) < 20:
        seed = refined[i % len(refined)] if refined else ""
        add  = synth("æœ¬è£½å“") if not seed else seed.replace("ã§ã™ã€‚", "ã«ãªã‚Šã¾ã™ã€‚")
        if not add.endswith("ã€‚"): add += "ã€‚"
        refined.append(soft_clip_sentence(add))
        i += 1

    return refined[:20]

# ====== æ›¸ãå‡ºã— ======
def ensure_outdir():
    os.makedirs(OUT_DIR, exist_ok=True)

def write_raw(products, all_raw):
    with open(RAW_PATH, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["å•†å“å"] + [f"ALT_raw_{i+1}" for i in range(20)])
        for p, lines in zip(products, all_raw):
            row = [p] + (lines[:20] + [""] * max(0, 20 - len(lines)))
            w.writerow(row)

def write_refined(products, all_refined):
    with open(REF_PATH, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["å•†å“å"] + [f"ALT_{i+1}" for i in range(20)])
        for p, lines in zip(products, all_refined):
            row = [p] + (lines[:20] + [""] * max(0, 20 - len(lines)))
            w.writerow(row)

def write_diff(products, all_raw, all_refined):
    with open(DIFF_PATH, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        header = ["å•†å“å"] + [f"ALT_raw_{i+1}" for i in range(20)] + [f"ALT_refined_{i+1}" for i in range(20)]
        w.writerow(header)
        for p, r, ref in zip(products, all_raw, all_refined):
            r_line   = (r[:20]   + [""] * max(0, 20 - len(r)))
            ref_line = (ref[:20] + [""] * max(0, 20 - len(ref)))
            w.writerow([p] + r_line + ref_line)

# ====== ãƒ¡ã‚¤ãƒ³ ======
def main():
    print("ğŸŒ¸ ALTé•·æ–‡ç”Ÿæˆ v4.4ï¼ˆè‡ªç„¶æ–‡å­¦ç¿’ï¼‹çŸ¥è¦‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼‰")
    client, model, temperature, max_tokens = init_env_and_client()
    ensure_outdir()

    products = load_products(INPUT_CSV)
    print(f"âœ… å¯¾è±¡å•†å“: {len(products)}ä»¶")

    kb_payload, kb_text, forbidden_all = summarize_knowledge_structured()

    all_raw, all_refined = [], []
    for p in tqdm(products, desc="ğŸ§  AIç”Ÿæˆ", total=len(products)):
        try:
            raw_lines = call_openai_20_lines(
                client, model, temperature, max_tokens,
                p, kb_text, forbidden_all
            )
        except Exception:
            # å®Œå…¨å¤±æ•—æ™‚ã®ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ï¼ˆ20æœ¬ãƒ€ãƒŸãƒ¼ï¼‰
            raw_lines = [f"{p} ã¯ä½¿ã„ã‚„ã™ã•ã¨è€ä¹…æ€§ã‚’ä¸¡ç«‹ã—ã€æ—¥å¸¸ã®ä¸ä¾¿ã‚’æ¸›ã‚‰ã—ã¾ã™ã€‚"] * 20

        refined = refine_20_lines(raw_lines)
        all_raw.append(raw_lines[:20])
        all_refined.append(refined)
        time.sleep(0.2)  # ã‚¹ãƒ­ãƒƒãƒˆãƒªãƒ³ã‚°

    write_raw(products, all_raw)
    write_refined(products, all_refined)
    write_diff(products, all_raw, all_refined)

    def avg_len(blocks):
        lens = [len(x) for lines in blocks for x in lines if x]
        return (sum(lens) / max(1, len(lens))) if lens else 0.0

    print("âœ… å‡ºåŠ›å®Œäº†:")
    print(f"   - AIç”Ÿå‡ºåŠ›: {RAW_PATH}")
    print(f"   - æ•´å½¢å¾Œ   : {REF_PATH}")
    print(f"   - å·®åˆ†æ¯”è¼ƒ : {DIFF_PATH}")
    print(f"ğŸ“ æ–‡å­—æ•°(å¹³å‡): raw={avg_len(all_raw):.1f} / refined={avg_len(all_refined):.1f}")
    print("ğŸ”’ ä»•æ§˜: è‰¯ã„ä¾‹/æ‚ªã„ä¾‹ã®few-shotèª˜å°ï¼‹assistantçŸ¥è¦‹æ³¨å…¥ãƒ»ç¦å‰‡/å¥ç‚¹/ç®‡æ¡æ›¸ãå‰¥ãŒã—ãƒ»æ¬ æè‡ªå‹•è£œå®Œ")

if __name__ == "__main__":
    main()
import atlas_autosave_core
