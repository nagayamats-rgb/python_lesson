# -*- coding: utf-8 -*-
"""
ALTé•·æ–‡ç”Ÿæˆ v4.6ï¼ˆè‡ªç„¶æ–‡ï¼‹å®‰å®šç”Ÿæˆï¼‹ãƒªãƒˆãƒ©ã‚¤ä¿è­·ï¼‰
------------------------------------------------------------
- å¯¾è±¡: æ¥½å¤© ALT å°‚ç”¨
- å…¥åŠ›: ./rakuten.csv ï¼ˆUTF-8, ãƒ˜ãƒƒãƒ€ã«ã€Œå•†å“åã€ï¼‰
- å‡ºåŠ›:
    1. output/ai_writer/alt_text_ai_raw_longform_v4.6.csv
    2. output/ai_writer/alt_text_refined_final_longform_v4.6.csv
    3. output/ai_writer/alt_text_diff_longform_v4.6.csv
- ç‰¹å¾´:
    âœ… gpt-5å¯¾å¿œï¼ˆ.envå›ºå®šï¼‰
    âœ… åè©žç¾…åˆ—ç¦æ­¢ãƒ»è‡ªç„¶æ–‡å¼·åˆ¶
    âœ… OpenAIç©ºå¿œç­”å¯¾ç­–
    âœ… å®‰å®šã‚¹ãƒ­ãƒƒãƒˆãƒªãƒ³ã‚°ï¼‹ãƒªãƒˆãƒ©ã‚¤ä¿è­·
"""

import os
import re
import csv
import glob
import json
import time
from dotenv import load_dotenv
from collections import defaultdict

try:
    from tqdm.auto import tqdm
except Exception:
    def tqdm(x, **k): return x

try:
    from openai import OpenAI
except Exception:
    raise SystemExit("openai SDKãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚`pip install openai python-dotenv` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

# ====== å®šæ•° ======
INPUT_CSV = "./rakuten.csv"
OUT_DIR = "./output/ai_writer"
RAW_PATH = os.path.join(OUT_DIR, "alt_text_ai_raw_longform_v4.6.csv")
REF_PATH = os.path.join(OUT_DIR, "alt_text_refined_final_longform_v4.6.csv")
DIFF_PATH = os.path.join(OUT_DIR, "alt_text_diff_longform_v4.6.csv")
SEMANTICS_DIR = "./output/semantics"

FORBIDDEN = [
    "ç”»åƒ", "å†™çœŸ", "è¦‹ãŸç›®", "ä¸Šã®ç”»åƒ", "ä¸‹ã®å†™çœŸ", "å½“åº—", "å½“ç¤¾", "ãƒ¬ãƒ“ãƒ¥ãƒ¼", "ãƒ©ãƒ³ã‚­ãƒ³ã‚°",
    "ã‚¯ãƒªãƒƒã‚¯", "ã“ã¡ã‚‰", "ç«¶åˆ", "å„ªä½æ€§", "æ¥­ç•Œæœ€é«˜", "æœ€å®‰", "No.1", "ãƒŠãƒ³ãƒãƒ¼ãƒ¯ãƒ³",
    "ãƒªãƒ³ã‚¯", "ãƒšãƒ¼ã‚¸", "ã‚«ãƒ¼ãƒˆ", "è³¼å…¥ã¯ã“ã¡ã‚‰", "é€æ–™ç„¡æ–™ï¼ˆç¢ºç´„ï¼‰", "è¿”é‡‘ä¿è¨¼"
]

RAW_MIN, RAW_MAX = 100, 130
FINAL_MIN, FINAL_MAX = 80, 110

# ====== æ­£è¦è¡¨ç¾ ======
LEADING_ENUM_RE = re.compile(r"^\s*[\dâ‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©\-\*\ãƒ»â—]\s*[\.ï¼Žã€]?\s*")
MULTI_COMMA_RE = re.compile(r"ã€{3,}")
WS_RE = re.compile(r"\s+")

# ====== ç’°å¢ƒè¨­å®š ======
def init_env_and_client():
    load_dotenv(override=True)
    api_key = os.getenv("OPENAI_API_KEY", "").strip()
    if not api_key:
        raise SystemExit("âŒ OPENAI_API_KEY ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚.env ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    model = os.getenv("OPENAI_MODEL", "gpt-5").strip()
    temp = float(os.getenv("OPENAI_TEMPERATURE", "1.2").strip())
    max_tokens = int(os.getenv("OPENAI_MAX_TOKENS", "2000").strip())
    client = OpenAI(api_key=api_key)
    return client, model, temp, max_tokens

# ====== å…¥åŠ› ======
def load_products(path):
    if not os.path.exists(path):
        raise SystemExit(f"âŒ å…¥åŠ›CSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path}")
    products = []
    with open(path, "r", encoding="utf-8", newline="") as f:
        reader = csv.DictReader(f)
        if "å•†å“å" not in reader.fieldnames:
            raise SystemExit("âŒ CSVã«ã€Žå•†å“åã€åˆ—ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        for r in reader:
            nm = (r.get("å•†å“å") or "").strip()
            if nm:
                products.append(nm)
    seen, uniq = set(), []
    for nm in products:
        if nm not in seen:
            uniq.append(nm)
            seen.add(nm)
    return uniq

# ====== çŸ¥è¦‹è¦ç´„ ======
def safe_load_json(path):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None

def summarize_knowledge():
    if not os.path.isdir(SEMANTICS_DIR):
        return "ä¸»è¦ã‚¹ãƒšãƒƒã‚¯ãƒ»ç”¨é€”ãƒ»å¯¾è±¡ãƒ»æ©Ÿèƒ½ãƒ»ä¾¿ç›Šã‚’è‡ªç„¶ã«å«ã‚ã‚‹ã€‚", FORBIDDEN[:]
    files = glob.glob(os.path.join(SEMANTICS_DIR, "*.json"))
    clusters, market, concept, tmpl, forbid_local = [], [], [], [], []
    for p in files:
        data = safe_load_json(p)
        if not data:
            continue
        name = os.path.basename(p).lower()
        if isinstance(data, list):
            data_list = data
            data_dict = {}
        elif isinstance(data, dict):
            data_list = []
            data_dict = data
        else:
            continue
        # clusters
        if "lexical" in name or "cluster" in name:
            arr = data_dict.get("clusters") if data_dict else data_list
            for c in arr or []:
                if isinstance(c, dict):
                    clusters += [t for t in c.get("terms", []) if isinstance(t, str)]
                elif isinstance(c, str):
                    clusters.append(c)
        # market
        if "market" in name:
            arr = data_dict.get("vocabulary") if data_dict else data_list
            for v in arr or []:
                if isinstance(v, dict) and isinstance(v.get("vocabulary"), str):
                    market.append(v["vocabulary"])
                elif isinstance(v, str):
                    market.append(v)
        # semantic
        if "semantic" in name:
            for k in ["concepts", "targets", "use_cases", "scenes"]:
                vals = data_dict.get(k) if data_dict else []
                if isinstance(vals, list):
                    concept += [v for v in vals if isinstance(v, str)]
        # template
        if "template" in name:
            tmpl += [v for v in data_dict.get("templates", []) if isinstance(v, str)]
        # forbid
        if "forbid" in name:
            forbid_local += [v for v in data_dict.get("forbidden_words", []) if isinstance(v, str)]

    all_forbid = list({*FORBIDDEN, *forbid_local})
    def cap(xs, n): return "ã€".join(list(dict.fromkeys([x for x in xs if isinstance(x, str)]))[:n])
    text = f"èªžå½™:{cap(clusters,6)} / å¸‚å ´èªž:{cap(market,6)} / æ§‹é€ :{cap(concept,5)} / éª¨å­:{cap(tmpl,3)}ã€‚"
    text += "è‡ªç„¶ãªæ—¥æœ¬èªžã§1ã€œ2æ–‡ã€å¥ç‚¹çµ‚æ­¢ã§æ›¸ãã€‚"
    return text, all_forbid

# ====== ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ======
SYSTEM_PROMPT = (
    "ã‚ãªãŸã¯æ—¥æœ¬èªžã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚"
    "æ¥½å¤©ã®å•†å“ç”»åƒALTã‚’è‡ªç„¶ãªæ—¥æœ¬èªžã§ä½œæˆã—ã¾ã™ã€‚"
    "å¿…ãš1ã€œ2æ–‡ã®è‡ªç„¶æ–‡ã§å¥ç‚¹ã€Œã€‚ã€ã§çµ‚ãˆã‚‹ã“ã¨ã€‚\n"
    "åè©žã®ç¾…åˆ—ã¯ç¦æ­¢ï¼ˆä¾‹: iPhone14ã€iPhone15ã€ã‚±ãƒ¼ãƒ–ãƒ«ã€å……é›»ï¼‰ã€‚"
    "æ–‡æœ«ã¯ã€Žã€œã§ãã‚‹ã€ã€Žã€œã§ã™ã€ã€Žã€œã—ã¾ã™ã€ã€Žã€œã«ä¾¿åˆ©ã§ã™ã€ãªã©ä¸å¯§èªžã§çµ‚ãˆã‚‹ã€‚\n"
    "ç”»åƒã‚„å†™çœŸã€ãƒ¬ãƒ“ãƒ¥ãƒ¼ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã€å½“åº—ãªã©ã®ãƒ¡ã‚¿èªžã¯ç¦æ­¢ã€‚\n"
)

def build_user_prompt(product, knowledge_text, forbid_words):
    forbid_txt = "ã€".join(sorted(set(forbid_words)))
    return (
        f"å•†å“å: {product}\n"
        f"{knowledge_text}\n"
        f"ç¦æ­¢èªž: {forbid_txt}\n"
        "20è¡Œã®è‡ªç„¶æ–‡ALTã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚å„è¡Œã¯å¥ç‚¹çµ‚æ­¢ã®1ã€œ2æ–‡ã€‚"
    )

# ====== OpenAIå‘¼ã³å‡ºã—ï¼ˆå®‰å®šç‰ˆï¼‰ ======
def call_openai_20_lines(client, model, temp, max_tokens, product, kb_text, forbid_words, retry=3, wait=6):
    user_prompt = build_user_prompt(product, kb_text, forbid_words)
    for attempt in range(retry):
        try:
            res = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt},
                ],
                max_completion_tokens=max_tokens,
                temperature=temp,
            )
            content = (res.choices[0].message.content or "").strip()
            if not content:
                raise ValueError("ç©ºå¿œç­”ã‚’æ¤œå‡º")
            lines = [ln.strip() for ln in content.split("\n") if ln.strip()]
            return lines[:80]
        except Exception as e:
            print(f"âš ï¸ OpenAIã‚¨ãƒ©ãƒ¼({attempt+1}/{retry}): {e}")
            time.sleep(wait)
    # fallback
    return [f"{product}ã¯é«˜å“è³ªãªè¨­è¨ˆã§ã€å¿«é©ã«ä½¿ç”¨ã§ãã¾ã™ã€‚"] * 20

# ====== ãƒ­ãƒ¼ã‚«ãƒ«æ•´å½¢ ======
def looks_listy(s):
    if not s: return True
    if LEADING_ENUM_RE.search(s): return True
    if "ã€‚" not in s and "ã€" in s: return True
    return False

def rewrite_listy_to_sentence(s):
    t = LEADING_ENUM_RE.sub("", s)
    bits = [b.strip(" ã€‚ã€ãƒ»-â€”") for b in t.split("ã€") if len(b.strip()) > 1]
    if not bits: return s.strip() + "ã€‚"
    head = "ã€".join(bits[:2])
    tail = "ã€".join(bits[2:]) if len(bits) > 2 else ""
    cand = f"{head}ã‚’å‚™ãˆã€{tail}ã§ã‚‚ä½¿ã„ã‚„ã™ã„è¨­è¨ˆã§ã™ã€‚" if tail else f"{head}ã«å¯¾å¿œã—ã€æ—¥å¸¸ã‚’å¿«é©ã«ã—ã¾ã™ã€‚"
    for ng in FORBIDDEN:
        cand = cand.replace(ng, "")
    if not cand.endswith("ã€‚"):
        cand += "ã€‚"
    return cand

def normalize_lines(lines):
    out = []
    for ln in lines:
        if not ln: continue
        ln = ln.strip()
        if looks_listy(ln):
            ln = rewrite_listy_to_sentence(ln)
        if not ln.endswith("ã€‚"):
            ln += "ã€‚"
        out.append(ln)
    return out

def soft_clip_sentence(t):
    t = WS_RE.sub(" ", t)
    if not t.endswith("ã€‚"):
        t += "ã€‚"
    if len(t) > 120:
        cut = t[:120]
        p = cut.rfind("ã€‚")
        if p != -1:
            t = cut[:p+1]
    for ng in FORBIDDEN:
        t = t.replace(ng, "")
    return t.strip()

def refine_20_lines(raw_lines):
    raw_lines = normalize_lines(raw_lines)
    refined = [soft_clip_sentence(x) for x in raw_lines if len(x) >= 20]
    uniq = list(dict.fromkeys(refined))
    while len(uniq) < 20:
        uniq.append(f"{uniq[-1]}ã‚ˆã‚Šä½¿ã„ã‚„ã™ã„ãƒ‡ã‚¶ã‚¤ãƒ³ã§ã™ã€‚")
    return uniq[:20]

# ====== å‡ºåŠ› ======
def ensure_outdir(): os.makedirs(OUT_DIR, exist_ok=True)
def write_csv(path, products, data, prefix):
    with open(path, "w", encoding="utf-8", newline="") as f:
        w = csv.writer(f)
        w.writerow(["å•†å“å"] + [f"{prefix}_{i+1}" for i in range(20)])
        for p, lines in zip(products, data):
            row = [p] + (lines[:20] + [""] * max(0, 20 - len(lines)))
            w.writerow(row)

# ====== ãƒ¡ã‚¤ãƒ³ ======
def main():
    print("ðŸŒ¸ ALTé•·æ–‡ç”Ÿæˆ v4.6ï¼ˆè‡ªç„¶æ–‡ï¼‹å®‰å®šç”Ÿæˆï¼‹ãƒªãƒˆãƒ©ã‚¤ï¼‰")
    client, model, temp, max_tokens = init_env_and_client()
    ensure_outdir()

    products = load_products(INPUT_CSV)
    print(f"âœ… å¯¾è±¡å•†å“: {len(products)}ä»¶")

    kb_text, forb = summarize_knowledge()
    all_raw, all_ref = [], []

    for p in tqdm(products, desc="ðŸ§  ç”Ÿæˆä¸­", total=len(products)):
        raw = call_openai_20_lines(client, model, temp, max_tokens, p, kb_text, forb)
        refined = refine_20_lines(raw)
        all_raw.append(raw[:20])
        all_ref.append(refined)
        time.sleep(0.8)

    write_csv(RAW_PATH, products, all_raw, "ALT_raw")
    write_csv(REF_PATH, products, all_ref, "ALT")

    # diff å‡ºåŠ›
    with open(DIFF_PATH, "w", encoding="utf-8", newline="") as f:
        w = csv.writer(f)
        w.writerow(["å•†å“å"] + [f"ALT_raw_{i+1}" for i in range(20)] + [f"ALT_ref_{i+1}" for i in range(20)])
        for p, raw, ref in zip(products, all_raw, all_ref):
            w.writerow([p] + raw[:20] + ref[:20])

    def avg_len(block):
        lens = [len(x) for lines in block for x in lines if x]
        return sum(lens) / len(lens) if lens else 0

    print("âœ… å‡ºåŠ›å®Œäº†:")
    print(f"   - AIç”Ÿå‡ºåŠ›: {RAW_PATH}")
    print(f"   - æ•´å½¢å¾Œ   : {REF_PATH}")
    print(f"   - å·®åˆ†æ¯”è¼ƒ : {DIFF_PATH}")
    print(f"ðŸ“ å¹³å‡æ–‡å­—æ•° raw={avg_len(all_raw):.1f}, refined={avg_len(all_ref):.1f}")

if __name__ == "__main__":
    main()
import atlas_autosave_core
