# -*- coding: utf-8 -*-
"""
v5.2 Sales Copy Persona Writer (ALT for Rakuten, 20 lines each)
- å…¥åŠ›: /Users/tsuyoshi/Desktop/python_lesson/sauce/rakuten.csv  ï¼ˆUTF-8, ãƒ˜ãƒƒãƒ€: ã€Œå•†å“åã€ï¼‰
- å‡ºåŠ›:
    1) ./output/ai_writer/alt_text_ai_raw_salescopy_v5_2.csv        â€¦ AIã®ç”Ÿå‡ºåŠ›ï¼ˆ20æœ¬/å•†å“ï¼‰
    2) ./output/ai_writer/alt_text_refined_salescopy_v5_2.csv       â€¦ ãƒ­ãƒ¼ã‚«ãƒ«æ•´å½¢å¾Œï¼ˆ80ã€œ110å­—ï¼‰
    3) ./output/ai_writer/alt_text_diff_salescopy_v5_2.csv          â€¦ raw/refined ã®æ¨ªä¸¦ã³æ¯”è¼ƒ
- çŸ¥è¦‹: ./output/semantics/*.json ã‚’â€œè¦ï¼ˆã‹ã‚“ãªã‚ï¼‰â€ã¨ã—ã¦ã‚†ã‚‹ãé›†ç´„ï¼ˆå­˜åœ¨ç¯„å›²ã§OKï¼‰
- OpenAI:
    - .env ã‹ã‚‰ OPENAI_API_KEY ã‚’å–å¾—
    - OPENAI_MODELï¼ˆæŒ‡å®šãªã‘ã‚Œã° 'gpt-4o'ï¼‰/ OPENAI_TEMPERATURE / OPENAI_MAX_TOKENS ã‚’ä½¿ç”¨
    - chat.completionsï¼ˆresponse_format={"type":"text"}ï¼‰
- ä»•æ§˜è¦ç‚¹:
    - æ¥½å¤©ALTå°‚ç”¨ï¼ˆYahooã§ä½¿ã†è²©ä¿ƒèªã¯é¿ã‘ã‚‹ï¼‰
    - ç”»åƒæå†™èªãƒ»ãƒ¡ã‚¿èªãƒ»ç«¶åˆæ¯”è¼ƒã®ãƒ¡ã‚¿è¡¨ç¾ã¯ç¦æ­¢
    - å„å•†å“20æœ¬ã€1ã€œ2æ–‡ã€æ–‡å°¾ã¯å¥ç‚¹ã€Œã€‚ã€ã¨ä½“è¨€æ­¢ã‚ã‚’è‡ªç„¶ã«æ··åœ¨
    - 80ã€œ110å­—ãƒ¬ãƒ³ã‚¸ã«ãƒ­ãƒ¼ã‚«ãƒ«ã§æ•´å½¢
    - é‡è¤‡/è¿‘ä¼¼ï¼ˆ>=0.90ï¼‰ã‚’é™¤å»ã—è£œå®Œ
    - ç©ºè¡Œ/å´©ã‚Œã¯è‡ªå‹•è£œå®Œï¼ˆå•†å“èª¬æ˜è£œå®Œæ–‡ã§åŸ‹ã‚ã‚‹ï¼‰
"""

import os
import re
import csv
import glob
import json
import time
from pathlib import Path
from difflib import SequenceMatcher
from collections import defaultdict

from dotenv import load_dotenv

# tqdmï¼ˆæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ã‚‚å‹•ãï¼‰
try:
    from tqdm.auto import tqdm
except Exception:
    def tqdm(x, **k): return x

# OpenAI SDKï¼ˆæ–°æ—§æ··åœ¨å¯¾ç­–ï¼‰
try:
    from openai import OpenAI
except Exception:
    raise SystemExit("âŒ openai SDK ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚`pip install openai python-dotenv` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

# ==============
# 0) å®šæ•°ãƒ»I/O
# ==============
# å…¥åŠ›CSVï¼ˆå›ºå®šãƒ‘ã‚¹ï¼UTF-8ï¼‰
INPUT_CSV = "/Users/tsuyoshi/Desktop/python_lesson/sauce/rakuten.csv"

# å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
OUT_DIR = "./output/ai_writer"
RAW_PATH = os.path.join(OUT_DIR, "alt_text_ai_raw_salescopy_v5_2.csv")
REF_PATH = os.path.join(OUT_DIR, "alt_text_refined_salescopy_v5_2.csv")
DIFF_PATH = os.path.join(OUT_DIR, "alt_text_diff_salescopy_v5_2.csv")

# çŸ¥è¦‹ï¼ˆè¦ï¼šã‹ã‚“ãªã‚ï¼‰
SEMANTICS_DIR = "./output/semantics"

# ç¦å‰‡èªï¼ˆç”»åƒæå†™èªãƒ»ãƒ¡ã‚¿ãƒ»åº—èˆ—ãƒ¡ã‚¿ãªã©ï¼‰â€” æ¥½å¤©ALTå°‚ç”¨
FORBIDDEN_BASE = [
    "ç”»åƒ", "å†™çœŸ", "è¦‹ãŸç›®", "ä¸Šã®ç”»åƒ", "ä¸‹ã®å†™çœŸ",
    "å½“åº—", "å½“ç¤¾", "ã‚·ãƒ§ãƒƒãƒ—", "ãƒ¬ãƒ“ãƒ¥ãƒ¼", "å£ã‚³ãƒŸ", "ãƒ©ãƒ³ã‚­ãƒ³ã‚°",
    "ã‚¯ãƒªãƒƒã‚¯", "ã“ã¡ã‚‰", "ãƒªãƒ³ã‚¯", "ã‚«ãƒ¼ãƒˆ", "è³¼å…¥ã¯ã“ã¡ã‚‰",
    "æœ€å®‰", "No.1", "ãƒŠãƒ³ãƒãƒ¼ãƒ¯ãƒ³", "å£²ä¸ŠNo1", "æ¥­ç•Œæœ€é«˜", "ç«¶åˆ", "ç«¶åˆå„ªä½æ€§",
    "è¿”é‡‘ä¿è¨¼", "é€æ–™ç„¡æ–™ï¼ˆç¢ºç´„ï¼‰", "ãƒã‚¤ãƒ³ãƒˆé‚„å…ƒ", "é™å®šã‚¯ãƒ¼ãƒãƒ³"  # â† Yahooçš„è²©ä¿ƒèªã¯é¿ã‘ã‚‹
]

# å¥èª­ç‚¹ã‚„ç®‡æ¡æ›¸ãã®æƒé™¤
LEADING_ENUM_RE = re.compile(r"^\s*[\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åâ‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©\-\*\ãƒ»\u2022]\s*[\.ï¼ã€]?\s*")
WS_RE = re.compile(r"\s+")
MULTI_COMMA_RE = re.compile(r"ã€{3,}")

# æ–‡é•·ãƒãƒªã‚·ãƒ¼
RAW_MIN, RAW_MAX = 100, 130     # AIã«ã¯ã“ã®ãƒ¬ãƒ³ã‚¸ã‚’ç‹™ã‚ã›ã‚‹
FINAL_MIN, FINAL_MAX = 80, 110  # ãƒ­ãƒ¼ã‚«ãƒ«ã§æœ€çµ‚èª¿æ•´

# é‡è¤‡è¿‘ä¼¼ã®é–¾å€¤
SIM_THRESHOLD = 0.90

# ä½“è¨€æ­¢ã‚ã®é©ç”¨ç‡ï¼ˆç›®å®‰ï¼‰
TAIGEN_RATE = 0.35

# ==============
# 1) â€œè¦ï¼ˆã‹ã‚“ãªã‚ï¼‰â€
# ==============
KANNAME_BANNER = "â›© è¦ï¼ˆã‹ã‚“ãªã‚ï¼‰: çŸ¥è¦‹ã‚’ä¸­æ¢ã§çµ±åˆã—ã€ALTæœ€é©åŒ–ã¸åæ˜ "

# ==============
# 2) ENV & Client
# ==============
def init_env_and_client():
    load_dotenv(override=True)
    api_key = (os.getenv("OPENAI_API_KEY") or "").strip()
    if not api_key:
        raise SystemExit("âŒ OPENAI_API_KEY ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚.env ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    model = (os.getenv("OPENAI_MODEL") or "gpt-4o").strip()
    try:
        temperature = float(os.getenv("OPENAI_TEMPERATURE") or "1.0")
    except Exception:
        temperature = 1.0
    try:
        max_tokens = int(os.getenv("OPENAI_MAX_TOKENS") or "1200")
    except Exception:
        max_tokens = 1200

    client = OpenAI(api_key=api_key)
    return client, model, temperature, max_tokens

# ==============
# 3) å…¥åŠ›ï¼ˆå•†å“åï¼‰
# ==============
def load_products(path: str):
    if not os.path.exists(path):
        raise SystemExit(f"âŒ å…¥åŠ›CSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path}")
    products = []
    with open(path, "r", encoding="utf-8", newline="") as f:
        reader = csv.DictReader(f)
        if "å•†å“å" not in reader.fieldnames:
            raise SystemExit("âŒ å…¥åŠ›CSVã«ã€å•†å“åã€ãƒ˜ãƒƒãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        for r in reader:
            nm = (r.get("å•†å“å") or "").strip()
            if nm:
                products.append(nm)
    # é‡è¤‡é™¤å»ï¼ˆé †åºä¿æŒï¼‰
    seen, uniq = set(), []
    for nm in products:
        if nm not in seen:
            seen.add(nm)
            uniq.append(nm)
    return uniq

# ==============
# 4) çŸ¥è¦‹é›†ç´„ï¼ˆè¦ï¼‰
# ==============
def safe_load_json(path):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None

def summarize_knowledge_relaxed():
    """
    /output/semantics é…ä¸‹ã® JSON ç¾¤ï¼ˆå½¢å¼ãƒãƒ©ãƒãƒ©OKï¼‰ã‚’ã€Œè¦ã€ã¨ã—ã¦ç·©ã‚„ã‹ã«çµ±åˆã€‚
    - lexical_clusters_*.json    â†’ èªå½™ã‚¯ãƒ©ã‚¹ã‚¿
    - market_vocab_*.json        â†’ å¸‚å ´èªå½™
    - structured_semantics_*.jsonâ†’ æ§‹é€ çš„è¦³ç‚¹ï¼ˆç”¨é€”/å¯¾è±¡/ã‚·ãƒ¼ãƒ³/ç‰¹å¾´/ä¾¿ç›Šãªã©ï¼‰
    - styled_persona_*.json      â†’ ãƒˆãƒ¼ãƒ³ãƒ»æ–‡ä½“
    - normalized_*.json          â†’ ç¦å‰‡èªãªã©
    - template_composer.json     â†’ éª¨å­ãƒ’ãƒ³ãƒˆ
    """
    clusters, market, semantics, persona, templates, forbid_local = [], [], [], [], [], []
    if not os.path.isdir(SEMANTICS_DIR):
        # è¦ã®åˆæœŸçŸ¥è¦‹ï¼ˆæœ€ä½é™ï¼‰
        base_text = ("çŸ¥è¦‹: ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»ã‚¹ãƒšãƒƒã‚¯ãƒ»å¯¾å¿œæ©Ÿç¨®ãƒ»åˆ©ç”¨ã‚·ãƒ¼ãƒ³ãƒ»å¯¾è±¡ãƒ»ä¾¿ç›Šã‚’è‡ªç„¶ã«ç¹”ã‚Šè¾¼ã¿ã€"
                     "ç®‡æ¡æ›¸ãã‚’é¿ã‘ã€è‡ªç„¶ãªæ—¥æœ¬èªã§2æ–‡ä»¥å†…ã‚’åŸºæœ¬ã€‚")
        return base_text, FORBIDDEN_BASE[:]

    for p in glob.glob(os.path.join(SEMANTICS_DIR, "*.json")):
        data = safe_load_json(p)
        if data is None:
            continue
        name = os.path.basename(p).lower()

        try:
            # é…åˆ—/dict æ··åœ¨ã‚’å¸å
            if "lexical" in name:
                if isinstance(data, list):
                    for item in data:
                        if isinstance(item, dict) and "terms" in item and isinstance(item["terms"], list):
                            clusters += [t for t in item["terms"] if isinstance(t, str)]
                        elif isinstance(item, str):
                            clusters.append(item)
                elif isinstance(data, dict):
                    arr = data.get("clusters") or data.get("lexical") or []
                    if isinstance(arr, list):
                        for c in arr:
                            if isinstance(c, dict) and isinstance(c.get("terms"), list):
                                clusters += [t for t in c["terms"] if isinstance(t, str)]

            elif "market_vocab" in name or "market" in name:
                if isinstance(data, list):
                    for v in data:
                        if isinstance(v, dict) and isinstance(v.get("vocabulary"), str):
                            market.append(v["vocabulary"])
                        elif isinstance(v, str):
                            market.append(v)
                elif isinstance(data, dict):
                    vocab = data.get("vocabulary") or data.get("vocab") or []
                    if isinstance(vocab, list):
                        market += [x for x in vocab if isinstance(x, str)]

            elif "structured_semantics" in name or "semantic" in name:
                if isinstance(data, dict):
                    for k in ["concepts", "semantics", "frames", "features", "facets", "benefits", "scenes", "targets", "use_cases"]:
                        arr = data.get(k) or []
                        if isinstance(arr, list):
                            semantics += [x for x in arr if isinstance(x, str)]
                elif isinstance(data, list):
                    semantics += [x for x in data if isinstance(x, str)]

            elif "styled_persona" in name or "persona" in name:
                if isinstance(data, dict):
                    t = data.get("tone") or data.get("style") or {}
                    if isinstance(t, dict):
                        for v in t.values():
                            if isinstance(v, str):
                                persona.append(v)
                    # fallback: ãƒ•ãƒ©ãƒƒãƒˆæ–‡å­—åˆ—ã®é…åˆ—ã‚‚æ‹¾ã†
                    for k in ["persona", "tones", "styles"]:
                        arr = data.get(k) or []
                        if isinstance(arr, list):
                            persona += [x for x in arr if isinstance(x, str)]
                elif isinstance(data, list):
                    persona += [x for x in data if isinstance(x, str)]

            elif "normalized" in name or "forbid" in name:
                if isinstance(data, dict):
                    fw = data.get("forbidden_words") or []
                    if isinstance(fw, list):
                        forbid_local += [w for w in fw if isinstance(w, str)]
                elif isinstance(data, list):
                    forbid_local += [w for w in data if isinstance(w, str)]

            elif "template_composer" in name or "template" in name:
                if isinstance(data, dict):
                    hints = data.get("hints") or data.get("templates") or []
                    if isinstance(hints, list):
                        templates += [h for h in hints if isinstance(h, str)]
                elif isinstance(data, list):
                    templates += [x for x in data if isinstance(x, str)]

        except Exception:
            # å½¢å¼ä¸ä¸€è‡´ã¯é»™ã£ã¦ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå …ç‰¢é‡è¦–ï¼‰
            pass

    # ãƒ¦ãƒ‹ãƒ¼ã‚¯ï¼†ä¸Šé™
    def uniq_cap(xs, n):
        return list(dict.fromkeys([x for x in xs if isinstance(x, str)]))[:n]

    clusters = uniq_cap(clusters, 18)
    market   = uniq_cap(market,   18)
    semantics= uniq_cap(semantics,18)
    persona  = uniq_cap(persona,   8)
    templates= uniq_cap(templates, 6)
    forbid_all = list(dict.fromkeys(FORBIDDEN_BASE + uniq_cap(forbid_local, 64)))

    # â€œè¦â€ãƒ†ã‚­ã‚¹ãƒˆï¼ˆAIã«æ¸¡ã™æ—¥æœ¬èªçŸ¥è¦‹ï¼‰
    kb_parts = []
    if clusters:  kb_parts.append(f"èªå½™: {'ã€'.join(clusters)}")
    if market:    kb_parts.append(f"å¸‚å ´èª: {'ã€'.join(market)}")
    if semantics: kb_parts.append(f"æ§‹é€ : {'ã€'.join(semantics)}")
    if templates: kb_parts.append(f"éª¨å­: {'ã€'.join(templates)}")
    if persona:   kb_parts.append(f"ãƒˆãƒ¼ãƒ³: {'ã€'.join(persona)}")

    if kb_parts:
        kb_text = "çŸ¥è¦‹ï¼ˆè¦ï¼‰: " + " / ".join(kb_parts) + "ã€‚"
    else:
        kb_text = ("çŸ¥è¦‹ï¼ˆè¦ï¼‰: ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»ã‚¹ãƒšãƒƒã‚¯ãƒ»å¯¾å¿œæ©Ÿç¨®ãƒ»åˆ©ç”¨ã‚·ãƒ¼ãƒ³ãƒ»å¯¾è±¡ãƒ»ä¾¿ç›Šã‚’è‡ªç„¶ã«ç¹”ã‚Šè¾¼ã‚€ã€‚"
                   "ç®‡æ¡æ›¸ãã‚’é¿ã‘ã€è‡ªç„¶ãªæ—¥æœ¬èªã§2æ–‡ä»¥å†…ã‚’åŸºæœ¬ã€‚")
    kb_text += " ALTã¯ç”»åƒæå†™èªã‚„ECãƒ¡ã‚¿èªã‚’ä½¿ã‚ãšã€è‡ªç„¶æ–‡ã¨ã—ã¦èª­ã‚ã‚‹ã“ã¨ã€‚"
    return kb_text, forbid_all

# ==============
# 5) ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
# ==============
SYSTEM_PROMPT = (
    "ã‚ãªãŸã¯æ—¥æœ¬èªã«ç²¾é€šã—ãŸSEOã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã‚ã‚Šã€"
    "æ¥½å¤©å¸‚å ´ã®å•†å“ãƒšãƒ¼ã‚¸ã§é«˜ã„æˆç´„ç‡ã‚’èª‡ã‚‹å£²ã‚Œã£å­ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚"
    "èª­è€…ã«ä¼ã‚ã‚‹è‡ªç„¶ãªãƒªã‚ºãƒ ã§ã€ALTã¨ã—ã¦ä½¿ãˆã‚‹ç´¹ä»‹æ–‡ã‚’20æœ¬ä½œæˆã—ã¦ãã ã•ã„ã€‚\n"
    "å‡ºåŠ›ã¯20è¡Œã€‚å„è¡Œã¯1ã€œ2æ–‡ä»¥å†…ã€‚å¥ç‚¹ã€Œã€‚ã€ã§çµ‚ãˆã‚‹æ–‡ã¨ä½“è¨€æ­¢ã‚ã®æ–‡ã‚’è‡ªç„¶ã«æ··ãœã¦ãã ã•ã„ã€‚\n"
    "å•†å“åãƒ»æ©Ÿèƒ½ãƒ»ç‰¹å¾´ãƒ»å¯¾å¿œæ©Ÿç¨®ãƒ»åˆ©ç”¨ã‚·ãƒ¼ãƒ³ãƒ»ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ»ãƒ™ãƒãƒ•ã‚£ãƒƒãƒˆã‚’è‡ªç„¶ã«ç¹”ã‚Šè¾¼ã¿ã€"
    "SEOã‚’æ„è­˜ã—ã¤ã¤ã€èª­ã¿ã‚„ã™ã•ã‚’æœ€å„ªå…ˆã«ã—ã¦ãã ã•ã„ã€‚\n"
    "åŒä¸€å•†å“ã®20æœ¬ã¯ã€æ§‹æ–‡ãƒ»èªå½™ãƒ»èªå°¾ãƒ»ç„¦ç‚¹ãƒ»è¦–ç‚¹ã‚’å¤‰ãˆã€å¤šæ§˜æ€§ã‚’ç¢ºä¿ã€‚æ„å‘³ã®é‡è¤‡ã‚„èªé †ã®ç„¼ãç›´ã—ã¯ç¦æ­¢ã€‚\n"
    "ç¦æ­¢èªï¼ˆç”»åƒã€å†™çœŸã€å½“åº—ã€ãƒ¬ãƒ“ãƒ¥ãƒ¼ã€ãƒªãƒ³ã‚¯ã€æœ€å®‰ã€No.1ã€ç«¶åˆã€ç«¶åˆå„ªä½æ€§ã€ãªã©ï¼‰ã¯ä½¿ã‚ãªã„ã§ãã ã•ã„ã€‚\n"
    "å‡ºåŠ›ã¯ãƒ†ã‚­ã‚¹ãƒˆ20è¡Œã®ã¿ï¼ˆJSON/ç®‡æ¡æ›¸ã/ç•ªå·/ãƒ©ãƒ™ãƒ«ç¦æ­¢ï¼‰ã€‚"
)

def build_user_prompt(product: str, knowledge_text: str, forbid_words: list):
    forbid_txt = "ã€".join(sorted(set([w for w in forbid_words if isinstance(w, str)])))
    hint = (
        "æ§‹æˆãƒ’ãƒ³ãƒˆï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬åŒ–ã—ãªã„ï¼‰ï¼šå•†å“ã‚¹ãƒšãƒƒã‚¯â†’å¼·ã¿ï¼ˆã‚³ã‚¢ï¼‰â†’ã©ã‚“ãªäººâ†’ã©ã‚“ãªã‚·ãƒ¼ãƒ³â†’ä¾¿ç›Šã€‚"
        "å¿…è¦ã«å¿œã˜ã¦å¯¾å¿œæ©Ÿç¨®ã‚„å‹ç•ªã‚‚è‡ªç„¶ã«å«ã‚ã‚‹ã€‚"
    )
    target = (
        f"å•†å“å: {product}\n"
        f"{knowledge_text}\n"
        f"{hint}\n"
        f"ç¦æ­¢èªï¼ˆçµ¶å¯¾ã«ä½¿ã‚ãªã„ï¼‰: {forbid_txt}\n"
        "20è¡Œã§ã€å„è¡Œã¯è‡ªç„¶ãªæ—¥æœ¬èªã®æ–‡ã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
    )
    return target

# ==============
# 6) OpenAI å‘¼ã³å‡ºã—
# ==============
def call_openai_20_lines(client, model, temperature, max_tokens, product, kb_text, forbid_words, retry=3, wait=6):
    user_prompt = build_user_prompt(product, kb_text, forbid_words)
    last_err = None
    for _ in range(retry):
        try:
            res = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt},
                ],
                response_format={"type": "text"},
                max_completion_tokens=max_tokens,
                temperature=temperature,
            )
            content = (res.choices[0].message.content or "").strip()
            if content:
                # è¡Œã«åˆ†è§£ã—ã¦ç•ªå·ãƒ»ç®‡æ¡æ›¸ããƒ»å…ˆé ­è¨˜å·ã‚’å‰¥ãŒã™
                lines = []
                for ln in content.splitlines():
                    s = ln.strip()
                    if not s:
                        continue
                    s = LEADING_ENUM_RE.sub("", s)
                    s = s.strip("ãƒ»-â€”â—ã€€")
                    if s:
                        lines.append(s)
                # ä½™åˆ†ãªè¡ŒãŒè¿”ã‚‹ã“ã¨ãŒã‚ã‚‹ã®ã§æœ€å¤§60ã¾ã§ä¿æŒï¼ˆå¾Œã§20æŠ½å‡ºï¼‰
                return lines[:60]
        except Exception as e:
            last_err = e
            time.sleep(wait)
    raise RuntimeError(f"OpenAIå¿œç­”ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ: {last_err}")

# ==============
# 7) ãƒ­ãƒ¼ã‚«ãƒ«æ•´å½¢
# ==============
def soft_clip_sentence(text: str, min_len=FINAL_MIN, max_len=FINAL_MAX) -> str:
    t = (text or "").strip()
    if not t:
        return ""
    # ç•ªå·/ç®‡æ¡æ›¸ãæƒé™¤
    t = LEADING_ENUM_RE.sub("", t).strip("ãƒ»-â€”â—ã€€")
    # ä½™è¨ˆãªç©ºç™½åœ§ç¸®
    t = WS_RE.sub(" ", t)
    t = MULTI_COMMA_RE.sub("ã€ã€", t)

    # å¥ç‚¹è£œå®Œ or ä½“è¨€æ­¢ã‚è¨±å®¹ï¼ˆã™ã§ã«å¥ç‚¹ãªã—ãªã‚‰ãã®ã¾ã¾è¨±å®¹ã€‚å¾Œæ®µã§å‰²åˆèª¿æ•´ï¼‰
    if t.endswith(("ã€‚", "ï¼", "ï¼Ÿ")):
        end_mark = True
    else:
        end_mark = False

    # é•·ã™ãã‚‹å ´åˆï¼šmax_len+10 ç¨‹åº¦ã¾ã§è¨±å®¹ â†’ æœ€å¾Œã®ã€Œã€‚ã€ã§ã‚«ãƒƒãƒˆã—ã¦80ã€œ110ã«å¯„ã›ã‚‹
    hard_cap = max_len + 10
    if len(t) > hard_cap:
        cut = t[:hard_cap]
        p = cut.rfind("ã€‚")
        if p != -1 and p + 1 >= min_len:  # è‡ªç„¶ãªå¥ç‚¹çµ‚æ­¢ãŒminã‚’æº€ãŸã™ãªã‚‰æ¡ç”¨
            t = cut[:p+1]
        else:
            t = cut

    # ç¦å‰‡èªã¯æœ€çµ‚ã§é™¤å»ï¼ˆå®Œå…¨é™¤å»ï¼‰
    return t.strip()

def ends_with_punctuation(s: str) -> bool:
    return s.endswith(("ã€‚", "ï¼", "ï¼Ÿ"))

def to_taigen_if_needed(s: str) -> str:
    """èªå°¾è‡ªç„¶åŒ–ï¼šã§ã™/ã¾ã™ ã‚’ä½“è¨€æ­¢ã‚ã¸ï¼ˆè»½ç‡ãªå‰Šã‚Šã‚’é¿ã‘ã‚‹ï¼‰"""
    t = s.strip()
    # æ—¢ã«ä½“è¨€æ­¢ã‚ã£ã½ã„ãªã‚‰ãã®ã¾ã¾
    if ends_with_punctuation(t):
        if t.endswith("ã€‚"):
            # 3ã€œ4æ–‡å­—ã®æ•¬ä½“ã‚’é™å®šçš„ã«è½ã¨ã™
            for rep in ("ã—ã¾ã™ã€‚", "ã§ãã¾ã™ã€‚", "ã§ã—ãŸã€‚", "ã§ã™ã€‚", "ã¾ã™ã€‚"):
                if t.endswith(rep) and len(t) > len(rep) + 8:
                    return t[:-len("ã§ã™ã€‚")] + "ã§ã™" if rep == "ã§ã™ã€‚" else t[:-1]  # æœ«å°¾å¥ç‚¹ã ã‘å‰Šã‚‹
        return t
    # ä½“è¨€æ­¢ã‚ã£ã½ã„æœ«å°¾ãªã‚‰ãã®ã¾ã¾
    return t

def uniq_by_similarity(lines, threshold=SIM_THRESHOLD):
    """é«˜é¡ä¼¼ã®æ–‡ã‚’é™¤å»"""
    uniq = []
    for s in lines:
        is_dup = False
        for u in uniq:
            if SequenceMatcher(None, s, u).ratio() >= threshold:
                is_dup = True
                break
        if not is_dup:
            uniq.append(s)
    return uniq

def fallback_sentence(product: str) -> str:
    return f"{product} ã®ä½¿ã„å‹æ‰‹ã‚’é«˜ã‚ã‚‹è¨­è¨ˆã§ã€æ—¥å¸¸ã®ä¸ä¾¿ã‚’æ¸›ã‚‰ã™å®Ÿç”¨çš„ãªä¸€å“ã§ã™ã€‚"

def refine_20_lines(product: str, raw_lines, forbid_words):
    # æ­£è¦åŒ– â†’ å¥ç‚¹/ä½“è¨€æ­¢ã‚è¨±å®¹ â†’ é•·ã•æ•´å½¢
    norm = []
    for ln in raw_lines:
        if not ln:
            continue
        s = soft_clip_sentence(ln)
        if not s:
            continue
        norm.append(s)

    # é¡ä¼¼é™¤å»
    norm = uniq_by_similarity(norm)

    # ç¦å‰‡èªå‰Šé™¤ï¼ˆå®Œå…¨é™¤å»ï¼‰
    for i, s in enumerate(norm):
        for ng in forbid_words:
            if ng and ng in s:
                s = s.replace(ng, "")
        norm[i] = s.strip()

    # æœ«å°¾å¥ç‚¹ã¨ä½“è¨€æ­¢ã‚ã®æ··åœ¨ï¼ˆå¾Œã§å‰²åˆã‚’æ•´ãˆã‚‹ï¼‰
    out = []
    for s in norm:
        if not s:
            continue
        # â€œå˜èªç¾…åˆ—â€ã£ã½ã„çŸ­æ–‡ã¯ç ´æ£„
        if len(s) < 20:
            continue
        out.append(s)

    # å¥ç‚¹çµ‚æ­¢ã§è‡ªç„¶ãªé•·ã•ã¸å†èª¿æ•´
    final = []
    for s in out:
        ss = s
        if len(ss) > FINAL_MAX + 10:
            cut = ss[:FINAL_MAX + 10]
            p = cut.rfind("ã€‚")
            if p != -1 and p + 1 >= FINAL_MIN:
                ss = cut[:p+1]
            else:
                ss = cut
        if not ss:
            continue
        final.append(ss)

    # ä½“è¨€æ­¢ã‚å‰²åˆã‚’æ•´ãˆã‚‹ï¼ˆç´„ TAIGEN_RATEï¼‰
    rng = list(range(len(final)))
    if rng:
        target_n = max(1, int(len(final) * TAIGEN_RATE))
        chosen = set()
        i = 0
        while len(chosen) < target_n and i < len(rng):
            idx = rng[i]
            if final[idx].endswith("ã€‚"):
                # â€œã§ã™/ã¾ã™/ã¾ã—ãŸâ€ã®å ´åˆã¯å¥ç‚¹ã ã‘æ®‹ã—ã¤ã¤è‡ªç„¶ã«
                final[idx] = to_taigen_if_needed(final[idx])
                if final[idx].endswith("ã€‚"):
                    # ä½“è¨€åŒ–ã—ãã‚Œãªã‘ã‚Œã°æ–‡æœ«å¥ç‚¹ã¯ç¶­æŒ
                    pass
                else:
                    # ä½“è¨€ã«ãªã£ã¦å¥ç‚¹æ¶ˆãˆãŸå ´åˆ â†’ ãã®ã¾ã¾
                    pass
                chosen.add(idx)
            i += 1

    # 20æœ¬ã«æº€ãŸãªã‘ã‚Œã°è£œå®Œï¼ˆèªå°¾å¤‰å½¢ï¼‰
    def light_variation(s: str) -> str:
        t = s
        # èªå°¾ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆè»½ã„ç½®æ›ï¼‰
        t = t.replace("ã—ã¾ã™ã€‚", "ã§ãã¾ã™ã€‚")
        t = t.replace("ã§ãã¾ã™ã€‚", "ã—ã‚„ã™ã„ã§ã™ã€‚")
        t = t.replace("ã§ã™ã€‚", "ã«ãªã‚Šã¾ã™ã€‚")
        if t == s:
            # å¥å†…ã«è»½å¾®ãªåŠ©è©ã‚’è¿½åŠ ï¼ˆéå‰°ãªå¤‰åŒ–ã¯é¿ã‘ã‚‹ï¼‰
            t = re.sub(r"([^\sã€ã€‚]{3,})", r"\1ã€", t, count=1)
            t = t.replace("ã€ã€", "ã€")
        # æœ€å¾Œã«è»½ãæ•´å½¢
        t = soft_clip_sentence(t)
        return t

    base = final[:]
    i = 0
    while len(final) < 20 and base:
        cand = light_variation(base[i % len(base)])
        # é¡ä¼¼æŠ‘åˆ¶
        if all(SequenceMatcher(None, cand, x).ratio() < SIM_THRESHOLD for x in final):
            final.append(cand)
        i += 1
        if i > 200:  # å¿µã®ãŸã‚ã®ãƒ–ãƒ¬ãƒ¼ã‚¯
            break

    # ã¾ã è¶³ã‚Šãªã‘ã‚Œã°è£œå®Œæ–‡
    while len(final) < 20:
        cand = fallback_sentence(product)
        if all(SequenceMatcher(None, cand, x).ratio() < SIM_THRESHOLD for x in final):
            final.append(cand)
        else:
            final.append(cand + "")  # ã©ã†ã—ã¦ã‚‚åŸ‹ã¾ã‚‰ãªã„å ´åˆã¯ãã®ã¾ã¾

    # å¤šã™ãã‚‹ãªã‚‰å…ˆé ­20
    return final[:20]

# ==============
# 8) æ›¸ãå‡ºã—
# ==============
def ensure_outdir():
    os.makedirs(OUT_DIR, exist_ok=True)

def write_raw(products, all_raw):
    with open(RAW_PATH, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["å•†å“å"] + [f"ALT_raw_{i+1}" for i in range(20)])
        for p, lines in zip(products, all_raw):
            row = [p] + (lines[:20] + [""] * max(0, 20 - len(lines)))
            w.writerow(row)

def write_refined(products, all_refined):
    with open(REF_PATH, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["å•†å“å"] + [f"ALT_{i+1}" for i in range(20)])
        for p, lines in zip(products, all_refined):
            row = [p] + (lines[:20] + [""] * max(0, 20 - len(lines)))
            w.writerow(row)

def write_diff(products, all_raw, all_refined):
    with open(DIFF_PATH, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        header = ["å•†å“å"] + [f"ALT_raw_{i+1}" for i in range(20)] + [f"ALT_refined_{i+1}" for i in range(20)]
        w.writerow(header)
        for p, r, ref in zip(products, all_raw, all_refined):
            r_line = (r[:20] + [""] * max(0, 20 - len(r)))
            ref_line = (ref[:20] + [""] * max(0, 20 - len(ref)))
            w.writerow([p] + r_line + ref_line)

# ==============
# 9) ãƒ¡ã‚¤ãƒ³
# ==============
def main():
    print("ğŸŒ¸ v5.2 Sales Copy Persona Writerï¼ˆALTÃ—è‡ªç„¶æ–‡Ã—è¦ï¼‰")
    print(KANNAME_BANNER)

    client, model, temperature, max_tokens = init_env_and_client()
    ensure_outdir()

    products = load_products(INPUT_CSV)
    print(f"âœ… å¯¾è±¡å•†å“: {len(products)}ä»¶")

    knowledge_text, forbid_all = summarize_knowledge_relaxed()
    print("âœ… è¦ï¼ˆçŸ¥è¦‹ï¼‰èª­è¾¼å®Œäº†")

    all_raw, all_refined = [], []

    for p in tqdm(products, total=len(products), desc="ğŸ§  ç”Ÿæˆä¸­"):
        # 1) AIã§20æœ¬ï¼ˆâ‰§20æœ¬è¿”ã‚‹å ´åˆã‚‚ã‚ã‚Šï¼‰
        try:
            raw_lines = call_openai_20_lines(
                client, model, temperature, max_tokens,
                product=p, kb_text=knowledge_text, forbid_words=forbid_all,
                retry=3, wait=6
            )
        except Exception as e:
            # å…¨å¤±æ•— â†’ ãƒ€ãƒŸãƒ¼ã§20æœ¬
            raw_lines = [f"{p} ã®ä½¿ã„å‹æ‰‹ã‚’é«˜ã‚ã‚‹è¨­è¨ˆã§ã€æ—¥å¸¸ã®ä¸ä¾¿ã‚’æ¸›ã‚‰ã™å®Ÿç”¨çš„ãªä¸€å“ã§ã™ã€‚"] * 20

        # 2) ãƒ­ãƒ¼ã‚«ãƒ«æ•´å½¢ â†’ 20æœ¬ã«ç¢ºå®š
        refined_lines = refine_20_lines(p, raw_lines, forbid_all)

        all_raw.append(raw_lines[:20])
        all_refined.append(refined_lines)

        # éè² è·å›é¿
        time.sleep(0.2)

    # 3) æ›¸ãå‡ºã—
    write_raw(products, all_raw)
    write_refined(products, all_refined)
    write_diff(products, all_raw, all_refined)

    # ç°¡æ˜“ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    def avg_len(blocks):
        lens = [len(x) for lines in blocks for x in lines if x]
        return (sum(lens) / max(1, len(lens))) if lens else 0.0

    print("âœ… å‡ºåŠ›å®Œäº†:")
    print(f"   - AIç”Ÿå‡ºåŠ›: {RAW_PATH}")
    print(f"   - æ•´å½¢å¾Œ   : {REF_PATH}")
    print(f"   - å·®åˆ†æ¯”è¼ƒ : {DIFF_PATH}")
    print(f"ğŸ“ æ–‡å­—æ•°(å¹³å‡): raw={avg_len(all_raw):.1f} / refined={avg_len(all_refined):.1f}")
    print("ğŸ”’ ä»•æ§˜: ")
    print(f"   - AIã¯ç´„{RAW_MIN}ã€œ{RAW_MAX}å­—ãƒ»1ã€œ2æ–‡ã€å¥ç‚¹/ä½“è¨€æ­¢ã‚æ··åœ¨ã€ç¦å‰‡é©ç”¨ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰")
    print(f"   - ãƒ­ãƒ¼ã‚«ãƒ«æ•´å½¢ã§{FINAL_MIN}ã€œ{FINAL_MAX}å­—ã«èª¿æ•´ã€é‡è¤‡é™¤å»ãƒ»ç¦å‰‡å†é©ç”¨ãƒ»ç©ºæ¬„è£œå®Œ")

if __name__ == "__main__":
    main()
import atlas_autosave_core
