# -*- coding: utf-8 -*-
"""
v5.3_salescopy_fusion.py
- ç›®çš„: æ¥½å¤©ALTï¼ˆ20æœ¬/å•†å“ï¼‰ã‚’ã€ŒSEOã«å¼·ã„è‡ªç„¶æ–‡ã€ã§å®‰å®šç”Ÿæˆï¼ˆKOTOHAäººæ ¼ã‚¨ãƒ³ã‚¸ãƒ³é€£æºï¼‰
- å…¥åŠ›: /Users/tsuyoshi/Desktop/python_lesson/sauce/rakuten.csvï¼ˆUTF-8, ãƒ˜ãƒƒãƒ€ã«ã€Œå•†å“åã€ï¼‰
- çŸ¥è¦‹: ./output/semantics/*.json ã‚’ç·©ã‚„ã‹çµ±åˆï¼ˆlist/dictæ··åœ¨ã«å …ç‰¢ï¼‰
- äººæ ¼: ./config/kotoha_persona.jsonï¼ˆç’°å¢ƒå¤‰æ•° KOTOHA_PERSONA=on ã§æœ‰åŠ¹ï¼‰
- å‡ºåŠ›:
    1) output/ai_writer/alt_text_ai_raw_salescopy_v5_3.csv
    2) output/ai_writer/alt_text_refined_salescopy_v5_3.csv
    3) output/ai_writer/alt_text_diff_salescopy_v5_3.csv
- ãƒ¢ãƒ‡ãƒ«/æ¸©åº¦/ãƒˆãƒ¼ã‚¯ãƒ³: .envã‚’å®Œå…¨æº–æ‹ ï¼ˆOPENAI_MODEL, OPENAI_TEMPERATURE, OPENAI_MAX_TOKENSï¼‰
- å®‰å®šåŒ–: JSONéä¾å­˜ / response_format={"type":"text"} / backoffãƒªãƒˆãƒ©ã‚¤ / æ¬ æè£œå®Œ / é‡è¤‡æŠ‘æ­¢
"""

import os
import re
import csv
import glob
import json
import time
import math
from collections import defaultdict

from dotenv import load_dotenv

# é€²æ—
try:
    from tqdm.auto import tqdm
except Exception:
    def tqdm(x, **k): return x

# OpenAI SDKï¼ˆæ–°ï¼‰
try:
    from openai import OpenAI
except Exception:
    raise SystemExit("âŒ openai SDK ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚`pip install openai python-dotenv` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

# ==========
# å®šæ•°ç¾¤
# ==========
BASE_DIR = os.path.abspath(os.path.dirname(__file__))

INPUT_CSV = "/Users/tsuyoshi/Desktop/python_lesson/sauce/rakuten.csv"  # å›ºå®š
OUT_DIR   = os.path.join(BASE_DIR, "output", "ai_writer")
RAW_PATH  = os.path.join(OUT_DIR, "alt_text_ai_raw_salescopy_v5_3.csv")
REF_PATH  = os.path.join(OUT_DIR, "alt_text_refined_salescopy_v5_3.csv")
DIFF_PATH = os.path.join(OUT_DIR, "alt_text_diff_salescopy_v5_3.csv")

SEMANTICS_DIR = os.path.join(BASE_DIR, "output", "semantics")
PERSONA_PATH  = os.path.join(BASE_DIR, "config", "kotoha_persona.json")

# ç”Ÿæˆãƒ¬ãƒ³ã‚¸ã¨æ•´å½¢ãƒ¬ãƒ³ã‚¸
RAW_MIN, RAW_MAX   = 100, 130   # AIã¯ã¾ãšã“ã®é•·ã•ã‚’ç‹™ã†
FINAL_MIN, FINAL_MAX = 80, 110  # ãƒ­ãƒ¼ã‚«ãƒ«ã§ã“ã®ç¯„å›²ã«åã‚ã‚‹

# ç¦å‰‡ï¼ˆç”»åƒæå†™èªãƒ»ECãƒ¡ã‚¿èªãƒ»èª‡å¤§ï¼‰
FORBIDDEN = [
    "ç”»åƒ", "å†™çœŸ", "è¦‹ãŸç›®", "ä¸Šã®ç”»åƒ", "ä¸‹ã®å†™çœŸ", "ã‚¤ãƒ¡ãƒ¼ã‚¸å›³",
    "å½“åº—", "å½“ç¤¾", "ãƒ¬ãƒ“ãƒ¥ãƒ¼", "ãƒ©ãƒ³ã‚­ãƒ³ã‚°", "ã‚¯ãƒªãƒƒã‚¯", "ã“ã¡ã‚‰", "ãƒªãƒ³ã‚¯", "è³¼å…¥ã¯ã“ã¡ã‚‰",
    "æœ€å®‰", "No.1", "ãƒŠãƒ³ãƒãƒ¼ãƒ¯ãƒ³", "å£²ä¸ŠNo1", "æ¥­ç•Œæœ€é«˜", "ç«¶åˆ", "ç«¶åˆå„ªä½æ€§", "è¿”é‡‘ä¿è¨¼"
]

# æ­£è¦è¡¨ç¾
LEADING_ENUM_RE = re.compile(r"^\s*[\dâ‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©\-\*\ãƒ»\u2022]\s*[\.ï¼ã€]?\s*")
MULTI_COMMA_RE  = re.compile(r"ã€{3,}")
SPACE_RE        = re.compile(r"\s+")
EXTRA_BRACKETS  = re.compile(r"[ã€ã€‘\[\]]")  # ä¸è¦ãªè£…é£¾æ‹¬å¼§ã‚’è»½ãé™¤å»

# ä½“è¨€æ­¢ã‚ã‚’è¨±å¯ï¼ˆå¼·åˆ¶ã§ã¯ãªã„ï¼‰
ALLOW_TAIGEN = True

# =================
# 0) ç’°å¢ƒ/ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
# =================
def init_env_and_client():
    load_dotenv(override=True)
    api_key = os.getenv("OPENAI_API_KEY", "").strip()
    if not api_key:
        raise SystemExit("âŒ OPENAI_API_KEY ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚.env ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    model = (os.getenv("OPENAI_MODEL") or "gpt-5").strip()
    temperature = float(os.getenv("OPENAI_TEMPERATURE") or "0.9")
    max_tokens  = int(os.getenv("OPENAI_MAX_TOKENS") or "1500")
    mode = (os.getenv("OPENAI_MODE") or "chat").strip().lower()  # äº’æ›
    persona_switch = (os.getenv("KOTOHA_PERSONA") or "on").strip().lower() in ("on", "true", "1", "yes")
    client = OpenAI(api_key=api_key)
    return client, model, temperature, max_tokens, mode, persona_switch

# =================
# 1) ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# =================
def load_products(path: str):
    if not os.path.exists(path):
        raise SystemExit(f"âŒ å…¥åŠ›CSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path}")
    products = []
    with open(path, "r", encoding="utf-8", newline="") as f:
        reader = csv.DictReader(f)
        if "å•†å“å" not in reader.fieldnames:
            raise SystemExit("âŒ å…¥åŠ›CSVã«ã€å•†å“åã€ãƒ˜ãƒƒãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        for r in reader:
            nm = (r.get("å•†å“å") or "").strip()
            if nm:
                products.append(nm)
    # é †åºç¶­æŒã®é‡è¤‡é™¤å»
    seen, uniq = set(), []
    for nm in products:
        if nm not in seen:
            uniq.append(nm)
            seen.add(nm)
    return uniq

# =================
# 2) äººæ ¼ãƒ­ãƒ¼ãƒ‰
# =================
def load_persona(path: str, enabled: bool):
    if not enabled:
        return None
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        # æœŸå¾…ã‚­ãƒ¼: core_values, tone, style, seo, guardrails
        return data
    except Exception:
        return None

def build_persona_system(persona):
    """
    KOTOHAäººæ ¼ â†’ SYSTEMãƒ¡ãƒƒã‚»ãƒ¼ã‚¸åŒ–
    """
    if not persona:
        return None
    core = persona.get("core_values") or {}
    tone = persona.get("tone") or {}
    style = persona.get("style") or {}
    seo = persona.get("seo") or {}
    guard = persona.get("guardrails") or {}

    parts = []
    parts.append("ã‚ãªãŸã¯ã€SEOã«å¼·ã„æ—¥æœ¬èªã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã€ã§ã™ã€‚æ¥½å¤©ã®ã‚µã‚¤ãƒˆå†…SEOã‚’æœ€å¤§åŒ–ã—ã¤ã¤ã€è‡ªç„¶ã§èª­ã¿ã‚„ã™ã„æ–‡ç« ã‚’ä½œæˆã—ã¾ã™ã€‚")
    if core:
        parts.append(f"ä¿¡æ¡: {', '.join([f'{k}:{v}' for k,v in core.items() if isinstance(v,str)])}")
    if tone:
        parts.append(f"ãƒˆãƒ¼ãƒ³: {', '.join([f'{k}:{v}' for k,v in tone.items() if isinstance(v,str)])}")
    if style:
        parts.append(f"æ–‡ä½“: {', '.join([f'{k}:{v}' for k,v in style.items() if isinstance(v,str)])}")
    if seo:
        parts.append(f"SEOæŒ‡é‡: {', '.join([f'{k}:{v}' for k,v in seo.items() if isinstance(v,str)])}")
    if guard:
        parts.append(f"ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«: {', '.join([f'{k}:{v}' for k,v in guard.items() if isinstance(v,str)])}")
    parts.append("å‡ºåŠ›ã¯20è¡Œã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆå„è¡Œ1ã€œ2æ–‡ã®è‡ªç„¶æ–‡ã€å¥ç‚¹ã€Œã€‚ã€ã§çµ‚ãˆã‚‹ã€JSONã‚„ãƒ©ãƒ™ãƒ«ãªã—ï¼‰ã€‚")
    return " ".join(parts)

# =================
# 3) çŸ¥è¦‹ã‚µãƒãƒª
# =================
def safe_load_json(path):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None

def summarize_knowledge():
    """
    ./output/semantics/*.json ã‚’ç·©ã‚„ã‹ã«å¸åã—ã€ãƒ†ã‚­ã‚¹ãƒˆçŸ¥è¦‹ã¨è¿½åŠ ç¦å‰‡ã‚’è¿”ã™ã€‚
    list/dict æ··åœ¨ãƒ»ã‚­ãƒ¼æºã‚Œã«å …ç‰¢ã€‚
    """
    if not os.path.isdir(SEMANTICS_DIR):
        base = "çŸ¥è¦‹: å•†å“åãƒ»å¯¾å¿œæ©Ÿç¨®ãƒ»ã‚¹ãƒšãƒƒã‚¯ãƒ»æ©Ÿèƒ½ãƒ»ç”¨é€”ãƒ»å¯¾è±¡ãƒ»ãƒ™ãƒãƒ•ã‚£ãƒƒãƒˆã‚’è‡ªç„¶ã«ç¹”ã‚Šè¾¼ã¿ã€ç”»åƒæå†™èªã¯ä½¿ã‚ãšã€2æ–‡ä»¥å†…ã€å¥ç‚¹çµ‚æ­¢ã€‚"
        return base, []

    files = glob.glob(os.path.join(SEMANTICS_DIR, "*.json"))
    clusters, market, semantics, persona_tone, template = [], [], [], [], []
    forbidden_local = []

    def flatten(x):
        if isinstance(x, list):
            for v in x:
                if isinstance(v, str):
                    yield v
                elif isinstance(v, dict):
                    for vv in flatten(list(v.values())):
                        yield vv
        elif isinstance(x, dict):
            for vv in flatten(list(x.values())):
                yield vv

    for p in files:
        name = os.path.basename(p).lower()
        data = safe_load_json(p)
        if data is None:
            continue
        try:
            if "lexical" in name:
                # ä¾‹: {"clusters":[{"terms":[...]}]} / [{"terms":[...]}] / ["èª","å½™"]
                if isinstance(data, dict):
                    arr = data.get("clusters") or data.get("lexical") or []
                else:
                    arr = data
                for c in flatten(arr):
                    if isinstance(c, str):
                        clusters.append(c)
            elif "market_vocab" in name or "market" in name:
                for v in flatten(data):
                    if isinstance(v, str):
                        market.append(v)
            elif "structured_semantics" in name or "semantic" in name:
                # ä¾‹: {"concepts":[...], "scenes":[...], "targets":[...], "use_cases":[...]}
                if isinstance(data, dict):
                    for k in ["concepts", "semantics", "frames", "features", "facets", "benefits", "targets", "scenes", "use_cases"]:
                        for v in data.get(k, []) or []:
                            if isinstance(v, str):
                                semantics.append(v)
                else:
                    for v in flatten(data):
                        if isinstance(v, str):
                            semantics.append(v)
            elif "styled_persona" in name or "persona" in name:
                if isinstance(data, dict):
                    t = data.get("tone") or {}
                    for v in t.values():
                        if isinstance(v, str):
                            persona_tone.append(v)
                else:
                    for v in flatten(data):
                        if isinstance(v, str):
                            persona_tone.append(v)
            elif "normalized" in name or "forbid" in name:
                if isinstance(data, dict):
                    fw = data.get("forbidden_words") or []
                    for w in fw:
                        if isinstance(w, str):
                            forbidden_local.append(w)
            elif "template_composer" in name or "template" in name:
                for v in flatten(data):
                    if isinstance(v, str):
                        template.append(v)
        except Exception:
            # å½¢å¼ä¸ä¸€è‡´ã¯ç„¡è¦–ã—ã¦ç¶šè¡Œ
            pass

    # ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ã—ã¦è©°ã‚ã™ããªã„
    cap = lambda xs, n: [x for i, x in enumerate(xs) if isinstance(x, str) and xs.index(x) == i][:n]
    clusters = cap(clusters, 15)
    market   = cap(market,   15)
    semantics= cap(semantics,10)
    persona_tone = cap(persona_tone, 6)
    template = cap(template, 4)

    parts = []
    if clusters: parts.append("èªå½™: " + "ã€".join(clusters))
    if market:   parts.append("å¸‚å ´èª: " + "ã€".join(market))
    if semantics:parts.append("æ§‹é€ : " + "ã€".join(semantics))
    if template: parts.append("éª¨å­: " + "ã€".join(template))
    if persona_tone: parts.append("ãƒˆãƒ¼ãƒ³: " + "ã€".join(persona_tone))

    text = "çŸ¥è¦‹: "
    if parts:
        text += " / ".join(parts) + "ã€‚"
    text += "ç”»åƒæå†™èªã¯ä½¿ã‚ãšã€æ¥½å¤©ã®ã‚µã‚¤ãƒˆå†…SEOã«åŠ¹ãè‡ªç„¶æ–‡ã§ã€å„è¡Œã¯1ã€œ2æ–‡ã€å¥ç‚¹ã§çµ‚ãˆã‚‹ã€‚"
    return text, list({*FORBIDDEN, *forbidden_local})

# =================
# 4) ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
# =================
BASE_SYSTEM = (
    "ã‚ãªãŸã¯ã€SEOã«å¼·ã„æ—¥æœ¬èªã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã€ã§ã™ã€‚"
    "æ¥½å¤©ã®ã‚µã‚¤ãƒˆå†…SEOã‚’æœ€å¤§åŒ–ã—ã¤ã¤ã€è‡ªç„¶ã§èª­ã¿ã‚„ã™ã„ALTãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚"
    "å‡ºåŠ›ã¯20è¡Œã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆå„è¡Œ1ã€œ2æ–‡ã€å¥ç‚¹ã€Œã€‚ã€ã§çµ‚ãˆã‚‹ã€JSONã‚„ãƒ©ãƒ™ãƒ«ãªã—ï¼‰ã€‚"
)

def build_user_prompt(product: str, knowledge_text: str, forbidden_words):
    forbid_txt = "ã€".join(sorted(set([w for w in forbidden_words if isinstance(w, str)])))
    hint = (
        "æ§‹æˆãƒ’ãƒ³ãƒˆï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ã§ã¯ãªãè‡ªç„¶ã«ï¼‰ï¼š"
        "å•†å“ã‚¹ãƒšãƒƒã‚¯â†’ã‚³ã‚¢ã‚³ãƒ³ãƒ”ã‚¿ãƒ³ã‚¹â†’ã©ã‚“ãªäººâ†’ã‚·ãƒ¼ãƒ³â†’ãƒ™ãƒãƒ•ã‚£ãƒƒãƒˆã€‚"
    )
    rules = (
        f"ç¦æ­¢èªï¼ˆå¿…ãšä½¿ã‚ãªã„ï¼‰: {forbid_txt}\n"
        f"å„è¡Œã¯å…¨è§’ç´„{RAW_MIN}ã€œ{RAW_MAX}æ–‡å­—ã€‚ä½“è¨€æ­¢ã‚ã¯å¿…è¦ã«å¿œã˜ã¦å¯ã€‚"
    )
    return (
        f"å•†å“å: {product}\n"
        f"{knowledge_text}\n"
        f"{hint}\n"
        f"{rules}\n"
        "20è¡Œã§å‡ºåŠ›ã€‚å„è¡Œã¯è‡ªç„¶æ–‡ï¼ˆ1ã€œ2æ–‡ï¼‰ã§ã€å¥ç‚¹ã€Œã€‚ã€ã§çµ‚ãˆã‚‹ã“ã¨ã€‚"
    )

# =================
# 5) OpenAI å‘¼ã³å‡ºã—
# =================
def call_openai_20_lines(client, model, temperature, max_tokens, system_prompt, user_prompt, retry=3, wait=6):
    last_err = None
    for _ in range(retry):
        try:
            res = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user",   "content": user_prompt},
                ],
                response_format={"type": "text"},
                temperature=temperature,
                max_completion_tokens=max_tokens,
            )
            content = (res.choices[0].message.content or "").strip()
            if content:
                lines = [LEADING_ENUM_RE.sub("", ln).strip("ãƒ»-â€”â—ã€€").strip() for ln in content.split("\n")]
                lines = [EXTRA_BRACKETS.sub("", ln) for ln in lines if ln.strip()]
                return lines[:80]  # å¿µã®ãŸã‚å¤šã‚ã«ä¿æŒ
        except Exception as e:
            last_err = e
            time.sleep(wait)
    raise RuntimeError(f"OpenAIå¿œç­”ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ: {last_err}")

# =================
# 6) ãƒ­ãƒ¼ã‚«ãƒ«æ•´å½¢
# =================
def soft_clip_sentence(text: str, min_len=FINAL_MIN, max_len=FINAL_MAX) -> str:
    t = (text or "").strip()
    if not t:
        return ""
    # ç•°å¸¸ãªç•ªå·ãƒ»ç®‡æ¡æ›¸ãå‰¥ãŒã—
    t = LEADING_ENUM_RE.sub("", t).strip("ãƒ»-â€”â—ã€€")
    # ã‚¹ãƒšãƒ¼ã‚¹åœ§ç¸® & é€£ç¶šèª­ç‚¹ç¸®ç´„
    t = SPACE_RE.sub(" ", t)
    t = MULTI_COMMA_RE.sub("ã€ã€", t)
    # ç¦å‰‡èªã®å®Œå…¨é™¤å»ï¼ˆå˜ç´”ç½®æ›ï¼‰
    for ng in FORBIDDEN:
        if ng and ng in t:
            t = t.replace(ng, "")
    t = t.strip()

    # å¥ç‚¹çµ‚æ­¢ã‚’å¼·åˆ¶ã€‚ãŸã ã—ä½“è¨€æ­¢ã‚è¨±å®¹ã®å ´åˆã¯ã€2æ–‡ä»¥å†…ã§å¥ç‚¹ãƒŠã‚·ã‚‚æ®‹ã™ãŒæœ€çµ‚1æ–‡ã¯åŸºæœ¬ã€Œã€‚ã€ã§ç· ã‚ã‚‹
    if not ALLOW_TAIGEN or ("ã€‚" not in t):
        if not t.endswith("ã€‚"):
            t += "ã€‚"

    # 120è¶…ãªã‚‰æ–‡æœ«ã€Œã€‚ã€ã¾ã§ã‚«ãƒƒãƒˆ
    if len(t) > 120:
        cut = t[:120]
        pos = cut.rfind("ã€‚")
        t = cut[:pos+1] if pos != -1 else cut

    return t.strip()

def is_sentence_like(s: str) -> bool:
    if not s: return False
    # 10æ–‡å­—æœªæº€ã¯çŸ­ã™ã
    if len(s) < 10:
        return False
    # å¥ç‚¹ã‚„åŠ©è©ãŒã¾ã‚‹ã§ç„¡ã„å˜èªç¾…åˆ—ã¯è½ã¨ã™
    has_punct = ("ã€‚" in s) or ("ã€" in s)
    has_particle = any(p in s for p in ["ã‚’", "ã«", "ã§", "ãŒ", "ã¨", "ã‚‚", "ã¸", "ã‚ˆã‚Š", "ã‹ã‚‰"])
    return has_punct or has_particle

def de_duplicate(lines):
    uniq, seen = [], set()
    for ln in lines:
        key = ln
        if key not in seen:
            uniq.append(ln)
            seen.add(key)
    return uniq

def light_variation(s: str) -> str:
    if not s: return s
    s2 = s
    # èªå°¾ã®è»½ã„ãƒ–ãƒ¬ã§é‡è¤‡å›é¿
    s2 = s2.replace("ã—ã¾ã™ã€‚", "ã§ãã¾ã™ã€‚")
    s2 = s2.replace("ã§ãã¾ã™ã€‚", "ã—ã‚„ã™ã„ã§ã™ã€‚")
    s2 = s2.replace("ã§ã™ã€‚", "ã«ãªã‚Šã¾ã™ã€‚")
    if s2 == s:
        s2 = re.sub(r"([^\sã€ã€‚]{2,})", r"\1ã€", s, count=1)
        s2 = s2.replace("ã€ã€", "ã€")
        if not s2.endswith("ã€‚"):
            s2 += "ã€‚"
    return soft_clip_sentence(s2)

def fallback_template(product: str) -> list:
    """
    æ¬ æ/çŸ­æ–‡ç”¨ã®2æ–‡ãƒ†ãƒ³ãƒ—ãƒ¬ï¼ˆè‡ªç„¶æ–‡ãƒ™ãƒ¼ã‚¹ï¼‰
    """
    base = [
        f"{product}ã®æ©Ÿèƒ½ã¨ã‚¹ãƒšãƒƒã‚¯ã‚’æ´»ã‹ã—ã€æ—¥å¸¸ã®ä¸ä¾¿ã‚’æ¸›ã‚‰ã™è¨­è¨ˆã€‚ä½¿ã„ã‚„ã™ã•ã¨è€ä¹…æ€§ã‚’ä¸¡ç«‹ã—ã€å¹…åºƒã„æ©Ÿç¨®ã§å¿«é©ã«ä½¿ãˆã¾ã™ã€‚",
        f"{product}ã¯ãƒ“ã‚¸ãƒã‚¹ã‹ã‚‰æ™®æ®µä½¿ã„ã¾ã§ãƒãƒ«ãƒã«æ´»èºã€‚è£…ç€ã‚„è¨­å®šãŒç°¡å˜ã§ã€æŒã¡é‹ã³ã‚„ã™ãã€æ¯æ—¥ã®å°ã•ãªæ‰‹é–“ã‚’æ¸›ã‚‰ã—ã¾ã™ã€‚",
        f"{product}ã¯é«˜ã„äº’æ›æ€§ã¨å®‰å®šæ€§ãŒç‰¹é•·ã€‚è¤‡æ•°ãƒ‡ãƒã‚¤ã‚¹ã®åˆ‡æ›¿ãˆã‚„å¤–å‡ºå…ˆã§ã‚‚ã‚¹ãƒ ãƒ¼ã‚ºã«ä½¿ãˆã€ä»•äº‹ã¨ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã®ä¸¡ç«‹ã‚’æ”¯ãˆã¾ã™ã€‚",
        f"{product}ã¯è»½é‡ãƒ»ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆãªã†ãˆè€ä¹…æ€§ã«ã‚‚é…æ…®ã€‚è‡ªå®…ã‚„ã‚ªãƒ•ã‚£ã‚¹ã€æ—…è¡Œå…ˆã§ã‚‚å–ã‚Šå›ã—ãŒè‰¯ãã€ã‚¹ãƒˆãƒ¬ã‚¹ãªãä½¿ãˆã¾ã™ã€‚",
    ]
    return [soft_clip_sentence(x) for x in base]

def refine_block(raw_lines, product: str):
    # æ­£è¦åŒ– â†’ è‡ªç„¶æ–‡ãƒ•ã‚£ãƒ«ã‚¿ â†’ é‡è¤‡é™¤å» â†’ é•·ã•æ•´å½¢
    norm = []
    for ln in raw_lines:
        if not ln: 
            continue
        ln = EXTRA_BRACKETS.sub("", ln)
        ln = soft_clip_sentence(ln)
        if is_sentence_like(ln):
            norm.append(ln)

    # é‡è¤‡é™¤å»
    norm = de_duplicate(norm)

    # è¶³ã‚Šãªã‘ã‚Œã°ãƒ†ãƒ³ãƒ—ãƒ¬è£œå®Œï¼ˆ2æ–‡å˜ä½ï¼‰
    i = 0
    while len(norm) < 20:
        fb = fallback_template(product)
        # è»½ã„ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¶³ã™
        norm.extend([light_variation(x) for x in fb])
        norm = de_duplicate(norm)
        i += 1
        if i > 5:  # ç„¡é™æ‹¡å¼µé˜²æ­¢
            break

    # å¤šã™ãã‚Œã°å…ˆé ­20æœ¬
    return norm[:20]

# =================
# 7) æ›¸ãå‡ºã—
# =================
def ensure_outdir():
    os.makedirs(OUT_DIR, exist_ok=True)

def write_raw(products, all_raw):
    with open(RAW_PATH, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["å•†å“å"] + [f"ALT_raw_{i+1}" for i in range(20)])
        for p, lines in zip(products, all_raw):
            row = [p] + (lines[:20] + [""] * max(0, 20 - len(lines)))
            w.writerow(row)

def write_refined(products, all_refined):
    with open(REF_PATH, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["å•†å“å"] + [f"ALT_{i+1}" for i in range(20)])
        for p, lines in zip(products, all_refined):
            row = [p] + (lines[:20] + [""] * max(0, 20 - len(lines)))
            w.writerow(row)

def write_diff(products, all_raw, all_refined):
    with open(DIFF_PATH, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        header = ["å•†å“å"] + [f"ALT_raw_{i+1}" for i in range(20)] + [f"ALT_refined_{i+1}" for i in range(20)]
        w.writerow(header)
        for p, r, ref in zip(products, all_raw, all_refined):
            r_line = (r[:20] + [""] * max(0, 20 - len(r)))
            ref_line = (ref[:20] + [""] * max(0, 20 - len(ref)))
            w.writerow([p] + r_line + ref_line)

# =================
# 8) ãƒ¡ã‚¤ãƒ³
# =================
def main():
    print("ğŸŒ¸ ALTç”Ÿæˆ v5.3ï¼ˆSEOè‡ªç„¶æ–‡ï¼‹KOTOHAäººæ ¼ï¼‹çŸ¥è¦‹é€£æºï¼‰")
    client, model, temperature, max_tokens, mode, persona_on = init_env_and_client()
    ensure_outdir()

    # äººæ ¼
    persona = load_persona(PERSONA_PATH, persona_on)
    system_prompt = build_persona_system(persona) if persona else BASE_SYSTEM

    # çŸ¥è¦‹
    kb_text, forbidden_all = summarize_knowledge()

    # å•†å“
    products = load_products(INPUT_CSV)
    print(f"âœ… å¯¾è±¡å•†å“: {len(products)}ä»¶")

    all_raw, all_refined = [], []

    for p in tqdm(products, desc="ğŸ§  ç”Ÿæˆä¸­", total=len(products)):
        user_prompt = build_user_prompt(p, kb_text, forbidden_all)
        try:
            raw_lines = call_openai_20_lines(
                client, model, temperature, max_tokens,
                system_prompt, user_prompt, retry=3, wait=6
            )
        except Exception:
            # ãƒ€ã‚¦ãƒ³æ™‚ã®ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•
            raw_lines = fallback_template(p) * 5

        refined = refine_block(raw_lines, p)

        all_raw.append(raw_lines[:20])
        all_refined.append(refined)

        time.sleep(0.2)  # è»½ã„ã‚¹ãƒ­ãƒƒãƒˆãƒªãƒ³ã‚°

    # æ›¸ãå‡ºã—
    write_raw(products, all_raw)
    write_refined(products, all_refined)
    write_diff(products, all_raw, all_refined)

    def avg_len(blocks):
        lens = [len(x) for lines in blocks for x in lines if x]
        return (sum(lens) / max(1, len(lens))) if lens else 0.0

    print("âœ… å‡ºåŠ›å®Œäº†:")
    print(f"   - AIç”Ÿå‡ºåŠ›: {RAW_PATH}")
    print(f"   - æ•´å½¢å¾Œ   : {REF_PATH}")
    print(f"   - å·®åˆ†æ¯”è¼ƒ : {DIFF_PATH}")
    print(f"ğŸ“ æ–‡å­—æ•°(å¹³å‡): raw={avg_len(all_raw):.1f} / refined={avg_len(all_refined):.1f}")
    print("ğŸ”’ ä»•æ§˜:")
    print(f"   - AIã¯ç´„{RAW_MIN}ã€œ{RAW_MAX}å­—ãƒ»1ã€œ2æ–‡ã€å¥ç‚¹çµ‚æ­¢ã€ç¦å‰‡é©ç”¨ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰")
    print(f"   - ãƒ­ãƒ¼ã‚«ãƒ«æ•´å½¢ã§{FINAL_MIN}ã€œ{FINAL_MAX}å­—ã«è‡ªç„¶ã‚«ãƒƒãƒˆã€é‡è¤‡æŠ‘æ­¢ãƒ»æ¬ æè£œå®Œãƒ»èªå°¾ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³")

if __name__ == "__main__":
    main()
import atlas_autosave_core
