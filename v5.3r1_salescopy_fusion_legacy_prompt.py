# -*- coding: utf-8 -*-
"""
v5.3r1_salescopy_fusion_legacy_prompt.py
- æ¥½å¤©ALTç”Ÿæˆï¼ˆv3.3è‡ªç„¶æ–‡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ Ã— v5.3å®‰å®šæ§‹é€ ï¼‰
- å¥ç‚¹çµ‚æ­¢ãƒ»ç¦å‰‡ãƒ»ä½“è¨€æ­¢ã‚å¯ãƒ»æ–‡æœ«å¼•ç”¨ç¬¦ç¦æ­¢
"""

import os
import re
import csv
import glob
import json
import time
from dotenv import load_dotenv
from openai import OpenAI

try:
    from tqdm.auto import tqdm
except Exception:
    def tqdm(x, **k): return x

# =========================
# å®šæ•°
# =========================
BASE_DIR = os.path.abspath(os.path.dirname(__file__))
INPUT_CSV = "/Users/tsuyoshi/Desktop/python_lesson/sauce/rakuten.csv"
OUT_DIR   = os.path.join(BASE_DIR, "output", "ai_writer")

RAW_PATH  = os.path.join(OUT_DIR, "alt_text_ai_raw_salescopy_v5_3r1.csv")
REF_PATH  = os.path.join(OUT_DIR, "alt_text_refined_salescopy_v5_3r1.csv")
DIFF_PATH = os.path.join(OUT_DIR, "alt_text_diff_salescopy_v5_3r1.csv")

SEMANTICS_DIR = os.path.join(BASE_DIR, "output", "semantics")
FORBIDDEN = [
    "ç”»åƒ", "å†™çœŸ", "è¦‹ãŸç›®", "ä¸Šã®ç”»åƒ", "ä¸‹ã®å†™çœŸ", "ã‚¤ãƒ¡ãƒ¼ã‚¸å›³",
    "å½“åº—", "å½“ç¤¾", "ãƒ¬ãƒ“ãƒ¥ãƒ¼", "ãƒ©ãƒ³ã‚­ãƒ³ã‚°", "ã‚¯ãƒªãƒƒã‚¯", "ã“ã¡ã‚‰", "ãƒªãƒ³ã‚¯", "è³¼å…¥ã¯ã“ã¡ã‚‰",
    "æœ€å®‰", "No.1", "ãƒŠãƒ³ãƒãƒ¼ãƒ¯ãƒ³", "å£²ä¸ŠNo1", "æ¥­ç•Œæœ€é«˜", "ç«¶åˆ", "ç«¶åˆå„ªä½æ€§", "è¿”é‡‘ä¿è¨¼"
]
RAW_MIN, RAW_MAX = 100, 130
FINAL_MIN, FINAL_MAX = 80, 110

LEADING_ENUM_RE = re.compile(r"^\s*[\dâ‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©\-\*\ãƒ»\u2022]\s*[\.ï¼Žã€]?\s*")
SPACE_RE = re.compile(r"\s+")
MULTI_COMMA_RE = re.compile(r"ã€{3,}")

# =========================
# ç’°å¢ƒãƒ­ãƒ¼ãƒ‰
# =========================
def init_client():
    load_dotenv(override=True)
    key = os.getenv("OPENAI_API_KEY", "").strip()
    if not key:
        raise SystemExit("âŒ OPENAI_API_KEY ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    model = os.getenv("OPENAI_MODEL", "gpt-5")
    temp = float(os.getenv("OPENAI_TEMPERATURE", "0.9"))
    max_t = int(os.getenv("OPENAI_MAX_TOKENS", "1500"))
    client = OpenAI(api_key=key)
    return client, model, temp, max_t

# =========================
# å…¥åŠ›
# =========================
def load_products(path):
    if not os.path.exists(path):
        raise SystemExit(f"âŒ å…¥åŠ›CSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path}")
    products = []
    with open(path, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for r in reader:
            nm = (r.get("å•†å“å") or "").strip()
            if nm:
                products.append(nm)
    seen, uniq = set(), []
    for n in products:
        if n not in seen:
            uniq.append(n); seen.add(n)
    return uniq

# =========================
# çŸ¥è¦‹ï¼ˆã‚†ã‚‹è¦ç´„ï¼‰
# =========================
def safe_load_json(p):
    try:
        with open(p, "r", encoding="utf-8") as f: return json.load(f)
    except Exception: return None

def summarize_knowledge():
    if not os.path.isdir(SEMANTICS_DIR):
        return "çŸ¥è¦‹: ã‚¹ãƒšãƒƒã‚¯ãƒ»æ©Ÿèƒ½ãƒ»ç”¨é€”ãƒ»å¯¾è±¡ã‚’è‡ªç„¶ã«å«ã‚ã€å¥ç‚¹çµ‚æ­¢ãƒ»ç”»åƒèªžç¦æ­¢ã€‚", []

    files = glob.glob(os.path.join(SEMANTICS_DIR, "*.json"))
    words = []
    forb = []
    for p in files:
        data = safe_load_json(p)
        if not data: continue
        try:
            if isinstance(data, dict):
                for v in data.values():
                    if isinstance(v, list):
                        words.extend([x for x in v if isinstance(x, str)])
                    elif isinstance(v, str):
                        words.append(v)
            elif isinstance(data, list):
                words.extend([x for x in data if isinstance(x, str)])
        except Exception:
            pass
    words = list(dict.fromkeys(words))
    return "çŸ¥è¦‹: " + "ã€".join(words[:20]) + "ã€‚", list(set(FORBIDDEN + forb))

# =========================
# SYSTEM / USER ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆv3.3å¾©å…ƒï¼‹æ–‡æœ«å¼•ç”¨ç¬¦ç¦æ­¢ï¼‰
# =========================
SYSTEM_PROMPT = (
    "ã‚ãªãŸã¯æ¥½å¤©ã®SEOã«å¼·ã„æ—¥æœ¬èªžã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚"
    "ç›®çš„ã¯å•†å“ç”»åƒã®ALTãƒ†ã‚­ã‚¹ãƒˆã‚’è‡ªç„¶ãªæ—¥æœ¬èªžã§20æœ¬ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã™ã€‚"
    "ä»¥ä¸‹ã®æ¡ä»¶ã‚’åŽ³å®ˆã—ã¦ãã ã•ã„ï¼š\n"
    "ãƒ»ç”»åƒã‚„å†™çœŸã®æå†™èªžã‚’ä½¿ã‚ãªã„ã€‚\n"
    "ãƒ»ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ»ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ»ãƒªãƒ³ã‚¯ç­‰ã®ãƒ¡ã‚¿èªžã‚’ä½¿ã‚ãªã„ã€‚\n"
    "ãƒ»èª‡å¤§è¡¨ç¾ã‚„ç«¶åˆæ¯”è¼ƒã¯ç¦æ­¢ã€‚\n"
    "ãƒ»å„æ–‡ã¯å…¨è§’ç´„100ã€œ130æ–‡å­—ã€1ã€œ2æ–‡ã§è‡ªç„¶ã«ã€‚å¥ç‚¹ã€Œã€‚ã€ã§çµ‚ãˆã‚‹ã€‚\n"
    "ãƒ»ç®‡æ¡æ›¸ãã‚„ç•ªå·ã‚„ã€ŒALT:ã€ãªã©ã¯ä»˜ã‘ãªã„ã€‚\n"
    "ãƒ»å•†å“åã€å¯¾å¿œæ©Ÿç¨®ã€æ©Ÿèƒ½ã€ç”¨é€”ã€å¯¾è±¡ã‚’è‡ªç„¶ã«ç¹”ã‚Šè¾¼ã‚€ã€‚\n"
    "ãƒ»å‡ºåŠ›ã¯20è¡Œãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã€‚\n"
    "ãƒ»æ–‡æœ«ã«ã€Œ\"ã€ã‚„ã€Œ'ã€ã‚’ä»˜ã‘ãªã„ã“ã¨ã€‚"
)

def build_user_prompt(product, kb_text, forb):
    forbid_txt = "ã€".join(sorted(forb))
    return (
        f"å•†å“å: {product}\n"
        f"{kb_text}\n"
        f"ç¦æ­¢èªž: {forbid_txt}\n"
        "å„è¡Œã¯è‡ªç„¶ãªæ—¥æœ¬èªžã§1ã€œ2æ–‡æ§‹æˆã€‚å¥ç‚¹ã€Œã€‚ã€ã§çµ‚ãˆã‚‹ã€‚\n"
        "20è¡Œã§å‡ºåŠ›ã€‚"
    )

# =========================
# OpenAI å‘¼ã³å‡ºã—
# =========================
def call_openai_lines(client, model, temp, max_t, sys, usr, retry=3, wait=5):
    last_err = None
    for _ in range(retry):
        try:
            res = client.chat.completions.create(
                model=model,
                messages=[{"role": "system", "content": sys},
                          {"role": "user", "content": usr}],
                response_format={"type": "text"},
                temperature=temp,
                max_completion_tokens=max_t
            )
            content = (res.choices[0].message.content or "").strip()
            if not content:
                continue
            lines = [LEADING_ENUM_RE.sub("", l).strip("ãƒ»-â€”â—ã€€").strip()
                     for l in content.split("\n") if l.strip()]
            return lines[:80]
        except Exception as e:
            last_err = e
            time.sleep(wait)
    raise RuntimeError(f"OpenAIã‚¨ãƒ©ãƒ¼: {last_err}")

# =========================
# æ•´å½¢
# =========================
def soft_clip_sentence(text):
    if not text: return ""
    t = text.strip()
    if not t.endswith("ã€‚"): t += "ã€‚"
    t = SPACE_RE.sub(" ", t)
    t = MULTI_COMMA_RE.sub("ã€ã€", t)
    for ng in FORBIDDEN:
        if ng in t: t = t.replace(ng, "")
    if len(t) > 120:
        cut = t[:120]; p = cut.rfind("ã€‚")
        t = cut[:p+1] if p != -1 else cut
    return t.strip()

def refine_lines(raw, product):
    norm = [soft_clip_sentence(x) for x in raw if len(x.strip()) > 10]
    uniq = []
    for n in norm:
        if n not in uniq:
            uniq.append(n)
    while len(uniq) < 20:
        uniq.append(f"{product}ã®ç‰¹é•·ã‚’æ´»ã‹ã—ãŸè¨­è¨ˆã§ã€æ—¥å¸¸ã‚’å¿«é©ã«ã—ã¾ã™ã€‚")
    return uniq[:20]

# =========================
# æ›¸ãå‡ºã—
# =========================
def ensure_outdir(): os.makedirs(OUT_DIR, exist_ok=True)

def write_csv(path, headers, rows):
    with open(path, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(headers)
        for r in rows:
            w.writerow(r)

# =========================
# ãƒ¡ã‚¤ãƒ³
# =========================
def main():
    print("ðŸŒ¸ ALTç”Ÿæˆ v5.3r1ï¼ˆ3.3ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆÃ—5.3æ§‹é€ ï¼‰")
    client, model, temp, max_t = init_client()
    ensure_outdir()

    kb_text, forb = summarize_knowledge()
    products = load_products(INPUT_CSV)
    print(f"âœ… å•†å“æ•°: {len(products)}ä»¶")

    all_raw, all_ref = [], []

    for p in tqdm(products, desc="ðŸ§  ç”Ÿæˆä¸­", total=len(products)):
        usr = build_user_prompt(p, kb_text, forb)
        try:
            raw = call_openai_lines(client, model, temp, max_t, SYSTEM_PROMPT, usr)
        except Exception:
            raw = [f"{p}ã¯ä½¿ã„ã‚„ã™ã•ã¨è€ä¹…æ€§ã‚’å…¼ã­å‚™ãˆãŸè¨­è¨ˆã§ã™ã€‚"] * 20
        ref = refine_lines(raw, p)
        all_raw.append(raw[:20])
        all_ref.append(ref)
        time.sleep(0.2)

    write_csv(RAW_PATH, ["å•†å“å"] + [f"ALT_raw_{i+1}" for i in range(20)],
              [[p]+r for p, r in zip(products, all_raw)])
    write_csv(REF_PATH, ["å•†å“å"] + [f"ALT_{i+1}" for i in range(20)],
              [[p]+r for p, r in zip(products, all_ref)])
    write_csv(DIFF_PATH,
              ["å•†å“å"] + [f"ALT_raw_{i+1}" for i in range(20)] + [f"ALT_refined_{i+1}" for i in range(20)],
              [[p]+r+f for p, r, f in zip(products, all_raw, all_ref)])

    def avg_len(blocks): 
        lens = [len(x) for lines in blocks for x in lines if x]
        return sum(lens)/max(1,len(lens))

    print("âœ… å‡ºåŠ›å®Œäº†")
    print(f"ðŸ“ å¹³å‡æ–‡å­—æ•°: raw={avg_len(all_raw):.1f}, refined={avg_len(all_ref):.1f}")
    print(f"ðŸ’¾ å‡ºåŠ›å…ˆ:\n - {RAW_PATH}\n - {REF_PATH}\n - {DIFF_PATH}")
    print("ðŸ”’ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: v3.3è‡ªç„¶æ–‡ Ã— æ–‡æœ«å¼•ç”¨ç¬¦ç¦æ­¢")

if __name__ == "__main__":
    main()
import atlas_autosave_core
