# -*- coding: utf-8 -*-
"""
v5.3_salescopy_fusion.py
- ç›®çš„: æ¥½å¤©ALTï¼ˆ20æœ¬/å•†å“ï¼‰ã‚’ã€ŒSEOã«å¼·ã„è‡ªç„¶æ–‡ã€ã§å®‰å®šç”Ÿæˆï¼ˆKOTOHAäººæ ¼ã‚¨ãƒ³ã‚¸ãƒ³é€£æºï¼‰
- å…¥åŠ›: /Users/tsuyoshi/Desktop/python_lesson/sauce/rakuten.csvï¼ˆUTF-8, ãƒ˜ãƒƒãƒ€ã«ã€Œå•†å“åã€ï¼‰
- çŸ¥è¦‹: ./output/semantics/*.json ã‚’ç·©ã‚„ã‹çµ±åˆï¼ˆlist/dictæ··åœ¨ã«å …ç‰¢ï¼‰
- äººæ ¼: ./config/kotoha_persona.jsonï¼ˆç’°å¢ƒå¤‰æ•° KOTOHA_PERSONA=on ã§æœ‰åŠ¹ï¼‰
- å‡ºåŠ›:
    1) output/ai_writer/alt_text_ai_raw_salescopy_v5_3.csv
    2) output/ai_writer/alt_text_refined_salescopy_v5_3.csv
    3) output/ai_writer/alt_text_diff_salescopy_v5_3.csv
- ãƒ¢ãƒ‡ãƒ«/æ¸©åº¦/ãƒˆãƒ¼ã‚¯ãƒ³: .envã‚’å®Œå…¨æº–æ‹ ï¼ˆOPENAI_MODEL, OPENAI_TEMPERATURE, OPENAI_MAX_TOKENSï¼‰
- å®‰å®šåŒ–: JSONéä¾å­˜ / response_format={"type":"text"} / backoffãƒªãƒˆãƒ©ã‚¤ / æ¬ æè£œå®Œ / é‡è¤‡æŠ‘æ­¢
"""

import os
import re
import csv
import json
import time
import math
import glob
import copy
import random
import textwrap
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from dotenv import load_dotenv, find_dotenv  # â† è¿½åŠ 
load_dotenv(find_dotenv(usecwd=True))       # â† è¿½åŠ ï¼ˆcwd ã‹ã‚‰ä¸Šä½ã‚’æ¢ç´¢ï¼‰

# ---------- è¨­å®š ----------
BASE_DIR = Path(os.getcwd())
INPUT_PATH = Path("/Users/tsuyoshi/Desktop/python_lesson/sauce/rakuten.csv")
OUT_DIR = BASE_DIR / "output" / "ai_writer"
OUT_DIR.mkdir(parents=True, exist_ok=True)

RAW_PATH = OUT_DIR / "alt_text_ai_raw_salescopy_v5_3.csv"
REF_PATH = OUT_DIR / "alt_text_refined_salescopy_v5_3.csv"
DIFF_PATH = OUT_DIR / "alt_text_diff_salescopy_v5_3.csv"

SEMANTICS_DIR = BASE_DIR / "output" / "semantics"
PERSONA_PATH = BASE_DIR / "config" / "kotoha_persona.json"

MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
TEMP = float(os.getenv("OPENAI_TEMPERATURE", "0.3"))
MAX_TOKENS = int(os.getenv("OPENAI_MAX_TOKENS", "1200"))

RAW_MIN, RAW_MAX = 120, 130        # AIç›´å‡ºã—ã®æƒ³å®šå­—æ•°
FINAL_MIN, FINAL_MAX = 80, 110     # æ•´å½¢å¾Œã®ç›®æ¨™å­—æ•°

USE_PERSONA = os.getenv("KOTOHA_PERSONA", "off").lower() == "on"

# ---------- OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ ----------
try:
    from openai import OpenAI
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
except Exception:
    client = None

# ---------- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ----------
def load_csv_items(path: Path) -> List[Dict[str, Any]]:
    rows = []
    with open(path, newline="", encoding="utf-8") as f:
        rdr = csv.DictReader(f)
        for r in rdr:
            rows.append(r)
    return rows

def save_csv_rows(path: Path, rows: List[List[str]]):
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        for r in rows:
            w.writerow(r)

def safe_read_json(path: Path) -> Any:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None

def load_semantics() -> List[Any]:
    items = []
    if not SEMANTICS_DIR.exists():
        return items
    for p in sorted(SEMANTICS_DIR.glob("*.json")):
        data = safe_read_json(p)
        if data is None:
            continue
        items.append(data)
    return items

def persona_or_default() -> Dict[str, Any]:
    if USE_PERSONA and PERSONA_PATH.exists():
        p = safe_read_json(PERSONA_PATH)
        if isinstance(p, dict):
            return p
    return {"name": "KOTOHA", "tone": "neutral", "style": "plain"}

# ---------- v5.4: 3.3æº–æ‹ ã®SYSTEMï¼ˆï¼‹æ–‡æœ«ã®å¼•ç”¨ç¬¦ç¦æ­¢ï¼‰ ----------
BASE_SYSTEM = ("""\
ã‚ãªãŸã¯ECãƒ¢ãƒ¼ãƒ«ï¼ˆæ¥½å¤©ãƒ»Yahooï¼‰å°‚é–€ã®æ—¥æœ¬èªã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚
ç”»åƒã®æå†™ã¯ä¸€åˆ‡ã—ã¾ã›ã‚“ã€‚ALTï¼ˆä»£æ›¿ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã¨ã—ã¦è‡ªç„¶ãªæ—¥æœ¬èªã®çŸ­ã„æ–‡ã‚’ä½œæˆã—ã¾ã™ã€‚

ã€ç¦æ­¢ãƒ»åˆ¶ç´„ã€‘
ãƒ»1ã€œ2æ–‡ã®è‡ªç„¶æ–‡ã§æ›¸ãã“ã¨ã€‚å¥ç‚¹ï¼ˆã€‚ ï¼ ï¼Ÿ ã¾ãŸã¯å…¨è§’ã®å¯¾å¿œè¨˜å·ï¼‰ã§çµ‚ãˆã‚‹ã“ã¨ã€‚
ãƒ»çµµæ–‡å­—ãƒ»é¡”æ–‡å­—ãƒ»æ©Ÿç¨®ä¾å­˜æ–‡å­—ãƒ»è£…é£¾è¨˜å·ãƒ»HTMLã‚¿ã‚°ã¯ç¦æ­¢ã€‚
ãƒ»æ¯”è¼ƒåºƒå‘Šãƒ»ç«¶åˆå„ªä½ã®è¡¨ç¾ï¼ˆã€Œä»–ç¤¾ã‚ˆã‚Šã€ã€Œåœ§å€’çš„ã€ã€ŒNo.1ã€ç­‰ï¼‰ã¯ä½¿ã‚ãªã„ã€‚
ãƒ»ã€Œç”»åƒã€ã€Œå†™çœŸã€ã€Œæ˜ ã£ã¦ã„ã‚‹ã€ã€Œã‚¯ãƒªãƒƒã‚¯ã€ãªã©ã®ç”»åƒæ“ä½œ/è¦–è¦šãƒ¡ã‚¿èªã¯ä½¿ã‚ãªã„ã€‚
ãƒ»è£½å“ãƒ»å‹ç•ªãƒ»ç´ æãªã©å…·ä½“æƒ…å ±ã¯è‡ªç„¶ã«æ–‡ä¸­ã¸ç¹”ã‚Šè¾¼ã¿ã€ç¾…åˆ—ã«ã—ãªã„ã€‚
ãƒ»ãƒ–ãƒ©ãƒ³ãƒ‰ã‚„ä»•æ§˜ã«äº‹å®Ÿã¨ç•°ãªã‚‹æ–­å®šã¯ã—ãªã„ã€‚åŠ¹èƒ½ãƒ»åŒ»ç™‚çš„ä¸»å¼µã¯æ§ãˆã‚‹ã€‚
ãƒ»æ–‡æœ«ã‚’å¼•ç”¨ç¬¦ï¼ˆ" ' ã€Œ ã€ â€œ â€ ã€ ã€ï¼‰ã§çµ‚ã‚ã‚‰ã›ãªã„ã“ã¨ã€‚

ã€æ›¸ãæ–¹ã®ãƒ’ãƒ³ãƒˆã€‘ï¼ˆå¼·åˆ¶ã§ã¯ãªã„ï¼‰
ãƒ»ï¼ˆã‚¹ãƒšãƒƒã‚¯ï¼‰â†’ï¼ˆã‚³ã‚¢è¦ç´ ï¼‰â†’ï¼ˆèª°ã«ï¼‰â†’ï¼ˆåˆ©ç”¨ã‚·ãƒ¼ãƒ³ï¼‰â†’ï¼ˆå¾—ã‚‰ã‚Œã‚‹ãƒ™ãƒãƒ•ã‚£ãƒƒãƒˆï¼‰
ãƒ»èªå°¾ã‚’ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã•ã›ã€åŒã˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é€£æ‰“ã‚’é¿ã‘ã‚‹ã€‚
ãƒ»SEOã®éå‰°æ„è­˜ã¯é¿ã‘ã€è‡ªç„¶ã§èª­ã¿ã‚„ã™ã„1ã€œ2æ–‡ã«åã‚ã‚‹ã€‚

å‡ºåŠ›ã¯ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã€‚ALTã‚’20æœ¬ã€è¡ŒåŒºåˆ‡ã‚Šã§è¿”ã—ã¦ãã ã•ã„ã€‚
""")

def build_persona_system(persona: Dict[str, Any]) -> str:
    """
    v5.4: ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°ã®è¦ç¯„ã¯v3.3ã«æˆ»ã—ã€æ–‡æœ«ã®å¼•ç”¨ç¬¦ç¦æ­¢ã‚’è¿½åŠ ã€‚
    Personaã¯ä½¿ã‚ãšå›ºå®šSYSTEMã‚’è¿”ã—ã¾ã™ï¼ˆæŒ™å‹•ã‚’å®‰å®šåŒ–ï¼‰ã€‚
    """
    return BASE_SYSTEM

# ---------- USERãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ----------
def build_user_prompt(product_name: str, knowledge_text: str = "") -> str:
    # 3.3ç›¸å½“ã®ç´ ç›´ãªãƒ¦ãƒ¼ã‚¶æŒ‡ç¤ºã‚’ç¶­æŒ
    hint = ""
    if knowledge_text:
        hint = f"\nã€å‚è€ƒæƒ…å ±ï¼ˆè¦ç´„ï¼‰ã€‘\n{knowledge_text}\n"
    return textwrap.dedent(f"""\
        å•†å“å: {product_name}
        {hint}
        ä¸Šè¨˜ã®å•†å“ã«ã¤ã„ã¦ã€æ—¥æœ¬èªã®ALTãƒ†ã‚­ã‚¹ãƒˆã‚’20æœ¬ä½œæˆã—ã¦ãã ã•ã„ã€‚
        å„ALTã¯1ã€œ2æ–‡ã€è‡ªç„¶ãªæ—¥æœ¬èªã§ã€ã¾ãšãŠã‚ˆã{RAW_MIN}ã€œ{RAW_MAX}å­—ã‚’ç›®å®‰ã«ã—ã¦ãã ã•ã„ã€‚
        ç”»åƒã®æå†™ã¯æ›¸ã‹ãšã€å•†å“ç‰¹å¾´ãƒ»å‹ç•ªãƒ»ç´ æãªã©ã‚’è‡ªç„¶ã«ç¹”ã‚Šè¾¼ã‚“ã§ãã ã•ã„ã€‚
        è¡ŒåŒºåˆ‡ã‚Šã§20æœ¬ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
    """).strip()

# ---------- OPENAIå‘¼ã³å‡ºã— ----------
def call_openai_20_lines(client, model: str, system_prompt: str, user_prompt: str,
                         temperature: float = TEMP, max_tokens: int = MAX_TOKENS) -> List[str]:
    if client is None:
        # ãƒ€ãƒŸãƒ¼å‡ºåŠ›ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
        return [f"{i+1}è¡Œç›®ã®ã‚µãƒ³ãƒ—ãƒ«ALTã§ã™ã€‚è‡ªç„¶ãªæ—¥æœ¬èªã§å•†å“ç‰¹å¾´ã‚’ç¹”ã‚Šè¾¼ã¿ã¾ã™ã€‚" for i in range(20)]

    for retry in range(3):
        try:
            resp = client.chat.completions.create(
                model=model,
                temperature=temperature,
                max_tokens=max_tokens,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                response_format={"type": "text"},
            )
            txt = resp.choices[0].message.content.strip()
            # è²¼ã‚Šä»˜ã‘ã‚„ç®‡æ¡æ›¸ãã«ã‚‚è€ãˆã‚‹ã‚ˆã†ã«æ•´å½¢
            lines = [re.sub(r"^\s*[\-\d\.\)\]]\s*", "", ln).strip() for ln in txt.splitlines() if ln.strip()]
            # è¡Œæ•°èª¿æ•´
            if len(lines) < 20:
                lines += [""] * (20 - len(lines))
            elif len(lines) > 20:
                lines = lines[:20]
            return lines
        except Exception as e:
            if retry == 2:
                raise
            time.sleep(2.0 + retry)

# ---------- æ•´å½¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ ----------
TRAILING_QUOTES = {'"', "'", 'â€œ', 'â€', 'â€˜', 'â€™', 'ã€Œ', 'ã€', 'ã€', 'ã€'}

def normalize_line(t: str) -> str:
    if not t:
        return ""
    t = t.strip()
    t = re.sub(r"\s+", " ", t)
    # ç¦æ­¢èªã®è»½ã„æ­£è¦åŒ–ï¼ˆç”»åƒãƒ¡ã‚¿ï¼‰
    t = re.sub(r"(ç”»åƒ|å†™çœŸ|æ˜ ã£ã¦ã„ã‚‹|ã‚¯ãƒªãƒƒã‚¯|ã“ã¡ã‚‰|ã‚³ãƒãƒ©)", "", t)
    t = t.strip()
    return t

def is_natural_sentence(t: str) -> bool:
    if not t:
        return False
    # å¥ç‚¹/çµ‚ç«¯è¨˜å·ã§çµ‚ã‚ã‚‹ or ã“ã‚Œã‹ã‚‰ä»˜ä¸å¯èƒ½
    return True

def soft_clip_sentence(t: str, min_len: int = FINAL_MIN, max_len: int = FINAL_MAX) -> str:
    if not t:
        return ""
    t = t.strip()
    # æœ«å°¾ã«å¼•ç”¨ç¬¦ãŒã‚ã‚Œã°é™¤å»ï¼ˆv5.4ä»•æ§˜ï¼‰
    while t and t[-1] in TRAILING_QUOTES:
        t = t[:-1]
    t = t.strip()

    # å¥ç‚¹çµ‚æ­¢ã«æ•´ãˆã‚‹
    if not t.endswith(("ã€‚", "ï¼", "?", "ï¼Ÿ", "!")):
        t = t + "ã€‚"

    # ä¸Šé™ã‚’ã‚½ãƒ•ãƒˆã«ã‚«ãƒƒãƒˆï¼ˆå¥èª­ç‚¹ã‚„ã‚¹ãƒšãƒ¼ã‚¹å„ªå…ˆï¼‰
    if len(t) > max_len:
        # å¥ç‚¹ã‚’åŸºæº–ã«æ‰‹å‰ã§è½ã¨ã™
        cut = t[:max_len]
        # ç›´è¿‘ã®å¥ç‚¹/èª­ç‚¹/ç©ºç™½ã§åˆ‡ã‚‹
        m = re.search(r"[ã€‚ï¼ï¼ï¼Ÿ\.\,\s][^ã€‚ï¼ï¼ï¼Ÿ\.\,\s]*$", cut)
        if m:
            cut = cut[:m.start()].rstrip()
        if not cut:
            cut = t[:max_len].rstrip()
        # å†ã³å¼•ç”¨ç¬¦ãŒæœ«å°¾ã«æ¥ã‚‹å¯èƒ½æ€§ã‚‚è½ã¨ã™
        while cut and cut[-1] in TRAILING_QUOTES:
            cut = cut[:-1]
        # çµ‚ç«¯ã‚’å†ä»˜ä¸
        if not cut.endswith(("ã€‚", "ï¼", "?", "ï¼Ÿ", "!")):
            cut = cut + "ã€‚"
        t = cut
    # æœ€å°é•·ã‚’ä¸‹å›ã‚‹å ´åˆã€ç„¡ç†ã«è¿½è¨˜ã¯ã—ãªã„ï¼ˆè‡ªç„¶ã•å„ªå…ˆï¼‰
    return t

def refine_lines(lines: List[str]) -> List[str]:
    out = []
    seen = set()
    for ln in lines:
        s = normalize_line(ln)
        if not is_natural_sentence(s):
            continue
        s = soft_clip_sentence(s, FINAL_MIN, FINAL_MAX)
        if s and s not in seen:
            seen.add(s)
            out.append(s)
    # 20æœ¬ã«æº€ãŸãªã„å ´åˆã¯è£œå®Œï¼ˆç°¡æ˜“ï¼‰
    while len(out) < 20:
        out.append("è‡ªç„¶ãªæ—¥æœ¬èªã®ALTãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚è£½å“ã®ç‰¹å¾´ã‚’ä¼ãˆã¾ã™ã€‚")
    return out[:20]

# ---------- çŸ¥è¦‹ã®è¦ç´„ ----------
def summarize_knowledge(semantics: List[Any], limit: int = 10) -> str:
    # list/dictæ··åˆã‚’å—ã‘å…¥ã‚Œã¦ã€æµ…ãã¤ã¾ã‚€
    bag = []
    for item in semantics[:limit]:
        if isinstance(item, dict):
            for k, v in item.items():
                if isinstance(v, (str, int, float)):
                    bag.append(f"{k}:{v}")
                elif isinstance(v, list):
                    bag.extend([str(x) for x in v[:3]])
        elif isinstance(item, list):
            bag.extend([str(x) for x in item[:5]])
        elif isinstance(item, str):
            bag.append(item)
    txt = "; ".join([re.sub(r"\s+", " ", str(x)) for x in bag if x])
    return textwrap.shorten(txt, width=500, placeholder="â€¦")

# ---------- ãƒ¡ã‚¤ãƒ³å‡¦ç† ----------
def write_raw(items: List[Dict[str, Any]]) -> List[List[str]]:
    persona = persona_or_default()
    sys_prompt = build_persona_system(persona)

    semantics = load_semantics()
    know = summarize_knowledge(semantics)

    rows = [["å•†å“å", "ALT_1", "ALT_2", "ALT_3", "ALT_4", "ALT_5",
             "ALT_6", "ALT_7", "ALT_8", "ALT_9", "ALT_10",
             "ALT_11", "ALT_12", "ALT_13", "ALT_14", "ALT_15",
             "ALT_16", "ALT_17", "ALT_18", "ALT_19", "ALT_20"]]
    for it in items:
        name = (it.get("å•†å“å") or it.get("name") or "").strip()
        user_prompt = build_user_prompt(name, know)
        lines = call_openai_20_lines(client, MODEL, sys_prompt, user_prompt, TEMP, MAX_TOKENS)
        rows.append([name] + lines)
    save_csv_rows(RAW_PATH, rows)
    return rows

def write_refined(raw_rows: List[List[str]]) -> List[List[str]]:
    header = raw_rows[0]
    out = [header]
    for r in raw_rows[1:]:
        name, lines = r[0], r[1:]
        refined = refine_lines(lines)
        out.append([name] + refined)
    save_csv_rows(REF_PATH, out)
    return out

def diff_rows(raw_rows: List[List[str]], ref_rows: List[List[str]]) -> List[List[str]]:
    header = ["å•†å“å", "RAW", "REF", "å¤‰æ›´æœ‰ç„¡"]
    out = [header]
    for r_raw, r_ref in zip(raw_rows[1:], ref_rows[1:]):
        name = r_raw[0]
        diffs = []
        for a, b in zip(r_raw[1:], r_ref[1:]):
            diffs.append([name, a, b, "DIFF" if a != b else "SAME"])
    # ãƒ•ãƒ©ãƒƒãƒˆåŒ–
    flat = [header]
    for r_raw, r_ref in zip(raw_rows[1:], ref_rows[1:]):
        name = r_raw[0]
        for a, b in zip(r_raw[1:], r_ref[1:]):
            flat.append([name, a, b, "DIFF" if a != b else "SAME"])
    save_csv_rows(DIFF_PATH, flat)
    return flat

def avg_len(lines: List[str]) -> float:
    xs = [len(s or "") for s in lines]
    return sum(xs) / max(1, len(xs))

def main():
    print("ğŸ“¦ å…¥åŠ›CSV:", INPUT_PATH)
    items = load_csv_items(INPUT_PATH)
    print(f"   - ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(items)}")

    print("ğŸ§  çŸ¥è¦‹ã®èª­ã¿è¾¼ã¿:", SEMANTICS_DIR)

    print("ğŸ¤– OpenAIå‘¼ã³å‡ºã—ã§RAWç”Ÿæˆä¸­...")
    raw_rows = write_raw(items)

    print("âœ‚ï¸  æ•´å½¢ï¼ˆæ­£è¦åŒ–â†’å¥ç‚¹çµ‚æ­¢â†’é•·ã•æ•´å½¢â†’é‡è¤‡æŠ‘æ­¢ï¼‰...")
    ref_rows = write_refined(raw_rows)

    print("ğŸ” å·®åˆ†å‡ºåŠ›...")
    diff_rows(raw_rows, ref_rows)

    # è»½ã„ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    all_raw = [s for row in raw_rows[1:] for s in row[1:]]
    all_refined = [s for row in ref_rows[1:] for s in row[1:]]
    print(f"   - AIç”Ÿå‡ºåŠ›: {RAW_PATH}")
    print(f"   - æ•´å½¢å¾Œ   : {REF_PATH}")
    print(f"   - å·®åˆ†æ¯”è¼ƒ : {DIFF_PATH}")
    print(f"ğŸ“ æ–‡å­—æ•°(å¹³å‡): raw={avg_len(all_raw):.1f} / refined={avg_len(all_refined):.1f}")
    print("ğŸ”’ ä»•æ§˜:")
    print(f"   - AIã¯ç´„{RAW_MIN}ã€œ{RAW_MAX}å­—ãƒ»1ã€œ2æ–‡ã€å¥ç‚¹çµ‚æ­¢ã€ç¦å‰‡é©ç”¨ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰")
    print(f"   - ãƒ­ãƒ¼ã‚«ãƒ«æ•´å½¢ã§{FINAL_MIN}ã€œ{FINAL_MAX}å­—ã«è‡ªç„¶ã‚«ãƒƒãƒˆã€é‡è¤‡æŠ‘æ­¢ãƒ»æ¬ æè£œå®Œãƒ»èªå°¾ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³")

if __name__ == "__main__":
    main()
import atlas_autosave_core
