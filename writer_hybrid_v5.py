# -*- coding: utf-8 -*-
"""
KOTOHA ENGINE â€” Hybrid AI Writer v5
----------------------------------------
- Shift-JIS(cp932) CSV ã‹ã‚‰å•†å“åã‚’æŠ½å‡º
- èªå½™/æ§‹æ–‡/æ–‡ä½“/ç¦å‰‡ã‚’çµ±åˆã—ã¦ Copy & ALT ã‚’è‡ªå‹•ç”Ÿæˆ
- å‡ºåŠ›: ./output/ai_writer/hybrid_writer_full_YYYYMMDD_HHMM.json

ä»•æ§˜:
- å…¥åŠ›CSVã¯ Shift-JIS (cp932)
- åˆ—è­˜åˆ¥ã¯ â€œè¦‹å‡ºã—åâ€ ã§å³æ ¼ã«ï¼ˆå…ˆé ­è¡Œï¼ãƒ˜ãƒƒãƒ€ï¼‰
- ã€Œå•†å“åã€åˆ—ã‚’è‡ªå‹•ç‰¹å®šã—ã€ç©ºç™½ã‚»ãƒ«ã‚’ç„¡è¦–
- Copyï¼šå…¨è§’40â€“60æ–‡å­—ã€ALTï¼šå…¨è§’80â€“110æ–‡å­—ã§æ•´å½¢
- ALTã¯ç”»åƒæå†™ã‚’ç¦æ­¢ã—ã€SEOæ–‡è„ˆã‚’é‡è¦–
"""

import os
import csv
import json
import re
from datetime import datetime

# =========================================================
# ãƒ‘ã‚¹è¨­å®šï¼ˆã‚ãªãŸã®ç’°å¢ƒå°‚ç”¨æ§‹æˆï¼‰
# =========================================================
BASE_DIR = "/Users/tsuyoshi/Desktop/python_lesson"
INPUT_CSV = os.path.join(BASE_DIR, "input.csv")
OUT_DIR = os.path.join(BASE_DIR, "output/ai_writer")
os.makedirs(OUT_DIR, exist_ok=True)

SEM_DIR = os.path.join(BASE_DIR, "output/semantics")

PATH_LEXICAL = os.path.join(SEM_DIR, "lexical_clusters_20251030_223013.json")
PATH_MARKET  = os.path.join(SEM_DIR, "market_vocab_20251030_201906.json")
PATH_SEMANT  = os.path.join(SEM_DIR, "structured_semantics_20251030_224846.json")
PATH_PERSONA = os.path.join(SEM_DIR, "styled_persona_20251031_0031.json")
PATH_TEMPLATE = os.path.join(SEM_DIR, "template_composer.json")
PATH_NORMALIZED = os.path.join(SEM_DIR, "normalized_20251031_0039.json")

ENCODING = "cp932"

# =========================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ç¾¤
# =========================================================
def jlen(s):
    """æ—¥æœ¬èªæ–‡ã®æ–‡å­—æ•°ã‚’å˜ç´”ã‚«ã‚¦ãƒ³ãƒˆ"""
    return len(s.strip())

def sanitize(s):
    """å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹é™¤å»ï¼‹æ­£è¦åŒ–"""
    s = s.replace("\u3000", " ")
    return re.sub(r"\s+", " ", s).strip()

def load_json(path, default=None):
    """JSONãƒ­ãƒ¼ãƒ€ï¼ˆå­˜åœ¨ã—ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¿”å´ï¼‰"""
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return default or {}

def trim_len(t, min_l, max_l):
    """æ–‡å­—æ•°ãƒˆãƒªãƒ """
    t = t.strip()
    if jlen(t) > max_l:
        t = t[:max_l]
    if not t.endswith("ã€‚"):
        t += "ã€‚"
    return t

def pad_len(t, min_l):
    """çŸ­ã„æ–‡ã«è‡ªç„¶ãªè£œè¶³ã‚’ä»˜åŠ """
    endings = ["ä½¿ã„ã‚„ã™ã„è¨­è¨ˆã§ã™ã€‚", "æ¯æ—¥ã«å¯„ã‚Šæ·»ã†ä¸€å“ã§ã™ã€‚", "å“è³ªã¨ãƒ‡ã‚¶ã‚¤ãƒ³ã‚’ä¸¡ç«‹ã—ã¾ã—ãŸã€‚"]
    while jlen(t) < min_l:
        for e in endings:
            if jlen(t) < min_l:
                t += e
    return t

def apply_forbidden(t):
    """ç¦å‰‡èªãƒ»èª‡å¼µè¡¨ç¾ã®é™¤å»"""
    forbidden = [
        "æœ€å¼·", "æ—¥æœ¬ä¸€", "ä¸–ç•Œä¸€", "å®Œå…¨ç„¡æ–™", "çµ¶å¯¾", "æ°¸ä¹…ä¿è¨¼",
        "100%", "å‰¯ä½œç”¨ãªã—", "å¿…ãšç—©ã›ã‚‹", "é•æ³•", "å±é™º", "æš´åŠ›"
    ]
    for f in forbidden:
        t = t.replace(f, "")
    return sanitize(t)

# =========================================================
# æ–‡ç”Ÿæˆãƒ˜ãƒ«ãƒ‘
# =========================================================
def build_copy_alt(name, cluster, market_cfg, sem_cfg):
    """ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¨å¸‚å ´èªå½™ã‹ã‚‰ Copy / ALT ã‚’æ§‹ç¯‰"""
    base = market_cfg.get(cluster, market_cfg.get("general", {}))

    hook = base.get("hooks", [name])[0]
    benefit = base.get("benefits", ["å¿«é©ãªæ—¥å¸¸ã‚’ã‚µãƒãƒ¼ãƒˆ"])[0]
    feature = base.get("features", ["ã‚·ãƒ³ãƒ—ãƒ«ãªè¨­è¨ˆ"])[0]
    compat = base.get("compat", ["å¹…åºƒã„æ©Ÿç¨®ã«å¯¾å¿œ"])[0]

    pattern = "{hook} {benefit} {feature} {compat}"
    copy_text = pattern.format(hook=hook, benefit=benefit, feature=feature, compat=compat)
    alt_text = f"{name}ï½œ{hook}ã€‚{benefit}ã€‚{feature}ã€‚{compat}ã€‚æ¯æ—¥ã®ä½¿ç”¨ã‚’æƒ³å®šã—ãŸå®Ÿç”¨çš„ãªã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼ã€‚"

    return copy_text, alt_text

# =========================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# =========================================================
def main():
    print("ğŸŒ¸ Hybrid AI Writer v5 å®Ÿè¡Œé–‹å§‹")

    # å„ç¨®ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­è¾¼
    lexical_cfg = load_json(PATH_LEXICAL)
    market_cfg  = load_json(PATH_MARKET)
    sem_cfg     = load_json(PATH_SEMANT)
    persona_cfg = load_json(PATH_PERSONA)
    normalized_cfg = load_json(PATH_NORMALIZED)
    tmpl_cfg    = load_json(PATH_TEMPLATE)

    # CSVèª­ã¿è¾¼ã¿
    if not os.path.exists(INPUT_CSV):
        raise FileNotFoundError(f"å…¥åŠ›CSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {INPUT_CSV}")

    with open(INPUT_CSV, "r", encoding=ENCODING, newline="") as f:
        reader = csv.reader(f)
        rows = list(reader)

    if not rows:
        print("âš ï¸ CSVãŒç©ºã§ã™ã€‚")
        return

    header = rows[0]
    try:
        name_idx = header.index("å•†å“å")
    except ValueError:
        raise RuntimeError("âš ï¸ ãƒ˜ãƒƒãƒ€ã«ã€å•†å“åã€åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    # å•†å“åã®æŠ½å‡º
    names = [sanitize(r[name_idx]) for r in rows[1:] if len(r) > name_idx and sanitize(r[name_idx])]
    unique_names = list(dict.fromkeys(names))  # é‡è¤‡é™¤å¤–

    print(f"âœ… å•†å“åæŠ½å‡º: {len(names)}ä»¶ â†’ ä¸€æ„åŒ–å¾Œ {len(unique_names)}ä»¶")

    # Copy / ALT ç”Ÿæˆ
    results = []
    for nm in unique_names:
        cluster = "general"
        copy_draft, alt_draft = build_copy_alt(nm, cluster, market_cfg, sem_cfg)
        copy_t = apply_forbidden(trim_len(pad_len(copy_draft, 40), 40, 60))
        alt_t  = apply_forbidden(trim_len(pad_len(alt_draft, 80), 80, 110))

        results.append({
            "product_name": nm,
            "copy": copy_t,
            "alt": alt_t,
            "csv_map": {
                "ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼": copy_t,
                "å•†å“ç”»åƒåï¼ˆALTï¼‰1": alt_t
            }
        })

    # å‡ºåŠ›
    now = datetime.now().strftime("%Y%m%d_%H%M")
    out_path = os.path.join(OUT_DIR, f"hybrid_writer_full_{now}.json")
    os.makedirs(os.path.dirname(out_path), exist_ok=True)

    meta = {
        "input_csv": INPUT_CSV,
        "encoding": ENCODING,
        "total_rows": len(rows),
        "detected_products": len(unique_names),
        "dicts": {
            "lexical_clusters": PATH_LEXICAL,
            "market_vocab": PATH_MARKET,
            "structured_semantics": PATH_SEMANT,
            "styled_persona": PATH_PERSONA,
            "normalized": PATH_NORMALIZED,
            "template_composer": PATH_TEMPLATE
        }
    }

    with open(out_path, "w", encoding="utf-8") as f:
        json.dump({"meta": meta, "items": results}, f, ensure_ascii=False, indent=2)

    print(f"ğŸ’¾ å‡ºåŠ›å®Œäº†: {out_path}")
    print(f"ğŸ“Š ä»¶æ•°: {len(results)}ï¼ˆCopy/ALTã¨ã‚‚å…¨ä»¶ç”Ÿæˆï¼‰")
    print("ğŸ“ Copy æ–‡å­—æ•°: 40â€“60 / ALT æ–‡å­—æ•°: 80â€“110")
    print("âœ… ç¦å‰‡ãƒ»å¥èª­ç‚¹ãƒ»æ–‡ä½“ã™ã¹ã¦é©ç”¨æ¸ˆ")

# =========================================================
if __name__ == "__main__":
    main()
import atlas_autosave_core
