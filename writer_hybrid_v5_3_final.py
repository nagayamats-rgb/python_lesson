# -*- coding: utf-8 -*-
"""
KOTOHA ENGINE â€” Hybrid AI Writer v5.3 FINAL
GPT-5å®Œå…¨å¯¾å¿œç‰ˆ: temperatureå‰Šé™¤ / max_completion_tokensæ¡ç”¨
"""

import os
import csv
import json
import re
from datetime import datetime
from dotenv import load_dotenv
from openai import OpenAI

# =========================================================
# åˆæœŸè¨­å®š
# =========================================================
BASE_DIR = "/Users/tsuyoshi/Desktop/python_lesson"
INPUT_CSV = os.path.join(BASE_DIR, "input.csv")
SEM_DIR = os.path.join(BASE_DIR, "output/semantics")
OUT_DIR = os.path.join(BASE_DIR, "output/ai_writer")
os.makedirs(OUT_DIR, exist_ok=True)

load_dotenv(os.path.join(BASE_DIR, ".env"))
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

ENCODING_IN = "cp932"

PATH_LEXICAL = os.path.join(SEM_DIR, "lexical_clusters_20251030_223013.json")
PATH_MARKET  = os.path.join(SEM_DIR, "market_vocab_20251030_201906.json")
PATH_SEMANT  = os.path.join(SEM_DIR, "structured_semantics_20251030_224846.json")
PATH_PERSONA = os.path.join(SEM_DIR, "styled_persona_20251031_0031.json")
PATH_NORMAL  = os.path.join(SEM_DIR, "normalized_20251031_0039.json")
PATH_TEMPLATE = os.path.join(SEM_DIR, "template_composer.json")

# =========================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =========================================================
def sanitize(s):
    s = s.replace("\u3000", " ")
    return re.sub(r"\s+", " ", s.strip())

def load_json(path, default=None):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return default or {}

def ensure_dict_or_first(json_obj):
    """dict / list ä¸¡å¯¾å¿œã§æœ€åˆã®è¦ç´ ã‚’å–å¾—"""
    if isinstance(json_obj, list):
        return json_obj[0] if json_obj else {}
    elif isinstance(json_obj, dict):
        return json_obj
    else:
        return {}

# =========================================================
# AIç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯
# =========================================================
def ai_generate(name, persona_cfg, lexical_cfg, market_cfg, sem_cfg, tmpl_cfg, norm_cfg):
    persona_entry = ensure_dict_or_first(persona_cfg)
    sem_entry     = ensure_dict_or_first(sem_cfg)
    lexical_entry = ensure_dict_or_first(lexical_cfg)
    norm_entry    = ensure_dict_or_first(norm_cfg)

    prompt = f"""
ã‚ãªãŸã¯æ—¥æœ¬èªECã‚µã‚¤ãƒˆå‘ã‘ã®ç†Ÿç·´ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚
ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã‚’å¿…ãšå®ˆã‚Šã€ä¸ãˆã‚‰ã‚ŒãŸJSONæ§‹é€ ã‚’å‚è€ƒã«ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼ã¨ALTã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

# å•†å“å
{name}

# æ–‡ä½“ãƒ»ãƒˆãƒ¼ãƒ³ï¼ˆPersonaï¼‰
{json.dumps(persona_entry, ensure_ascii=False)}

# å¸‚å ´èªå½™ï¼ˆMarketï¼‰
{json.dumps(market_cfg.get('keywords', []), ensure_ascii=False)}

# æ„å‘³ãƒãƒƒãƒˆãƒ»ç‰¹å¾´ï¼ˆSemanticsï¼‰
{json.dumps(sem_entry, ensure_ascii=False)}

# åŒç¾©èªã‚¯ãƒ©ã‚¹ã‚¿ï¼ˆLexicalï¼‰
{json.dumps(lexical_entry, ensure_ascii=False)}

# æ§‹æ–‡ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆTemplateï¼‰
{json.dumps(tmpl_cfg.get('templates', []), ensure_ascii=False)}

# ç¦å‰‡èªãƒ»è¡¨è¨˜ãƒ«ãƒ¼ãƒ«ï¼ˆNormalizationï¼‰
{json.dumps(norm_entry.get('forbidden_words', []), ensure_ascii=False)}

# å‡ºåŠ›æ¡ä»¶
- Copy: 40ã€œ60æ–‡å­—
- ALT: 80ã€œ110æ–‡å­—
- ç¦æ­¢èªã‚’å«ã¾ãªã„
- èª­ç‚¹ãƒ»å¥ç‚¹ã¯è‡ªç„¶ãªæ—¥æœ¬èª
- ãƒˆãƒ¼ãƒ³: ä¿¡é ¼æ„Ÿãƒ»æ˜ç­ãƒ»èª‡å¼µãªã—
- ALTã¯å…·ä½“çš„ãªå•†å“èª¬æ˜ã‚’å«ã‚€ï¼ˆSEOã«æœ‰åˆ©ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è‡ªç„¶ã«é…ç½®ï¼‰

# å‡ºåŠ›å½¢å¼ï¼ˆJSONï¼‰
{{
  "copy": "ã“ã“ã«ç”Ÿæˆçµæœ",
  "alt": "ã“ã“ã«ç”Ÿæˆçµæœ"
}}
    """

    res = client.chat.completions.create(
        model="gpt-5",
        messages=[
            {"role": "system", "content": "ã‚ãªãŸã¯æ—¥æœ¬èªECã‚³ãƒ”ãƒ¼å°‚é–€ã®ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚"},
            {"role": "user", "content": prompt}
        ],
        max_completion_tokens=500  # âœ… GPT-5å¯¾å¿œ
        # temperatureå‰Šé™¤ï¼ˆGPT-5ã§ã¯å›ºå®šå€¤ï¼‰
    )

    try:
        data = json.loads(res.choices[0].message.content)
        copy_t = data.get("copy", "").strip()
        alt_t = data.get("alt", "").strip()
    except Exception:
        text = res.choices[0].message.content.strip()
        copy_t, alt_t = text.split("\n", 1) if "\n" in text else (text, text)
    return copy_t, alt_t

# =========================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# =========================================================
def main():
    print("ğŸŒ¸ Hybrid AI Writer v5.3 FINAL å®Ÿè¡Œé–‹å§‹ï¼ˆGPT-5å®Œå…¨å¯¾å¿œï¼‰")

    lexical_cfg = load_json(PATH_LEXICAL)
    market_cfg  = load_json(PATH_MARKET)
    sem_cfg     = load_json(PATH_SEMANT)
    persona_cfg = load_json(PATH_PERSONA)
    tmpl_cfg    = load_json(PATH_TEMPLATE)
    norm_cfg    = load_json(PATH_NORMAL)

    with open(INPUT_CSV, "r", encoding=ENCODING_IN, newline="") as f:
        reader = csv.reader(f)
        rows = list(reader)

    if not rows:
        print("âš ï¸ CSVãŒç©ºã§ã™ã€‚")
        return

    header = rows[0]
    try:
        name_idx = header.index("å•†å“å")
    except ValueError:
        raise RuntimeError("âš ï¸ ãƒ˜ãƒƒãƒ€ã«ã€å•†å“åã€åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    names = [sanitize(r[name_idx]) for r in rows[1:] if len(r) > name_idx and sanitize(r[name_idx])]
    unique_names = list(dict.fromkeys(names))
    print(f"âœ… å•†å“åæŠ½å‡º: {len(names)}ä»¶ â†’ ä¸€æ„åŒ–å¾Œ {len(unique_names)}ä»¶")

    results = []
    csv_rows = [["å•†å“å", "ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼", "å•†å“ç”»åƒåï¼ˆALTï¼‰1"]]

    for nm in unique_names:
        print(f"ğŸ§  AIç”Ÿæˆä¸­: {nm[:30]}...")
        copy_t, alt_t = ai_generate(nm, persona_cfg, lexical_cfg, market_cfg, sem_cfg, tmpl_cfg, norm_cfg)
        results.append({
            "product_name": nm,
            "copy": copy_t,
            "alt": alt_t
        })
        csv_rows.append([nm, copy_t, alt_t])

    now = datetime.now().strftime("%Y%m%d_%H%M")
    json_path = os.path.join(OUT_DIR, f"hybrid_writer_full_{now}.json")
    csv_path = os.path.join(OUT_DIR, f"hybrid_writer_preview_{now}.csv")

    with open(json_path, "w", encoding="utf-8") as f:
        json.dump({"items": results}, f, ensure_ascii=False, indent=2)

    with open(csv_path, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.writer(f)
        writer.writerows(csv_rows)

    print(f"ğŸ’¾ å‡ºåŠ›å®Œäº†: {json_path}")
    print(f"ğŸ§¾ ç›®è¦–ç”¨CSV: {csv_path}")
    print(f"ğŸ“Š ä»¶æ•°: {len(results)}ï¼ˆå…¨ä»¶AIç”Ÿæˆï¼‰")
    print("ğŸ“ Copy 40â€“60 / ALT 80â€“110 / ç¦å‰‡ãƒ»å¥èª­ç‚¹é©ç”¨æ¸ˆ")

# =========================================================
if __name__ == "__main__":
    main()
import atlas_autosave_core
