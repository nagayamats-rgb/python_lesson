# -*- coding: utf-8 -*-
"""
KOTOHA ENGINE â€” Hybrid AI Writer v5.5
GPT-5å®Œå…¨å¯¾å¿œï¼ˆjson_schemaæ§‹é€ å‡ºåŠ›ï¼‹ALT20ä»¶ï¼‰
"""

import os, csv, json, re
from datetime import datetime
from dotenv import load_dotenv
from openai import OpenAI

# ---------------------------------------------------------
# åˆæœŸè¨­å®š
# ---------------------------------------------------------
BASE_DIR = "/Users/tsuyoshi/Desktop/python_lesson"
INPUT_CSV = os.path.join(BASE_DIR, "input.csv")
SEM_DIR = os.path.join(BASE_DIR, "output/semantics")
OUT_DIR = os.path.join(BASE_DIR, "output/ai_writer")
os.makedirs(OUT_DIR, exist_ok=True)

load_dotenv(os.path.join(BASE_DIR, ".env"))
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
ENCODING_IN = "cp932"

# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
PATH_LEXICAL = os.path.join(SEM_DIR, "lexical_clusters_20251030_223013.json")
PATH_MARKET  = os.path.join(SEM_DIR, "market_vocab_20251030_201906.json")
PATH_SEMANT  = os.path.join(SEM_DIR, "structured_semantics_20251030_224846.json")
PATH_PERSONA = os.path.join(SEM_DIR, "styled_persona_20251031_0031.json")
PATH_NORMAL  = os.path.join(SEM_DIR, "normalized_20251031_0039.json")
PATH_TEMPLATE = os.path.join(SEM_DIR, "template_composer.json")

# ---------------------------------------------------------
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ---------------------------------------------------------
def sanitize(s):
    s = s.replace("\u3000", " ")
    return re.sub(r"\s+", " ", s.strip())

def load_json(path, default=None):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return default or {}

def ensure_dict_or_first(x):
    if isinstance(x, list):
        return x[0] if x else {}
    elif isinstance(x, dict):
        return x
    else:
        return {}

# ---------------------------------------------------------
# AIç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯
# ---------------------------------------------------------
def ai_generate(name, persona_cfg, lexical_cfg, market_cfg, sem_cfg, tmpl_cfg, norm_cfg):
    persona = ensure_dict_or_first(persona_cfg)
    sem     = ensure_dict_or_first(sem_cfg)
    lex     = ensure_dict_or_first(lexical_cfg)
    norm    = ensure_dict_or_first(norm_cfg)

    context = {
        "product_name": name,
        "persona": persona,
        "semantics": sem,
        "lexical": lex,
        "market": market_cfg.get("keywords", []),
        "templates": tmpl_cfg.get("templates", []),
        "forbidden_words": norm.get("forbidden_words", [])
    }

    # GPT-5 æ§‹é€ å‡ºåŠ›
    res = client.chat.completions.create(
        model="gpt-5",
        messages=[
            {"role": "system", "content": "ã‚ãªãŸã¯æ—¥æœ¬èªECã‚µã‚¤ãƒˆã®ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼AIã§ã™ã€‚"},
            {
                "role": "user",
                "content": (
                    "ä»¥ä¸‹ã®JSONæ§‹é€ æƒ…å ±ã‚’ã‚‚ã¨ã«ã€"
                    "40ã€œ60æ–‡å­—ã®é­…åŠ›çš„ãªã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼ã¨ã€"
                    "SEOæœ€é©åŒ–ã•ã‚ŒãŸALTãƒ†ã‚­ã‚¹ãƒˆ20ä»¶ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"
                    "ALTã¯80ã€œ110æ–‡å­—ã§ã€å•†å“ã®ç‰¹å¾´ãƒ»ç”¨é€”ãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è‡ªç„¶ã«å«ã‚ã¦ãã ã•ã„ã€‚\n\n"
                    + json.dumps(context, ensure_ascii=False, indent=2)
                )
            }
        ],
        response_format={
            "type": "json_schema",
            "json_schema": {
                "name": "ProductCopy",
                "schema": {
                    "type": "object",
                    "properties": {
                        "copy": {
                            "type": "string",
                            "description": "ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼ï¼ˆ40ã€œ60æ–‡å­—ï¼‰"
                        },
                        "alts": {
                            "type": "array",
                            "description": "ALTãƒ†ã‚­ã‚¹ãƒˆ20ä»¶ï¼ˆ80ã€œ110æ–‡å­—ï¼‰",
                            "items": {"type": "string"},
                            "minItems": 20,
                            "maxItems": 20
                        }
                    },
                    "required": ["copy", "alts"]
                }
            }
        },
        max_completion_tokens=1000
    )

    msg = res.choices[0].message
    if not getattr(msg, "content", None):
        print("âš ï¸ å¿œç­”ãªã—ï¼æ‹’å¦")
        return "ç”Ÿæˆå¤±æ•—", ["ç”Ÿæˆå¤±æ•—" for _ in range(20)]

    try:
        data = json.loads(msg.content)
        copy_t = data.get("copy", "").strip() or "ç”Ÿæˆå¤±æ•—"
        alts = data.get("alts", [])
        if not alts or len(alts) < 20:
            alts += [""] * (20 - len(alts))
        return copy_t, alts[:20]
    except Exception:
        return "ç”Ÿæˆå¤±æ•—", ["ç”Ÿæˆå¤±æ•—" for _ in range(20)]

# ---------------------------------------------------------
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ---------------------------------------------------------
def main():
    print("ğŸŒ¸ Hybrid AI Writer v5.5 å®Ÿè¡Œé–‹å§‹ï¼ˆGPT-5æ§‹é€ å‡ºåŠ›å¯¾å¿œï¼‰")

    cfg = {
        "lex": load_json(PATH_LEXICAL),
        "market": load_json(PATH_MARKET),
        "sem": load_json(PATH_SEMANT),
        "persona": load_json(PATH_PERSONA),
        "tmpl": load_json(PATH_TEMPLATE),
        "norm": load_json(PATH_NORMAL),
    }

    # å•†å“åæŠ½å‡º
    with open(INPUT_CSV, "r", encoding=ENCODING_IN, newline="") as f:
        reader = csv.reader(f)
        rows = list(reader)
    header = rows[0]
    name_idx = header.index("å•†å“å")
    names = [sanitize(r[name_idx]) for r in rows[1:] if len(r) > name_idx and sanitize(r[name_idx])]
    uniq = list(dict.fromkeys(names))
    print(f"âœ… å•†å“åæŠ½å‡º: {len(names)}ä»¶ â†’ ä¸€æ„åŒ–å¾Œ {len(uniq)}ä»¶")

    results = []
    csv_header = ["å•†å“å", "ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼"] + [f"å•†å“ç”»åƒåï¼ˆALTï¼‰{i}" for i in range(1, 21)]
    csv_rows = [csv_header]

    for nm in uniq:
        print(f"ğŸ§  AIç”Ÿæˆä¸­: {nm[:30]}...")
        copy_t, alts = ai_generate(nm, cfg["persona"], cfg["lex"], cfg["market"], cfg["sem"], cfg["tmpl"], cfg["norm"])
        csv_rows.append([nm, copy_t] + alts)
        results.append({"product_name": nm, "copy": copy_t, "alts": alts})

    now = datetime.now().strftime("%Y%m%d_%H%M")
    json_path = os.path.join(OUT_DIR, f"hybrid_writer_full_{now}.json")
    csv_path = os.path.join(OUT_DIR, f"hybrid_writer_preview_{now}.csv")

    with open(json_path, "w", encoding="utf-8") as f:
        json.dump({"items": results}, f, ensure_ascii=False, indent=2)
    with open(csv_path, "w", encoding="utf-8-sig", newline="") as f:
        csv.writer(f).writerows(csv_rows)

    print(f"ğŸ’¾ å‡ºåŠ›å®Œäº†: {json_path}")
    print(f"ğŸ§¾ ç›®è¦–ç”¨CSV: {csv_path}")
    print(f"ğŸ“Š ä»¶æ•°: {len(results)}ï¼ˆå…¨ä»¶AIç”Ÿæˆï¼ALT20ä»¶æ§‹é€ å‡ºåŠ›ï¼‰")

# ---------------------------------------------------------
if __name__ == "__main__":
    main()
import atlas_autosave_core
