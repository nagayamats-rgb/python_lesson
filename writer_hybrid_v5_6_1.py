# -*- coding: utf-8 -*-
"""
KOTOHA ENGINE â€” Hybrid AI Writer v5.6.1
GPT-5å®‰å…¨ãƒ¢ãƒ¼ãƒ‰ï¼šALT10Ã—2åˆ†å‰²ç”Ÿæˆï¼‹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
"""

import os, csv, json, re
from datetime import datetime
from dotenv import load_dotenv
from openai import OpenAI

BASE_DIR = "/Users/tsuyoshi/Desktop/python_lesson"
INPUT_CSV = os.path.join(BASE_DIR, "input.csv")
SEM_DIR = os.path.join(BASE_DIR, "output/semantics")
OUT_DIR = os.path.join(BASE_DIR, "output/ai_writer")
os.makedirs(OUT_DIR, exist_ok=True)

load_dotenv(os.path.join(BASE_DIR, ".env"))
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
ENCODING_IN = "cp932"

# --- ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ« ---
PATH_LEXICAL = os.path.join(SEM_DIR, "lexical_clusters_20251030_223013.json")
PATH_MARKET  = os.path.join(SEM_DIR, "market_vocab_20251030_201906.json")
PATH_SEMANT  = os.path.join(SEM_DIR, "structured_semantics_20251030_224846.json")
PATH_PERSONA = os.path.join(SEM_DIR, "styled_persona_20251031_0031.json")
PATH_NORMAL  = os.path.join(SEM_DIR, "normalized_20251031_0039.json")
PATH_TEMPLATE = os.path.join(SEM_DIR, "template_composer.json")

def sanitize(s):
    return re.sub(r"\s+", " ", s.replace("\u3000"," ").strip())

def load_json(p, d=None):
    try:
        with open(p,"r",encoding="utf-8") as f: return json.load(f)
    except: return d or {}

def short(x):
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè»½é‡åŒ–"""
    if isinstance(x, list): return x[:5]
    if isinstance(x, dict): return {k: x[k] for k in list(x.keys())[:5]}
    return x

# --- GPTå‘¼ã³å‡ºã— ---
def ai_generate(name, persona, lexical, market, sem, tmpl, norm):
    """2æ®µéšALTç”Ÿæˆ + ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
    # ğŸ”§ forbidden_wordsã‚’å®‰å…¨ã«å–å¾—
    if isinstance(norm, dict):
        forbidden_words = norm.get("forbidden_words", [])
    elif isinstance(norm, list):
        forbidden_words = norm
    else:
        forbidden_words = []

    base_prompt = f"""
ã‚ãªãŸã¯æ—¥æœ¬èªECã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚
å•†å“å: {name}
æ–‡ä½“: {json.dumps(short(persona),ensure_ascii=False)}
å¸‚å ´èªå½™: {json.dumps(short(market.get('keywords',[])),ensure_ascii=False)}
æ„å‘³èªç¾¤: {json.dumps(short(sem),ensure_ascii=False)}
ç¦å‰‡èª: {json.dumps(forbidden_words,ensure_ascii=False)}

å‡ºåŠ›å½¢å¼:
{{
  "copy": "40ã€œ60æ–‡å­—ã®ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼",
  "alts": ["ALTæ–‡1", "ALTæ–‡2", ...10ä»¶]
}}
ALTã¯80ã€œ110æ–‡å­—ã§ã€è‡ªç„¶ã‹ã¤SEOçš„ã«æœ‰åŠ¹ãªèª¬æ˜æ–‡ã«ã—ã¦ãã ã•ã„ã€‚
"""

    # --- copy + ALT(1-10) ---
    try:
        res1 = client.chat.completions.create(
            model="gpt-5",
            messages=[
                {"role":"system","content":"ã‚ãªãŸã¯æ—¥æœ¬èªECã‚³ãƒ”ãƒ¼ã®å°‚é–€å®¶ã§ã™ã€‚"},
                {"role":"user","content":base_prompt}
            ],
            response_format={
                "type":"json_schema",
                "json_schema":{
                    "name":"CopyAltBlock",
                    "schema":{
                        "type":"object",
                        "properties":{
                            "copy":{"type":"string"},
                            "alts":{
                                "type":"array","items":{"type":"string"},"maxItems":10
                            }
                        },
                        "required":["copy","alts"]
                    }
                }
            },
            max_completion_tokens=600
        )
        data1=json.loads(res1.choices[0].message.content)
        copy=data1.get("copy","ç”Ÿæˆå¤±æ•—").strip()
        alts1=data1.get("alts",[])
    except Exception:
        copy,alts1="ç”Ÿæˆå¤±æ•—",[]

    # --- ALT(11-20)è¿½åŠ  ---
    alt_prompt=f"å•†å“å: {name}\nä¸Šè¨˜ã¨ç•°ãªã‚‹å†…å®¹ã®ALTãƒ†ã‚­ã‚¹ãƒˆã‚’ã•ã‚‰ã«10ä»¶ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"
    try:
        res2 = client.chat.completions.create(
            model="gpt-5",
            messages=[
                {"role":"system","content":"ã‚ãªãŸã¯æ—¥æœ¬èªECã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚"},
                {"role":"user","content":alt_prompt}
            ],
            response_format={
                "type":"json_schema",
                "json_schema":{
                    "name":"AltBlock",
                    "schema":{
                        "type":"object",
                        "properties":{
                            "alts":{
                                "type":"array","items":{"type":"string"},"maxItems":10
                            }
                        },
                        "required":["alts"]
                    }
                }
            },
            max_completion_tokens=400
        )
        data2=json.loads(res2.choices[0].message.content)
        alts2=data2.get("alts",[])
    except Exception:
        alts2=[]

    alts=(alts1+alts2)[:20]
    if len(alts)<20: alts+=[""]*(20-len(alts))
    return copy,alts

def main():
    print("ğŸŒ¸ Hybrid AI Writer v5.6.1 å®Ÿè¡Œé–‹å§‹ï¼ˆå®‰å…¨ãƒ¢ãƒ¼ãƒ‰ï¼åˆ†å‰²ALTç”Ÿæˆï¼‰")

    cfg={
        "lex":load_json(PATH_LEXICAL),
        "market":load_json(PATH_MARKET),
        "sem":load_json(PATH_SEMANT),
        "persona":load_json(PATH_PERSONA),
        "tmpl":load_json(PATH_TEMPLATE),
        "norm":load_json(PATH_NORMAL)
    }

    # --- CSVèª­è¾¼ ---
    with open(INPUT_CSV,"r",encoding=ENCODING_IN) as f:
        rows=list(csv.reader(f))
    header=rows[0]; name_idx=header.index("å•†å“å")
    names=[sanitize(r[name_idx]) for r in rows[1:] if len(r)>name_idx and sanitize(r[name_idx])]
    uniq=list(dict.fromkeys(names))
    print(f"âœ… å•†å“åæŠ½å‡º: {len(names)}ä»¶ â†’ ä¸€æ„åŒ–å¾Œ {len(uniq)}ä»¶")

    results=[]; csv_rows=[["å•†å“å","ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼"]+[f"å•†å“ç”»åƒåï¼ˆALTï¼‰{i}" for i in range(1,21)]]

    for nm in uniq:
        print(f"ğŸ§  ç”Ÿæˆä¸­: {nm[:30]}...")
        copy,alts=ai_generate(nm,cfg["persona"],cfg["lex"],cfg["market"],cfg["sem"],cfg["tmpl"],cfg["norm"])
        results.append({"product_name":nm,"copy":copy,"alts":alts})
        csv_rows.append([nm,copy]+alts)

    now=datetime.now().strftime("%Y%m%d_%H%M")
    jpath=os.path.join(OUT_DIR,f"hybrid_writer_full_{now}.json")
    cpath=os.path.join(OUT_DIR,f"hybrid_writer_preview_{now}.csv")

    with open(jpath,"w",encoding="utf-8") as f: json.dump({"items":results},f,ensure_ascii=False,indent=2)
    with open(cpath,"w",encoding="utf-8-sig",newline="") as f: csv.writer(f).writerows(csv_rows)

    print(f"ğŸ’¾ å‡ºåŠ›å®Œäº†: {jpath}")
    print(f"ğŸ§¾ ç›®è¦–ç”¨CSV: {cpath}")
    print(f"ğŸ“Š ä»¶æ•°: {len(results)}ï¼ˆå…¨ä»¶AIç”Ÿæˆï¼ALT20ä»¶çµ±åˆï¼‰")

if __name__=="__main__":
    main()
import atlas_autosave_core
