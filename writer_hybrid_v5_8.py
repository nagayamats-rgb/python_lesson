# ===========================================
# ðŸŒ¸ KOTOHA ENGINE â€” Hybrid AI Writer v5.8 (Fixed)
# å•†å“åˆ¥ã‚¯ãƒ©ã‚¹ã‚¿çŸ¥è¦‹ãƒ–ãƒªãƒƒã‚¸ï¼ˆè¾žæ›¸æ§‹é€ å¯¾å¿œç‰ˆï¼‰
# ===========================================

import os, json, re, random, datetime
import pandas as pd
from openai import OpenAI
from dotenv import load_dotenv
from tqdm import tqdm

# ========== ç’°å¢ƒè¨­å®š ==========
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

INPUT_CSV = "./input.csv"
OUT_JSON = "./output/ai_writer/"
SEM_PATH = "./output/semantics/"

os.makedirs(OUT_JSON, exist_ok=True)

# ========== JSONãƒ­ãƒ¼ãƒ‰ ==========
def load_json(path):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def load_knowledge_files():
    files = {
        "cluster": f"{SEM_PATH}lexical_clusters_20251030_223013.json",
        "market": f"{SEM_PATH}market_vocab_20251030_201906.json",
        "semantic": f"{SEM_PATH}structured_semantics_20251030_224846.json",
        "persona": f"{SEM_PATH}styled_persona_20251031_0031.json",
        "normalized": f"{SEM_PATH}normalized_20251031_0039.json",
        "template": f"{SEM_PATH}template_composer.json"
    }
    return {k: load_json(v) for k, v in files.items()}

# ========== ã‚¯ãƒ©ã‚¹ã‚¿ãƒžãƒƒãƒãƒ³ã‚° ==========
def find_related_cluster(product_name, clusters):
    for c in clusters:
        if any(k in product_name for k in c.get("keywords", [])):
            return c
    return random.choice(clusters)

# ========== çŸ¥è¦‹è¦ç´„ãƒ–ãƒ­ãƒƒã‚¯ç”Ÿæˆ ==========
def make_knowledge_block(cluster, market, sem, persona, normalized, template):
    kws = "ã€".join(cluster.get("keywords", [])[:5])

    # --- marketæ§‹é€ ã«å¯¾å¿œ ---
    if isinstance(market, dict):
        trend_words = []
        for v in market.values():
            if isinstance(v, list):
                trend_words.extend(v)
        trend = "ã€".join(trend_words[:10])
    elif isinstance(market, list):
        trend = "ã€".join([m.get("vocabulary", "") for m in market if isinstance(m, dict)])[:10]
    else:
        trend = ""

    # --- personaæ§‹é€ ã«å¯¾å¿œ ---
    if isinstance(persona, list) and len(persona) > 0:
        tone = persona[0].get("tone", "èª å®Ÿã§çŸ¥çš„")
    elif isinstance(persona, dict):
        tone = persona.get("tone", "èª å®Ÿã§çŸ¥çš„")
    else:
        tone = "èª å®Ÿã§çŸ¥çš„"

    # --- normalizedæ§‹é€ ã«å¯¾å¿œ ---
    if isinstance(normalized, list) and len(normalized) > 0:
        forbid = "ã€".join(normalized[0].get("forbidden_words", []))
    elif isinstance(normalized, dict):
        forbid = "ã€".join(normalized.get("forbidden_words", []))
    else:
        forbid = ""

    # --- templateæ§‹é€ ã«å¯¾å¿œ ---
    if isinstance(template, dict) and "templates" in template:
        tmpl = "ãƒ»".join(template["templates"][:3])
    else:
        tmpl = ""

    # --- semanticsæ§‹é€ ã«å¯¾å¿œ ---
    if isinstance(sem, dict) and "concepts" in sem:
        concept = "ï¼‹".join(sem["concepts"][:3])
    else:
        concept = ""

    return f"""
ä¸»è¦èªžç¾¤ï¼š{kws}
å¸‚å ´èªžï¼š{trend}
æ§‹æ–‡æŒ‡é‡ï¼š{concept}
æ–‡ä½“ãƒˆãƒ¼ãƒ³ï¼š{tone}
ç¦æ­¢èªžï¼š{forbid}
ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆåž‹ï¼š{tmpl}
"""

# ========== AIç”Ÿæˆ ==========
def ai_generate(product_name, knowledge_block):
    prompt = f"""
ã‚ãªãŸã¯ç†Ÿç·´ã—ãŸæ—¥æœ¬èªžã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚
ä»¥ä¸‹ã®å•†å“ã«ã¤ã„ã¦ã€SEOã¨è‡ªç„¶æ–‡ã‚’ä¸¡ç«‹ã—ãŸæ–‡ç« ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ã€å•†å“åã€‘{product_name}

å‡ºåŠ›ä»•æ§˜ï¼š
ãƒ»ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼ï¼ˆ40ã€œ60æ–‡å­—ï¼‰
ãƒ»ALTãƒ†ã‚­ã‚¹ãƒˆï¼ˆ80ã€œ110æ–‡å­— Ã—20ä»¶ï¼‰

ã€çŸ¥è¦‹è¦ç´„ã€‘
{knowledge_block}
"""

    try:
        res = client.chat.completions.create(
            model="gpt-5",
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯æ—¥æœ¬èªžãƒžãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚"},
                {"role": "user", "content": prompt}
            ],
            max_completion_tokens=1600,
            temperature=0.9,
        )

        txt = res.choices[0].message.content.strip()

        # ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼æŠ½å‡º
        copy_match = re.findall(r'ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼[:ï¼š]?(.*)', txt)
        copy_text = copy_match[0].strip() if copy_match else txt.split("\n")[0][:60]

        # ALTæŠ½å‡º
        alts = re.findall(r'ALT[0-9ï¼-ï¼™]?[ï¼š:]\s*(.*)', txt)
        alt_texts = [a.strip() for a in alts if len(a) > 0][:20]

        # ALTè£œå®Œ
        while len(alt_texts) < 20:
            alt_texts.append(f"{product_name} ã®é­…åŠ›ã‚’ä¼ãˆã‚‹å•†å“ç”»åƒ")

        return copy_text, alt_texts

    except Exception as e:
        print(f"âš ï¸ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return "ç”Ÿæˆå¤±æ•—", [f"{product_name} ã®å•†å“ç”»åƒ" for _ in range(20)]

# ========== ãƒ¡ã‚¤ãƒ³ ==========
def main():
    print("ðŸŒ¸ Hybrid AI Writer v5.8 Fixed å®Ÿè¡Œé–‹å§‹ï¼ˆå•†å“åˆ¥ã‚¯ãƒ©ã‚¹ã‚¿çŸ¥è¦‹ãƒ–ãƒªãƒƒã‚¸ï¼‰")

    df = pd.read_csv(INPUT_CSV, encoding="cp932")
    products = [p for p in df["å•†å“å"].dropna().unique().tolist()]
    print(f"âœ… å•†å“åæŠ½å‡º: {len(products)}ä»¶")

    cfg = load_knowledge_files()
    clusters = cfg["cluster"]

    out_records = []
    now = datetime.datetime.now().strftime("%Y%m%d_%H%M")

    for nm in tqdm(products, desc="ðŸ§  å•†å“åˆ¥AIç”Ÿæˆä¸­"):
        cluster = find_related_cluster(nm, clusters)
        kb = make_knowledge_block(cluster, cfg["market"], cfg["semantic"],
                                  cfg["persona"], cfg["normalized"], cfg["template"])
        copy, alts = ai_generate(nm, kb)

        print(f"ðŸ§  {nm[:25]}... â†’ Copy:{len(copy)}å­— / ALT:{len(alts)}ä»¶")

        out_records.append({
            "å•†å“å": nm,
            "ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼": copy,
            **{f"ALT{i+1}": alts[i] for i in range(20)}
        })

    out_json_path = f"{OUT_JSON}hybrid_writer_full_{now}.json"
    with open(out_json_path, "w", encoding="utf-8") as f:
        json.dump(out_records, f, ensure_ascii=False, indent=2)

    out_csv_path = f"{OUT_JSON}hybrid_writer_preview_{now}.csv"
    pd.DataFrame(out_records).to_csv(out_csv_path, encoding="cp932", index=False)

    print(f"âœ… å‡ºåŠ›å®Œäº†: {out_json_path}")
    print(f"âœ… ç›®è¦–ç¢ºèªç”¨CSV: {out_csv_path}")

if __name__ == "__main__":
    main()
import atlas_autosave_core
