# ===========================================
# ðŸŒ¸ KOTOHA ENGINE â€” Hybrid AI Writer v5.8 Fixed2
# GPT-5ä»•æ§˜æº–æ‹ ï¼ˆtemperature/max_completion_tokenså¯¾å¿œï¼‰
# ===========================================

import os, json, re, random, datetime
import pandas as pd
from openai import OpenAI
from dotenv import load_dotenv
from tqdm import tqdm

# ====== åŸºæœ¬è¨­å®š ======
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

INPUT_CSV = "./input.csv"
OUT_JSON = "./output/ai_writer/"
SEM_PATH = "./output/semantics/"

os.makedirs(OUT_JSON, exist_ok=True)

# ====== JSONãƒ­ãƒ¼ãƒ‰ ======
def load_json(path):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def load_knowledge_files():
    files = {
        "cluster": f"{SEM_PATH}lexical_clusters_20251030_223013.json",
        "market": f"{SEM_PATH}market_vocab_20251030_201906.json",
        "semantic": f"{SEM_PATH}structured_semantics_20251030_224846.json",
        "persona": f"{SEM_PATH}styled_persona_20251031_0031.json",
        "normalized": f"{SEM_PATH}normalized_20251031_0039.json",
        "template": f"{SEM_PATH}template_composer.json"
    }
    return {k: load_json(v) for k, v in files.items()}

# ====== ã‚¯ãƒ©ã‚¹ã‚¿ãƒžãƒƒãƒ ======
def find_related_cluster(product_name, clusters):
    for c in clusters:
        if any(k in product_name for k in c.get("keywords", [])):
            return c
    return random.choice(clusters)

# ====== çŸ¥è¦‹è¦ç´„ ======
def make_knowledge_block(cluster, market, sem, persona, normalized, template):
    kws = "ã€".join(cluster.get("keywords", [])[:5])
    trend = ""
    if isinstance(market, dict):
        for v in market.values():
            if isinstance(v, list):
                trend += "ã€".join(v)
    tone = persona[0].get("tone", "èª å®Ÿã§çŸ¥çš„") if isinstance(persona, list) else "èª å®Ÿã§çŸ¥çš„"
    forbid = "ã€".join(normalized[0].get("forbidden_words", [])) if isinstance(normalized, list) else ""
    tmpl = "ãƒ»".join(template.get("templates", [])[:3]) if isinstance(template, dict) else ""
    concept = "ï¼‹".join(sem.get("concepts", [])[:3]) if isinstance(sem, dict) else ""
    return f"""
ä¸»è¦èªžç¾¤ï¼š{kws}
å¸‚å ´èªžï¼š{trend[:120]}
æ§‹æ–‡æŒ‡é‡ï¼š{concept}
æ–‡ä½“ãƒˆãƒ¼ãƒ³ï¼š{tone}
ç¦æ­¢èªžï¼š{forbid}
ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆåž‹ï¼š{tmpl}
"""

# ====== AIç”Ÿæˆ ======
def ai_generate(product_name, knowledge_block):
    prompt = f"""
ã‚ãªãŸã¯ç†Ÿç·´ã—ãŸæ—¥æœ¬èªžã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚
ä»¥ä¸‹ã®å•†å“ã«ã¤ã„ã¦ã€SEOã¨è‡ªç„¶æ–‡ã‚’ä¸¡ç«‹ã—ãŸæ–‡ç« ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ã€å•†å“åã€‘{product_name}

å‡ºåŠ›ä»•æ§˜ï¼š
ãƒ»ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼ï¼ˆ40ã€œ60æ–‡å­—ï¼‰
ãƒ»ALTãƒ†ã‚­ã‚¹ãƒˆï¼ˆ80ã€œ110æ–‡å­— Ã—20ä»¶ï¼‰

ã€çŸ¥è¦‹è¦ç´„ã€‘
{knowledge_block}
"""

    try:
        res = client.chat.completions.create(
            model="gpt-5",
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯æ—¥æœ¬èªžãƒžãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚"},
                {"role": "user", "content": prompt}
            ],
            max_completion_tokens=1500  # â† GPT-5ä»•æ§˜ã«æº–æ‹ 
        )

        txt = res.choices[0].message.content.strip()
        copy_match = re.findall(r'ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼[:ï¼š]?(.*)', txt)
        copy_text = copy_match[0].strip() if copy_match else txt.split("\n")[0][:60]
        alts = re.findall(r'ALT[0-9ï¼-ï¼™]?[ï¼š:]\s*(.*)', txt)
        alt_texts = [a.strip() for a in alts if len(a) > 0][:20]
        while len(alt_texts) < 20:
            alt_texts.append(f"{product_name} ã®é­…åŠ›ã‚’ä¼ãˆã‚‹å•†å“ç”»åƒ")
        return copy_text, alt_texts

    except Exception as e:
        print(f"âš ï¸ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return "ç”Ÿæˆå¤±æ•—", [f"{product_name} ã®å•†å“ç”»åƒ" for _ in range(20)]

# ====== ãƒ¡ã‚¤ãƒ³ ======
def main():
    print("ðŸŒ¸ Hybrid AI Writer v5.8 Fixed2 å®Ÿè¡Œé–‹å§‹ï¼ˆGPT-5ä»•æ§˜å¯¾å¿œï¼‰")

    df = pd.read_csv(INPUT_CSV, encoding="cp932")
    products = [p for p in df["å•†å“å"].dropna().unique().tolist()]
    print(f"âœ… å•†å“åæŠ½å‡º: {len(products)}ä»¶")

    cfg = load_knowledge_files()
    clusters = cfg["cluster"]

    out_records = []
    now = datetime.datetime.now().strftime("%Y%m%d_%H%M")

    for nm in tqdm(products, desc="ðŸ§  å•†å“åˆ¥AIç”Ÿæˆä¸­"):
        cluster = find_related_cluster(nm, clusters)
        kb = make_knowledge_block(cluster, cfg["market"], cfg["semantic"],
                                  cfg["persona"], cfg["normalized"], cfg["template"])
        copy, alts = ai_generate(nm, kb)
        print(f"ðŸ§  {nm[:25]}... â†’ Copy:{len(copy)}å­— / ALT:{len(alts)}ä»¶")
        out_records.append({"å•†å“å": nm, "ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼": copy, **{f"ALT{i+1}": alts[i] for i in range(20)}})

    out_json_path = f"{OUT_JSON}hybrid_writer_full_{now}.json"
    with open(out_json_path, "w", encoding="utf-8") as f:
        json.dump(out_records, f, ensure_ascii=False, indent=2)

    out_csv_path = f"{OUT_JSON}hybrid_writer_preview_{now}.csv"
    pd.DataFrame(out_records).to_csv(out_csv_path, encoding="cp932", index=False)

    print(f"âœ… å‡ºåŠ›å®Œäº†: {out_json_path}")
    print(f"âœ… ç›®è¦–ç¢ºèªç”¨CSV: {out_csv_path}")

if __name__ == "__main__":
    main()
import atlas_autosave_core
