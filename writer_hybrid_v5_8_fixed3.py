# ===========================================
# ðŸŒ¸ KOTOHA ENGINE â€” Hybrid AI Writer v5.8 Fixed3
# GPT-4o-miniä»•æ§˜ãƒ»å¿œç­”ä¿è¨¼ãƒ»JSONå‡ºåŠ›åˆ¶å¾¡
# ===========================================

import os, json, re, random, datetime, time
import pandas as pd
from openai import OpenAI
from dotenv import load_dotenv
from tqdm import tqdm

load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

INPUT_CSV = "./input.csv"
OUT_JSON = "./output/ai_writer/"
SEM_PATH = "./output/semantics/"

os.makedirs(OUT_JSON, exist_ok=True)

# ====== JSONãƒ­ãƒ¼ãƒ‰ ======
def load_json(path):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def load_knowledge_files():
    files = {
        "cluster": f"{SEM_PATH}lexical_clusters_20251030_223013.json",
        "market": f"{SEM_PATH}market_vocab_20251030_201906.json",
        "semantic": f"{SEM_PATH}structured_semantics_20251030_224846.json",
        "persona": f"{SEM_PATH}styled_persona_20251031_0031.json",
        "normalized": f"{SEM_PATH}normalized_20251031_0039.json",
        "template": f"{SEM_PATH}template_composer.json"
    }
    return {k: load_json(v) for k, v in files.items()}

# ====== ã‚¯ãƒ©ã‚¹ã‚¿ ======
def find_related_cluster(product_name, clusters):
    for c in clusters:
        if any(k in product_name for k in c.get("keywords", [])):
            return c
    return random.choice(clusters)

# ====== çŸ¥è¦‹ãƒ–ãƒ­ãƒƒã‚¯ ======
def make_knowledge_block(cluster, market, sem, persona, normalized, template):
    kws = "ã€".join(cluster.get("keywords", [])[:5])
    trend = ""
    if isinstance(market, dict):
        for v in market.values():
            if isinstance(v, list):
                trend += "ã€".join(v)
    tone = persona[0].get("tone", "èª å®Ÿã§çŸ¥çš„") if isinstance(persona, list) else "èª å®Ÿã§çŸ¥çš„"
    forbid = "ã€".join(normalized[0].get("forbidden_words", [])) if isinstance(normalized, list) else ""
    tmpl = "ãƒ»".join(template.get("templates", [])[:3]) if isinstance(template, dict) else ""
    concept = "ï¼‹".join(sem.get("concepts", [])[:3]) if isinstance(sem, dict) else ""
    return f"""
ä¸»è¦èªžç¾¤ï¼š{kws}
å¸‚å ´èªžï¼š{trend[:120]}
æ§‹æ–‡æŒ‡é‡ï¼š{concept}
æ–‡ä½“ãƒˆãƒ¼ãƒ³ï¼š{tone}
ç¦æ­¢èªžï¼š{forbid}
ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆåž‹ï¼š{tmpl}
"""

# ====== AIç”Ÿæˆ ======
def ai_generate(product_name, knowledge_block, retries=2):
    prompt = f"""
ã‚ãªãŸã¯æ—¥æœ¬èªžãƒžãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã‚³ãƒ”ãƒ¼ã®å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®JSONå½¢å¼ã§è¿”ç­”ã—ã¦ãã ã•ã„ã€‚

å‡ºåŠ›å½¢å¼:
{{
  "copy": "ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼ï¼ˆ40ã€œ60æ–‡å­—ï¼‰",
  "alts": ["ALTãƒ†ã‚­ã‚¹ãƒˆï¼ˆ80ã€œ110æ–‡å­—ï¼‰ã‚’20ä»¶"]
}}

å•†å“å: {product_name}

çŸ¥è¦‹è¦ç´„:
{knowledge_block}
"""

    for attempt in range(retries):
        try:
            res = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯ãƒ—ãƒ­ã®æ—¥æœ¬èªžã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_completion_tokens=1200,
            )
            content = res.choices[0].message.content.strip() if res.choices else ""
            if not content:
                time.sleep(1)
                continue

            # JSONãƒ‘ãƒ¼ã‚¹
            try:
                j = json.loads(content)
                copy = j.get("copy", "").strip()
                alts = j.get("alts", [])
            except Exception:
                copy_match = re.findall(r'copy[:ï¼š]?(.*)', content)
                copy = copy_match[0].strip() if copy_match else content.split("\n")[0][:60]
                alts = re.findall(r'ALT[0-9ï¼-ï¼™]?[ï¼š:]\s*(.*)', content)
                alts = [a.strip() for a in alts if len(a) > 0]

            while len(alts) < 20:
                alts.append(f"{product_name} ã®é­…åŠ›ã‚’ä¼ãˆã‚‹å•†å“ç”»åƒ")
            return copy, alts

        except Exception as e:
            print(f"âš ï¸ è©¦è¡Œ{attempt+1}å¤±æ•—: {e}")
            time.sleep(2)

    return "ç”Ÿæˆå¤±æ•—", [f"{product_name} ã®å•†å“ç”»åƒ" for _ in range(20)]

# ====== ãƒ¡ã‚¤ãƒ³ ======
def main():
    print("ðŸŒ¸ Hybrid AI Writer v5.8 Fixed3 å®Ÿè¡Œé–‹å§‹ï¼ˆå¿œç­”ä¿è¨¼ãƒ¢ãƒ¼ãƒ‰ï¼‰")

    df = pd.read_csv(INPUT_CSV, encoding="cp932")
    products = [p for p in df["å•†å“å"].dropna().unique().tolist()]
    print(f"âœ… å•†å“åæŠ½å‡º: {len(products)}ä»¶")

    cfg = load_knowledge_files()
    clusters = cfg["cluster"]

    out_records = []
    now = datetime.datetime.now().strftime("%Y%m%d_%H%M")

    for nm in tqdm(products, desc="ðŸ§  å•†å“åˆ¥AIç”Ÿæˆä¸­"):
        cluster = find_related_cluster(nm, clusters)
        kb = make_knowledge_block(cluster, cfg["market"], cfg["semantic"],
                                  cfg["persona"], cfg["normalized"], cfg["template"])
        copy, alts = ai_generate(nm, kb)
        print(f"ðŸ§  {nm[:25]}... â†’ Copy:{len(copy)}å­— / ALT:{len(alts)}ä»¶")
        out_records.append({"å•†å“å": nm, "ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼": copy, **{f"ALT{i+1}": alts[i] for i in range(20)}})

    out_json_path = f"{OUT_JSON}hybrid_writer_full_{now}.json"
    with open(out_json_path, "w", encoding="utf-8") as f:
        json.dump(out_records, f, ensure_ascii=False, indent=2)

    out_csv_path = f"{OUT_JSON}hybrid_writer_preview_{now}.csv"
    pd.DataFrame(out_records).to_csv(out_csv_path, encoding="cp932", index=False)

    print(f"âœ… å‡ºåŠ›å®Œäº†: {out_json_path}")
    print(f"âœ… ç›®è¦–ç¢ºèªç”¨CSV: {out_csv_path}")

if __name__ == "__main__":
    main()
import atlas_autosave_core
