# -*- coding: utf-8 -*-
"""
writer_splitter_perfect_integrated.py
å…¨ä»¶AIï¼‹ãƒ­ãƒ¼ã‚«ãƒ«çŸ¥è¦‹è¦ç´„ï¼‹3åˆ†å‰²å‡ºåŠ›ï¼ˆæ¥½å¤©/Yahoo/ALT20ï¼‰

â–  å…¥åŠ›
- ./input.csvï¼ˆShift-JIS / å…ˆé ­è¡Œãƒ˜ãƒƒãƒ€ / ã€Œå•†å“åã€åˆ—ï¼‰
- ./output/semantics/*.jsonï¼ˆãƒ­ãƒ¼ã‚«ãƒ«çŸ¥è¦‹ãƒ•ã‚¡ã‚¤ãƒ«ç¾¤ï¼šå­˜åœ¨ã™ã‚‹ç¯„å›²ã§è‡ªå‹•å¸åï¼‰
  - lexical_clusters_*.json
  - structured_semantics_*.json
  - styled_persona_*.json
  - market_vocab_*.json
  - normalized_*.json
  - template_composer.jsonï¼ˆä»»æ„ï¼‰

â–  å‡ºåŠ›
- ./output/ai_writer/rakuten_copy_YYYYMMDD_HHMM.csvï¼ˆæ¨å¥¨60â€“80/ä¸Šé™87ï¼‰
- ./output/ai_writer/yahoo_copy_YYYYMMDD_HHMM.csvï¼ˆæ¨å¥¨25â€“30/ä¸Šé™30ï¼‰
- ./output/ai_writer/alt_text_YYYYMMDD_HHMM.csvï¼ˆALTÃ—20åˆ—/å„80â€“110ï¼‰
- ./output/ai_writer/split_full_YYYYMMDD_HHMM.jsonlï¼ˆAIå¿œç­”ã®ç”Ÿãƒ­ã‚°ï¼‰

â–  ãƒ¢ãƒ‡ãƒ«ã¨ã‚­ãƒ¼
- .env ã® OPENAI_API_KEY ã‚’ä½¿ç”¨
- .env ã® OPENAI_MODEL ã‚’å„ªå…ˆï¼ˆæœªè¨­å®šæ™‚ã¯ gpt-4-turboï¼‰
- â€»æ¸©åº¦ã¯æœªæŒ‡å®šï¼ˆãƒ¢ãƒ‡ãƒ«æ—¢å®šå€¤=1ï¼‰/ max_tokens ã¯ä½¿ç”¨ã›ãš max_completion_tokens ã‚’ä½¿ç”¨
"""

import os
import re
import io
import json
import glob
import time
import unicodedata
from datetime import datetime

import pandas as pd
from tqdm import tqdm

from dotenv import load_dotenv
from openai import OpenAI
from openai import APIError, RateLimitError, BadRequestError, OpenAIError

# =========================
# åŸºæœ¬ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =========================

def now_stamp():
    return datetime.now().strftime("%Y%m%d_%H%M")

def ensure_dirs():
    os.makedirs("./output/ai_writer", exist_ok=True)
    os.makedirs("./output/semantics", exist_ok=True)

def read_input_csv(path="./input.csv"):
    # Shift-JIS(cp932)ã§èª­ã¿è¾¼ã‚€ã€‚ç©ºç™½ã‚»ãƒ«ã¯ãã®ã¾ã¾ç©ºæ–‡å­—æ‰±ã„ã«ã™ã‚‹
    df = pd.read_csv(path, encoding="cp932", dtype=str, keep_default_na=False, na_filter=False)
    # ã€Œå•†å“åã€åˆ—ã‚’æ¢ã™ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰
    if "å•†å“å" not in df.columns:
        # ä¼¼ãŸåˆ—åã®æ•‘æ¸ˆï¼ˆã‚¹ãƒšãƒ¼ã‚¹æ··å…¥ãªã©ï¼‰
        candidates = [c for c in df.columns if str(c).strip() == "å•†å“å"]
        if candidates:
            name_col = candidates[0]
        else:
            raise ValueError("ãƒ˜ãƒƒãƒ€ã«ã€å•†å“åã€åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆé ­è¡Œãƒ˜ãƒƒãƒ€ãƒ»åˆ—åã€å•†å“åã€ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
    else:
        name_col = "å•†å“å"
    names = [str(x).strip() for x in list(df[name_col])]
    # ç©ºç™½ã‚¹ã‚­ãƒƒãƒ—ãƒ»é‡è¤‡é™¤å»ï¼ˆé †åºä¿æŒï¼‰
    uniq = []
    seen = set()
    for n in names:
        if not n:
            continue
        if n not in seen:
            seen.add(n)
            uniq.append(n)
    return uniq

def load_json_safe(path):
    try:
        with io.open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        try:
            with io.open(path, "r", encoding="cp932") as f:
                return json.load(f)
        except Exception:
            return None

def glob_first(pattern):
    files = sorted(glob.glob(pattern))
    return files[0] if files else None

def pick_semantics():
    # å­˜åœ¨ã™ã‚‹ã‚‚ã®ã ã‘å–ã‚Šè¾¼ã‚€ï¼ˆæ¬ ã‘ã¦ã„ã¦ã‚‚å‹•ãï¼‰
    base = "./output/semantics"
    bundle = {}

    paths = {
        "lexical": glob_first(f"{base}/lexical_clusters_*.json"),
        "semantic": glob_first(f"{base}/structured_semantics_*.json"),
        "persona": glob_first(f"{base}/styled_persona_*.json"),
        "market": glob_first(f"{base}/market_vocab_*.json"),
        "normalized": glob_first(f"{base}/normalized_*.json"),
        "template": os.path.join(base, "template_composer.json") if os.path.exists(os.path.join(base, "template_composer.json")) else None,
    }
    for k, p in paths.items():
        bundle[k] = load_json_safe(p) if p else None
    return bundle

def to_str_list(x):
    # list[str] ã¸å¯„ã›ã‚‹
    if x is None:
        return []
    if isinstance(x, list):
        out = []
        for v in x:
            if isinstance(v, dict):
                # å€™è£œã«ãªã‚Šãã†ãªã‚­ãƒ¼ã‚’æ‹¾ã†
                for key in ("text","vocabulary","word","value","label","name"):
                    if key in v and isinstance(v[key], str):
                        out.append(v[key])
                        break
            elif isinstance(v, str):
                out.append(v)
        return out
    if isinstance(x, dict):
        # å€™è£œã«ãªã‚Šãã†ãªã‚­ãƒ¼ã‚’æ‹¾ã†
        for key in ("list","items","values","words","vocab","entries"):
            if key in x and isinstance(x[key], list):
                return to_str_list(x[key])
        return []
    if isinstance(x, str):
        return [x]
    return []

def jlen(s: str) -> int:
    # ã–ã£ãã‚Šå…¨è§’/åŠè§’ã®åŒºåˆ¥ãªãæ–‡å­—æ•°ã‚«ã‚¦ãƒ³ãƒˆï¼ˆè¦ä»¶ä¸Šã¯å…¨è§’ä¸Šé™ã ãŒã€å®‰å®šã®ãŸã‚lenã§é‹ç”¨ï¼‰
    return len(s)

def smart_truncate(text, max_len):
    if jlen(text) <= max_len:
        return text
    # å¥ç‚¹ãƒ»èª­ç‚¹ãƒ»ä¸­ç‚¹ãƒ»ç´„ç‰©ã§æ‰‹å‰ã‚«ãƒƒãƒˆ
    cut = text[:max_len]
    # æœ«å°¾ã‚’æ•´ãˆã‚‹
    cut = re.sub(r'[ã€ã€‚ãƒ»,.;:ï¼šï¼›ã€ã€‚â€¦ãƒ¼\-]\s*$', '', cut)
    return cut

def enforce_range(text, min_len, max_len):
    t = text.strip()
    if jlen(t) > max_len:
        t = smart_truncate(t, max_len)
    # è¶³ã‚Šãªã„æ™‚ã¯ãã®ã¾ã¾è¿”ã™ï¼ˆå†ç”Ÿæˆã¯ä¸Šä½ã§ï¼‰
    return t

def dedupe_preserve_order(seq):
    out, seen = [], set()
    for s in seq:
        if s not in seen:
            seen.add(s)
            out.append(s)
    return out

# =========================
# çŸ¥è¦‹è¦ç´„ãƒ–ãƒ­ãƒƒã‚¯ã®æ§‹ç¯‰
# =========================

def summarize_knowledge(name, kb):
    """
    å„JSONã®ä¸­èº«ãŒ list/dict ã„ãšã‚Œã§ã‚‚æ‹¾ãˆã‚‹ã‚ˆã†ã«è»½é‡è¦ç´„ã«æ•´å½¢
    """
    persona = kb.get("persona")
    lexical = kb.get("lexical")
    semantic = kb.get("semantic")
    market = kb.get("market")
    normalized = kb.get("normalized")
    template = kb.get("template")

    # persona
    tone_words = []
    if persona:
        if isinstance(persona, dict):
            for key in ("tone","style","voice","writing","brand","guidelines"):
                tone_words += to_str_list(persona.get(key))
        elif isinstance(persona, list):
            tone_words += to_str_list(persona)

    # lexical
    clusters = []
    if lexical:
        if isinstance(lexical, dict):
            for key in ("clusters","keywords","seed","lexicon","terms","phrases"):
                clusters += to_str_list(lexical.get(key))
        elif isinstance(lexical, list):
            clusters += to_str_list(lexical)

    # semantic
    concepts = []
    if semantic:
        if isinstance(semantic, dict):
            for key in ("concepts","semantics","frames","features","facets","benefits","scenes"):
                concepts += to_str_list(semantic.get(key))
        elif isinstance(semantic, list):
            concepts += to_str_list(semantic)

    # market
    trend = []
    scenes = []
    audience = []
    if market:
        if isinstance(market, dict):
            trend += to_str_list(market.get("vocabulary"))
            scenes += to_str_list(market.get("scenes"))
            audience += to_str_list(market.get("audience"))
        elif isinstance(market, list):
            trend += to_str_list(market)
    # normalized
    forbidden = []
    if normalized:
        if isinstance(normalized, dict):
            for key in ("forbidden_words","banned","ng","prohibited"):
                forbidden += to_str_list(normalized.get(key))
        elif isinstance(normalized, list):
            forbidden += to_str_list(normalized)

    # ç”»åƒæå†™ãƒ¯ãƒ¼ãƒ‰ç¦æ­¢ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ˜ç¢ºãªæ–¹é‡ï¼‰
    forbidden += ["ç”»åƒ", "å†™çœŸ", "ãƒ•ãƒ¬ãƒ¼ãƒ ", "æ§‹å›³", "è¢«å†™ä½“", "ç”»è§’", "è§£åƒåº¦", "ãƒ”ã‚¯ã‚»ãƒ«", "èƒŒæ™¯", "ç™½èƒŒæ™¯", "ã‚¤ãƒ¡ãƒ¼ã‚¸å›³", "ãƒœã‚±", "ã‚·ãƒ«ã‚¨ãƒƒãƒˆ"]

    # è»½é‡è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆï¼ˆAIã¸æœ€å°é™ï¼‰
    block = {
        "persona_tone": dedupe_preserve_order([w for w in tone_words if w])[:20],
        "lexical_hints": dedupe_preserve_order([w for w in clusters if w])[:40],
        "semantic_hints": dedupe_preserve_order([w for w in concepts if w])[:40],
        "market_trend": dedupe_preserve_order([w for w in trend if w])[:30],
        "market_scenes": dedupe_preserve_order([w for w in scenes if w])[:20],
        "market_audience": dedupe_preserve_order([w for w in audience if w])[:20],
        "forbidden": dedupe_preserve_order([w for w in forbidden if w]),
        "template_note": "ãƒ†ãƒ³ãƒ—ãƒ¬æ„Ÿã¯å‡ºã•ãªã„ãŒã€æ§‹æˆè¦ç´ ï¼ˆspec/competence/user/scene/benefitï¼‰ã¯æ„è­˜ã—ã¦è‡ªç„¶æ–‡ã«æº¶ã‹ã—è¾¼ã‚€ã€‚"
    }
    # æ–‡å­—åˆ—åŒ–ã—ã¦è¿”ã™ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸ‹ã‚ã‚„ã™ãï¼‰
    return json.dumps(block, ensure_ascii=False)

# =========================
# OpenAI ãƒ©ãƒƒãƒ‘
# =========================

def load_openai_client():
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY", "").strip()
    if not api_key:
        raise RuntimeError("OPENAI_API_KEY ãŒæœªè¨­å®šã§ã™ï¼ˆ.env ã‚’ç¢ºèªï¼‰")
    model = os.getenv("OPENAI_MODEL", "gpt-4-turbo").strip()
    client = OpenAI(api_key=api_key)
    return client, model

JSON_SCHEMA_HINT = """
å‡ºåŠ›ã¯å¿…ãšæ¬¡ã®JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ1å€‹ã§è¿”ã—ã¦ãã ã•ã„ï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ä¸å¯ï¼‰ï¼š
{
  "rakuten": "<60ã€œ80å­—æ¨å¥¨ãƒ»ä¸Šé™87å­—>",
  "yahoo": "<25ã€œ30å­—æ¨å¥¨ãƒ»ä¸Šé™30å­—>",
  "alts": ["<ALT1(80ã€œ110å­—)>", "...", "<ALT20(80ã€œ110å­—)>"]
}
æ³¨æ„ï¼š
- alts ã¯å¿…ãš20ä»¶ã€‚å†…å®¹ã¯ã™ã¹ã¦ãƒ¦ãƒ‹ãƒ¼ã‚¯ã€‚ç”»åƒãƒ»æ§‹å›³ãƒ»ç”»è§’ãªã©ã®èªã¯ä½¿ç”¨ç¦æ­¢ã€‚
- ç¦æ­¢èªï¼ˆforbiddenï¼‰ã‚’å«ã‚ãªã„ã“ã¨ã€‚
- å¥èª­ç‚¹ï¼ˆã€‚ï¼‰ã€èª­ç‚¹ï¼ˆã€ï¼‰ã‚’é©åˆ‡ã«ä½¿ã„è‡ªç„¶æ–‡ã«ã™ã‚‹ã“ã¨ã€‚
- ã€Œç«¶åˆå„ªä½æ€§ã€ãªã©ã®å†…éƒ¨èªã¯æ¶ˆè²»è€…å‘ã‘è‡ªç„¶è¡¨ç¾ã«è¨€ã„æ›ãˆã‚‹ã“ã¨ï¼ˆä¾‹ï¼šã€Œä»–ã¨æ¯”ã¹ã¦ä½¿ã„ã‚„ã™ã„ã€ãªã©ï¼‰ã€‚
- spec / competence / user / scene / benefit ã®è¦ç´ ã‚’è‡ªç„¶ã«æº¶ã‘è¾¼ã¾ã›ã‚‹ï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬æ„Ÿã¯å‡ºã•ãªã„ï¼‰ã€‚
"""

def build_messages(product_name, knowledge_json_text):
    system = (
        "ã‚ãªãŸã¯æ—¥æœ¬èªã®ECå•†å“ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚"
        "ä»¥ä¸‹ã®åˆ¶ç´„ã‚’å³æ ¼ã«å®ˆã£ã¦ãã ã•ã„ï¼š\n"
        "â€¢ ç”»åƒæå†™ãƒ»æ§‹å›³ãƒ»ç”»è§’ãƒ»è§£åƒåº¦ãªã©ã®ç”¨èªã‚’ä½¿ã‚ãªã„\n"
        "â€¢ ç¦æ­¢èªï¼ˆforbiddenï¼‰ã«å«ã¾ã‚Œã‚‹èªã‚’ä½¿ã‚ãªã„\n"
        "â€¢ æ¥½å¤©ã¯60ã€œ80å­—ã‚’æ¨å¥¨ã€ä¸Šé™87å­—\n"
        "â€¢ Yahooã¯25ã€œ30å­—ã€ä¸Šé™30å­—\n"
        "â€¢ ALTã¯å„80ã€œ110å­—ã€ã¡ãŒã†è¦–ç‚¹ã®20æœ¬\n"
        "â€¢ spec/competence/user/scene/benefitã‚’è‡ªç„¶ã«ç¹”ã‚Šäº¤ãœã‚‹\n"
        "â€¢ ãƒ†ãƒ³ãƒ—ãƒ¬æ„Ÿã¯å‡ºã•ãªã„\n"
        "â€¢ èª­ç‚¹ãƒ»å¥ç‚¹ã§æ—¥æœ¬èªã¨ã—ã¦é•å’Œæ„Ÿã®ãªã„ä¸€æ–‡ã«ã™ã‚‹\n"
    )
    user = (
        f"å¯¾è±¡å•†å“åï¼š{product_name}\n"
        f"å‚è€ƒçŸ¥è¦‹ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«è¦ç´„ï¼‰ï¼š{knowledge_json_text}\n"
        f"{JSON_SCHEMA_HINT}\n"
        "ã•ã‚ã€JSONã ã‘ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚"
    )
    return [
        {"role": "system", "content": system},
        {"role": "user",   "content": user}
    ]

def try_json_mode(client, model, messages, max_completion_tokens=700):
    """
    response_format={'type':'json_object'} ã‚’è©¦ã™ã€‚
    å¤±æ•—ã—ãŸã‚‰ä¾‹å¤–ã‚’æŠ•ã’ã‚‹ï¼ˆä¸Šä½ã§ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    """
    return client.chat.completions.create(
        model=model,
        messages=messages,
        # æ¸©åº¦ã¯æœªæŒ‡å®šï¼ˆãƒ¢ãƒ‡ãƒ«æ—¢å®šå€¤=1 å›ºå®šå•é¡Œã‚’å›é¿ï¼‰
        response_format={"type": "json_object"},
        max_completion_tokens=max_completion_tokens,
    )

def try_text_mode(client, model, messages, max_completion_tokens=700):
    """
    é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒ†ã‚­ã‚¹ãƒˆï¼‰ã§JSONã‚’è¿”ã—ã¦ã‚‚ã‚‰ã†
    """
    return client.chat.completions.create(
        model=model,
        messages=messages,
        max_completion_tokens=max_completion_tokens,
    )

def extract_json_from_text(text):
    # æœ€åˆã® { ã‹ã‚‰æœ€å¾Œã® } ã¾ã§ã‚’è²ªæ¬²ã«å–å¾—
    m = re.search(r'\{.*\}', text, flags=re.S)
    if not m:
        return None
    chunk = m.group(0)
    try:
        return json.loads(chunk)
    except Exception:
        # æœ«å°¾ , ã®é™¤å»ãªã©è»½ã„æ‰‹å½“ã¦
        chunk = re.sub(r',\s*}', '}', chunk)
        chunk = re.sub(r',\s*]', ']', chunk)
        try:
            return json.loads(chunk)
        except Exception:
            return None

def call_openai_for_product(client, model, product_name, knowledge_json_text, logf):
    messages = build_messages(product_name, knowledge_json_text)

    # ã¾ãšJSONãƒ¢ãƒ¼ãƒ‰ã‚’è©¦ã™
    for attempt in range(2):
        try:
            res = try_json_mode(client, model, messages, max_completion_tokens=900)
            raw = res.choices[0].message.content or ""
            if logf:
                logf.write(json.dumps({"product": product_name, "mode": "json", "raw": raw}, ensure_ascii=False) + "\n")
            data = json.loads(raw)
            return data
        except BadRequestError as e:
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæœªå¯¾å¿œãƒ¢ãƒ‡ãƒ«ãªã© â†’ ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã¸
            if logf:
                logf.write(json.dumps({"product": product_name, "mode": "json_error", "error": str(e)}, ensure_ascii=False) + "\n")
            break
        except (APIError, RateLimitError, OpenAIError) as e:
            if logf:
                logf.write(json.dumps({"product": product_name, "mode": "json_api_error", "error": str(e)}, ensure_ascii=False) + "\n")
            time.sleep(2)
        except Exception as e:
            if logf:
                logf.write(json.dumps({"product": product_name, "mode": "json_unknown", "error": str(e)}, ensure_ascii=False) + "\n")
            break

    # ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã§JSONæŠ½å‡º
    for attempt in range(3):
        try:
            res = try_text_mode(client, model, messages, max_completion_tokens=900)
            raw = res.choices[0].message.content or ""
            if logf:
                logf.write(json.dumps({"product": product_name, "mode": "text", "raw": raw}, ensure_ascii=False) + "\n")
            data = extract_json_from_text(raw)
            if data:
                return data
        except (APIError, RateLimitError, OpenAIError) as e:
            if logf:
                logf.write(json.dumps({"product": product_name, "mode": "text_api_error", "error": str(e)}, ensure_ascii=False) + "\n")
            time.sleep(2)
        except Exception as e:
            if logf:
                logf.write(json.dumps({"product": product_name, "mode": "text_unknown", "error": str(e)}, ensure_ascii=False) + "\n")
            time.sleep(1)

    return None

# =========================
# ãƒ­ãƒ¼ã‚«ãƒ«æ•´å½¢ï¼ˆå®‰å…¨å¼ï¼‰
# =========================

def cleanse_forbidden(text, forbidden):
    t = text.strip()
    if not forbidden:
        return t
    for w in forbidden:
        if not w:
            continue
        t = t.replace(w, "")
    # é€£ç¶šã‚¹ãƒšãƒ¼ã‚¹ãƒ»å¥èª­ç‚¹æ•´å½¢
    t = re.sub(r'\s+', ' ', t)
    t = re.sub(r'[ã€ã€‚]{2,}', 'ã€‚', t)
    return t.strip()

def local_refine(product_name, raw_obj, forbidden):
    """
    - é•·ã•åˆ¶ç´„ã®æœ€çµ‚ç¢ºèª
    - ç¦æ­¢èªå‰Šé™¤
    - ALTã®æœ¬æ•°ãƒ»ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–
    """
    rak = (raw_obj.get("rakuten") or "").strip()
    yah = (raw_obj.get("yahoo") or "").strip()
    alts = raw_obj.get("alts") or []

    # Rakuten: æ¨å¥¨60â€“80 / ä¸Šé™87
    rak = cleanse_forbidden(rak, forbidden)
    rak = enforce_range(rak, 60, 87)

    # Yahoo: æ¨å¥¨25â€“30 / ä¸Šé™30
    yah = cleanse_forbidden(yah, forbidden)
    yah = enforce_range(yah, 25, 30)

    # ALT: 20æœ¬ / å„80â€“110
    alts = [cleanse_forbidden(a or "", forbidden) for a in alts if isinstance(a, str)]
    alts = [enforce_range(a, 80, 110) for a in alts if a]
    alts = [a for a in alts if a]  # éç©º
    # ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–
    alts = dedupe_preserve_order(alts)
    # è¶³ã‚Šãªã„å ´åˆã¯Rak/Yahã‚’å¤‰æ›ã—ã¦è£œå®Œ
    while len(alts) < 20:
        # å¤‰å½¢ã—ã¦è£œå®Œï¼ˆå˜ç´”ã«èªå°¾ã‚„é †åºã‚’å°‘ã—æ›¿ãˆã‚‹ï¼‰
        base = rak if (len(alts) % 2 == 0 and rak) else yah
        if not base:
            base = product_name + " ã‚’ã‚ˆã‚Šå¿«é©ã«ä½¿ãˆã‚‹ã‚ˆã†ã«é…æ…®ã•ã‚ŒãŸè¨­è¨ˆã§ã€æ—¥å¸¸ã®ä¸æº€ã‚’æ¸›ã‚‰ã—ä¾¿åˆ©ã•ã‚’å®Ÿæ„Ÿã§ãã¾ã™ã€‚"
        variant = base
        # è»½å¤‰å½¢ï¼šèª­ç‚¹ã‚’è¿½åŠ ï¼åŠ©è©å…¥ã‚Œæ›¿ãˆ
        variant = variant.replace("ã€", "ã€")
        if jlen(variant) < 80:
            variant += " æ¯æ—¥ã®ä½¿ç”¨ã§å·®ãŒå‡ºã‚‹è¨­è¨ˆã§ã€ä½¿ã†ãŸã³ã«å¿«é©ã•ã‚’æ„Ÿã˜ã‚‰ã‚Œã¾ã™ã€‚"
        variant = enforce_range(variant, 80, 110)
        alts.append(variant)
    if len(alts) > 20:
        alts = alts[:20]

    return rak, yah, alts

# =========================
# æœ¬ä½“
# =========================

def main():
    ensure_dirs()
    client, model = load_openai_client()
    names = read_input_csv("./input.csv")
    kb = pick_semantics()

    out_time = now_stamp()
    path_rakuten = f"./output/ai_writer/rakuten_copy_{out_time}.csv"
    path_yahoo  = f"./output/ai_writer/yahoo_copy_{out_time}.csv"
    path_alt    = f"./output/ai_writer/alt_text_{out_time}.csv"
    path_log    = f"./output/ai_writer/split_full_{out_time}.jsonl"

    knowledge_forbidden_union = []
    if kb.get("normalized"):
        # æ­£è¦åŒ–ã®ç¦æ­¢èªé›†åˆï¼ˆã‚µãƒãƒ©ã‚¤ã‚¶ã§ã‚‚å¸åã—ã¦ã„ã‚‹ãŒã€ã“ã“ã§ã¯ç”Ÿã‚‚è¦‹ã‚‹ï¼‰
        if isinstance(kb["normalized"], dict):
            for key in ("forbidden_words","banned","ng","prohibited"):
                knowledge_forbidden_union += to_str_list(kb["normalized"].get(key))
        elif isinstance(kb["normalized"], list):
            knowledge_forbidden_union += to_str_list(kb["normalized"])
    # ç”»åƒæå†™ãƒ¯ãƒ¼ãƒ‰ã‚‚è¿½åŠ ï¼ˆå¿µæŠ¼ã—ï¼‰
    knowledge_forbidden_union += ["ç”»åƒ", "å†™çœŸ", "ãƒ•ãƒ¬ãƒ¼ãƒ ", "æ§‹å›³", "è¢«å†™ä½“", "ç”»è§’", "è§£åƒåº¦", "ãƒ”ã‚¯ã‚»ãƒ«", "èƒŒæ™¯", "ç™½èƒŒæ™¯", "ã‚¤ãƒ¡ãƒ¼ã‚¸å›³", "ãƒœã‚±", "ã‚·ãƒ«ã‚¨ãƒƒãƒˆ"]
    knowledge_forbidden_union = dedupe_preserve_order([w for w in knowledge_forbidden_union if w])

    print(f"ğŸŒ¸ writer_splitter_perfect_integrated å®Ÿè¡Œé–‹å§‹ï¼ˆå…¨ä»¶AIï¼‹çŸ¥è¦‹è¦ç´„ï¼‹3åˆ†å‰²ï¼‰")
    print(f"âœ… å•†å“åæŠ½å‡º: {len(names)}ä»¶ï¼ˆé‡è¤‡é™¤å»æ¸ˆï¼‰")

    # å‡ºåŠ›CSVã‚’å…ˆã«ç”¨æ„ï¼ˆè¿½è¨˜ã—ã¦ã„ãï¼‰
    df_rak = pd.DataFrame([], columns=["å•†å“å","æ¥½å¤©ã‚³ãƒ”ãƒ¼"])
    df_yah = pd.DataFrame([], columns=["å•†å“å","Yahooã‚³ãƒ”ãƒ¼"])
    # ALTã¯æ¨ªæŒã¡20åˆ—
    alt_cols = ["å•†å“å"] + [f"ALT{i}" for i in range(1, 21)]
    df_alt = pd.DataFrame([], columns=alt_cols)

    # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«
    logf = io.open(path_log, "w", encoding="utf-8")

    pbar = tqdm(total=len(names), desc="ğŸ§  å•†å“åˆ¥AIç”Ÿæˆä¸­", ncols=88)
    for nm in names:
        try:
            # ãƒ­ãƒ¼ã‚«ãƒ«çŸ¥è¦‹ã‚’è»½é‡è¦ç´„ã«
            knowledge_text = summarize_knowledge(nm, kb)
            # AIå‘¼ã³å‡ºã—
            data = call_openai_for_product(client, model, nm, knowledge_text, logf)
            if not data or not isinstance(data, dict):
                # ç©ºå¿œç­”â†’ç°¡æ˜“ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                data = {
                    "rakuten": f"{nm} ã®ç‰¹é•·ã‚’æ´»ã‹ã—ã€æ—¥å¸¸ã®ä¸æº€ã‚’æ¸›ã‚‰ã—ã¦å¿«é©ã«ä½¿ãˆã‚‹ã‚ˆã†é…æ…®ã—ãŸè¨­è¨ˆã§ã™ã€‚",
                    "yahoo": f"{nm} ã®ä½¿ã„ã‚„ã™ã•ã«é…æ…®ã—ãŸè¨­è¨ˆã€‚",
                    "alts": []
                }
            # ãƒ­ãƒ¼ã‚«ãƒ«æœ€çµ‚æ•´å½¢
            rak, yah, alts = local_refine(nm, data, knowledge_forbidden_union)

            # è¡Œè¿½åŠ 
            df_rak.loc[len(df_rak)] = [nm, rak]
            df_yah.loc[len(df_yah)] = [nm, yah]
            row = [nm] + alts
            df_alt.loc[len(df_alt)] = row

            pbar.set_postfix_str(f"{nm[:16]}â€¦ â†’ R:{jlen(rak)} / Y:{jlen(yah)} / ALT20")
        except KeyboardInterrupt:
            logf.write(json.dumps({"product": nm, "event": "keyboard_interrupt"}, ensure_ascii=False) + "\n")
            break
        except Exception as e:
            logf.write(json.dumps({"product": nm, "event": "exception", "error": str(e)}, ensure_ascii=False) + "\n")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ç©ºæ¬„è¿½åŠ ï¼ˆå¾Œã§å†ç”Ÿæˆå¯èƒ½ã«ï¼‰
            df_rak.loc[len(df_rak)] = [nm, ""]
            df_yah.loc[len(df_yah)] = [nm, ""]
            df_alt.loc[len(df_alt)] = [nm] + [""]*20
        finally:
            pbar.update(1)
    pbar.close()
    logf.close()

    # å‡ºåŠ›ï¼ˆExcel/Windowsäº’æ›ã®ãŸã‚BOMä»˜ãUTF-8ï¼‰
    df_rak.to_csv(path_rakuten, index=False, encoding="utf-8-sig")
    df_yah.to_csv(path_yahoo,  index=False, encoding="utf-8-sig")
    df_alt.to_csv(path_alt,    index=False, encoding="utf-8-sig")

    print("âœ… å‡ºåŠ›å®Œäº†:")
    print(f"   - æ¥½å¤©: {path_rakuten}")
    print(f"   - Yahoo: {path_yahoo}")
    print(f"   - ALT20: {path_alt}")
    print(f"   - JSONL: {path_log}")
    print("âœ… å…±é€šALT20ã¯ã€alt_text_*.csvã€ã«å…¨å•†å“ã¶ã‚“ã‚’æ¨ªæŒã¡ã§æ›¸ãå‡ºã—ã¾ã™ã€‚")

if __name__ == "__main__":
    main()
import atlas_autosave_core
