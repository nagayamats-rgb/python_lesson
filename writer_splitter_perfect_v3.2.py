# -*- coding: utf-8 -*-
"""
writer_splitter_perfect_v3.2.py
- å…¨ä»¶AIï¼‹çŸ¥è¦‹è¦ç´„ï¼‹3åˆ†å‰²ï¼ˆæ¥½å¤©ã¯æ¸©å­˜ã€Yahoo/ALTã®ã¿å†ç”Ÿæˆï¼‰
- å…¥åŠ›: ./input.csvï¼ˆShift-JIS, ãƒ˜ãƒƒãƒ€è¡Œã‚ã‚Š, ã€Œå•†å“åã€åˆ—ã‚’ç¸¦èµ°æŸ»ï¼‰
- å‡ºåŠ›: ./output/ai_writer/{rakuten_copy_*.csv, yahoo_copy_*.csv, alt_text_*.csv, split_full_*.jsonl}
"""

import os, re, csv, glob, json, time, datetime
from pathlib import Path

# ---- OpenAI SDK ----
from openai import OpenAI, OpenAIError

# ---- é€²æ—ãƒãƒ¼ï¼ˆtqdmãŒç„¡ã„ç’°å¢ƒã§ã‚‚å‹•ä½œï¼‰----
try:
    from tqdm import tqdm
except ImportError:
    tqdm = None

# ---------- åŸºæœ¬è¨­å®š ----------
BASE = Path(".")
AI_OUT = BASE / "output" / "ai_writer"
AI_OUT.mkdir(parents=True, exist_ok=True)

SEM_OUT = BASE / "output" / "semantics"  # çŸ¥è¦‹JSONãŒã‚ã‚‹æƒ³å®šã®ãƒ•ã‚©ãƒ«ãƒ€
INPUT_CSV = BASE / "input.csv"

# ãƒ¢ãƒ‡ãƒ«ï¼ˆGPT-4ç³»ã§å®‰å®šé‹ç”¨ï¼‰
MODEL = os.environ.get("OPENAI_MODEL", "gpt-4o-mini")

# ç¦å‰‡èªï¼ˆç”»åƒæå†™ãƒ»ä¸é©åˆ‡èªãƒ»ä¸è¦è¡¨ç¾ãªã©ï¼‰
FORBIDDEN = set([
    "ç”»åƒ", "å†™çœŸ", "ãƒ•ã‚©ãƒˆ", "ã‚¤ãƒ¡ãƒ¼ã‚¸", "è¦‹ãŸç›®", "æ˜ ã£ã¦ã„ã‚‹", "æ˜ åƒ",
    "ç«¶åˆå„ªä½æ€§", "ä»–ç¤¾å„ªä½æ€§", "ç¤¾å¤–ç§˜", "æ³¨æ„å–šèµ·", "â€»ç”»åƒã¯ã‚¤ãƒ¡ãƒ¼ã‚¸ã§ã™",
    "ã‚¯ãƒªãƒƒã‚¯", "ä¸Šã®å†™çœŸ", "ä¸‹ã®ç”»åƒ",
])

# å¥èª­ç‚¹ã‚»ãƒƒãƒˆï¼ˆALTã®ç¶ºéº—ãªçŸ­ç¸®ã«ä½¿ç”¨ï¼‰
SENT_END = "ã€‚ï¼ï¼!ï¼Ÿ?ï¼›;"

# ---------- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ----------

def nowstamp():
    return datetime.datetime.now().strftime("%Y%m%d_%H%M")

def load_csv_shiftjis(path: Path):
    """Shift-JISã§CSVèª­ã¿è¾¼ã¿ã—ã¦å…¨è¡Œè¿”ã™"""
    if not path.exists():
        raise FileNotFoundError(f"å…¥åŠ›ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path}")
    rows = []
    with open(path, "r", encoding="cp932", errors="ignore", newline="") as f:
        reader = csv.reader(f)
        rows = [r for r in reader]
    return rows

def find_header_and_column(rows, header_name="å•†å“å"):
    """ãƒ˜ãƒƒãƒ€è¡Œã‚’è¦‹ã¤ã‘ã€æŒ‡å®šåˆ—ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¿”ã™"""
    if not rows:
        return None, None
    header = rows[0]
    if header_name not in header:
        raise KeyError(f"ãƒ˜ãƒƒãƒ€ã«ã€Œ{header_name}ã€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    return header, header.index(header_name)

def extract_product_names(rows, col_idx):
    """ãƒ˜ãƒƒãƒ€ã®æ¬¡è¡Œã‹ã‚‰å•†å“ååˆ—ã‚’èµ°æŸ»ã€ç©ºç™½ç„¡è¦–ã§æŠ½å‡º"""
    names = []
    for r in rows[1:]:
        if col_idx >= len(r):
            continue
        nm = (r[col_idx] or "").strip()
        if nm:
            names.append(nm)
    # é‡è¤‡é™¤å»ï¼ˆé †åºç¶­æŒï¼‰
    seen = set()
    uniq = []
    for n in names:
        if n not in seen:
            seen.add(n)
            uniq.append(n)
    return uniq

def safe_json_load(path: Path, default=None):
    default = default if default is not None else {}
    if not path.exists():
        return default
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return default

def summarize_knowledge():
    """
    ä»»æ„ã®ãƒ­ãƒ¼ã‚«ãƒ«çŸ¥è¦‹JSONã‚’èª­ã¿ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨ã«è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆï¼ˆå­˜åœ¨ã—ãªã‘ã‚Œã°ç©ºï¼‰
    å¯¾å¿œãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå­˜åœ¨ã™ã‚Œã°ä½¿ã†ï¼‰:
      - lexical_clusters_*.jsonï¼ˆèªå½™/ã‚¯ãƒ©ã‚¹ã‚¿ï¼‰
      - structured_semantics_*.jsonï¼ˆæ¦‚å¿µ/æ§‹é€ ï¼‰
      - styled_persona_*.jsonï¼ˆèª¿å­/ãƒˆãƒ¼ãƒ³ï¼‰
      - market_vocab_*.jsonï¼ˆå¸‚å ´èªå½™/æµè¡Œï¼‰
      - normalized_*.jsonï¼ˆç¦å‰‡/æ­£è¦åŒ–ï¼‰
    """
    # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ‹¾ã†ãƒ˜ãƒ«ãƒ‘
    def latest(pattern):
        files = sorted(glob.glob(str(SEM_OUT / pattern)))
        return Path(files[-1]) if files else None

    lex = safe_json_load(latest("lexical_clusters_*.json") or Path(""), default=[])
    sem = safe_json_load(latest("structured_semantics_*.json") or Path(""), default={})
    per = safe_json_load(latest("styled_persona_*.json") or Path(""), default={})
    mar = safe_json_load(latest("market_vocab_*.json") or Path(""), default=[])
    nor = safe_json_load(latest("normalized_*.json") or Path(""), default={})

    # ã–ã£ãã‚Šè¦ç´„
def cap_list(xs, key=None, cap=10):
    out = []
    # â˜… ä¿®æ­£ï¼šdictãªã‚‰values()ã‚’å–ã£ã¦ãƒªã‚¹ãƒˆåŒ–
    if isinstance(xs, dict):
        xs = list(xs.values())
    for x in xs[:cap]:
        if isinstance(x, dict) and key:
            v = x.get(key, "")
        else:
            v = str(x)
        v = (v or "").strip()
        if v:
            out.append(v)
    return "ã€".join(out)

    trend = cap_list(mar, key="vocabulary", cap=12)
    clusters = []
    if isinstance(lex, dict) and "clusters" in lex and isinstance(lex["clusters"], list):
        clusters = [c.get("name", "") for c in lex["clusters"] if isinstance(c, dict)]
    elif isinstance(lex, list):
        clusters = [x.get("name","") if isinstance(x, dict) else str(x) for x in lex]
    clusters_str = "ã€".join([x for x in clusters[:12] if x])

    concepts = []
    if isinstance(sem, dict):
        for k,v in sem.items():
            if isinstance(v, (str, int, float)):
                concepts.append(f"{k}:{v}")
            elif isinstance(v, list):
                concepts.append(f"{k}:{'|'.join(map(str,v[:3]))}")
            elif isinstance(v, dict):
                concepts.append(f"{k}:{'|'.join(list(v.keys())[:3])}")
    concepts_str = "ã€".join(concepts[:12])

    tone = ""
    if isinstance(per, dict):
        t = per.get("tone", {})
        if isinstance(t, dict):
            tone = "ãƒ»".join([f"{k}:{v}" for k,v in t.items()])[:120]
        elif isinstance(t, list):
            tone = "ãƒ»".join(map(str, t))[:120]

    forbidden_local = []
    if isinstance(nor, dict):
        fw = nor.get("forbidden_words", [])
        if isinstance(fw, list):
            forbidden_local = [str(x) for x in fw]

    knowledge = []
    if trend:
        knowledge.append(f"å¸‚å ´èªå½™/ãƒˆãƒ¬ãƒ³ãƒ‰: {trend}")
    if clusters_str:
        knowledge.append(f"èªå½™ã‚¯ãƒ©ã‚¹ã‚¿: {clusters_str}")
    if concepts_str:
        knowledge.append(f"æ¦‚å¿µ/æ§‹é€ : {concepts_str}")
    if tone:
        knowledge.append(f"æ¨å¥¨ãƒˆãƒ¼ãƒ³: {tone}")
    knowledge_txt = "\n".join(knowledge)

    return knowledge_txt, set(forbidden_local)

def sanitize(text: str) -> str:
    """ç¦å‰‡èªã®é™¤å»ï¼†ä½™è¨ˆãªæ‹¬å¼§/é€£ç¶šç©ºç™½ã®æ•´ç†"""
    if not text:
        return ""
    t = text
    # ç¦å‰‡èªå‰Šé™¤
    for ng in FORBIDDEN:
        t = t.replace(ng, "")
    # é€£ç¶šç©ºç™½ã®æ•´å½¢
    t = re.sub(r"\s+", " ", t).strip()
    # ä¸è¦ãªæœ«å°¾è¨˜å·
    t = re.sub(r"[\/\|\-ãƒ»\s]+$", "", t)
    return t

def limit_length_ja(s: str, max_chars: int) -> str:
    """å…¨è§’åŸºæº–ã®æ–‡å­—æ•°ä¸Šé™ã§ä¸¸ã‚ï¼ˆã–ã£ãã‚Šï¼‰"""
    s = s.strip()
    return s if len(s) <= max_chars else s[:max_chars].rstrip()

def alt_shorten_to_range(s: str, min_len=80, max_len=110):
    """
    ALTã®æœ€çµ‚æ•´å½¢ï¼š
    - ã¾ãš100ã€œ130ã§ç”Ÿæˆ â†’ å¥ç‚¹ã§æ–‡ã‚’è½ã¨ã—ã¦80ã€œ110ã«åã‚ã‚‹
    - å¥èª­ç‚¹ã§ã®æ„å‘³å˜ä½å„ªå…ˆã€ç„¡ã‘ã‚Œã°å®‰å…¨ã«ä¸¸ã‚
    """
    s = sanitize(s)
    if not s:
        return s
    # å¥ç‚¹ã§è½ã¨ã—è¾¼ã¿
    if len(s) > max_len:
        # æ–‡æœ«ãƒãƒƒãƒï¼ˆ80ã€œ110ã«åã¾ã‚‹æœ€å¾Œã®å¥ç‚¹ï¼‰
        cut_idx = None
        for m in re.finditer(r"[ã€‚ï¼ï¼ï¼Ÿ!?]", s):
            pos = m.end()
            if min_len <= pos <= max_len:
                cut_idx = pos
        if cut_idx:
            s = s[:cut_idx]
        else:
            # æ¬¡å–„ç­–ï¼šmax_lenã§ä¸¸ã‚ã‚‹
            s = s[:max_len]
    # ä¸‹é™ã‚’æº€ãŸã•ãªã„å ´åˆã¯ã€ãã®ã¾ã¾ï¼ˆçŸ­ã™ãã‚‹ã‚±ãƒ¼ã‚¹ã¯å†ç”Ÿæˆå¯¾è±¡ã«å›ã™ã®ãŒæœ¬æ¥ï¼‰
    return s.strip(" ã€€ã€ï¼Œ.")

def find_latest_file(pattern: str):
    files = sorted(glob.glob(str(AI_OUT / pattern)))
    return Path(files[-1]) if files else None

def read_latest_rakuten_or_empty():
    """æ—¢å­˜ã®æ¥½å¤©CSVã‚’èª­ã¿è¾¼ã¿ï¼ˆæœ€ã‚‚æ–°ã—ã„ã‚‚ã®ï¼‰ã€‚ç„¡ã‘ã‚Œã°ç©ºã‚’è¿”ã™ã€‚"""
    latest = find_latest_file("rakuten_copy_*.csv")
    if not latest:
        return {}
    out = {}
    with open(latest, "r", encoding="utf-8", newline="") as f:
        reader = csv.DictReader(f)
        # æœŸå¾…ã‚«ãƒ©ãƒ : å•†å“å,Rakuten_Copy
        # å£Šã‚Œã¦ã„ã¦ã‚‚è½ã¡ãšã«æ‹¾ãˆã‚‹ç¯„å›²ã§æ‹¾ã†
        for row in reader:
            nm = (row.get("å•†å“å") or "").strip()
            rk = (row.get("Rakuten_Copy") or row.get("æ¥½å¤©ã‚³ãƒ”ãƒ¼") or "").strip()
            if nm:
                out[nm] = rk
    return out

# ---------- OpenAIå‘¼ã³å‡ºã— ----------

def build_client():
    # OPENAI_API_KEY ã¯ç’°å¢ƒå¤‰æ•°ã§
    return OpenAI()

def call_openai_json(client, model, messages, max_completion_tokens=800):
    """
    JSONãƒ¢ãƒ¼ãƒ‰ã§å‘¼ã³å‡ºã—ã€content[0].text ã‚’JSONã¨ã—ã¦è¿”ã™ã€‚
    å†è©¦è¡Œã¯ä¸Šä½ã§è¡Œã†ã€‚
    """
    try:
        res = client.chat.completions.create(
            model=model,
            messages=messages,
            response_format={"type": "json_object"},
            max_completion_tokens=max_completion_tokens,
        )
        txt = (res.choices[0].message.content or "").strip()
        if not txt:
            raise ValueError("Empty content")
        return json.loads(txt)
    except OpenAIError as e:
        raise
    except Exception as e:
        raise

def prompt_messages(product_name: str, knowledge_text: str):
    """
    Yahooï¼ˆ25ã€œ30æ–‡å­—ï¼‰ï¼† ALTï¼ˆ100ã€œ130æ–‡å­—ï¼‰ã‚’åŒæ™‚JSONã§å‡ºã•ã›ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€‚
    ç”»åƒæå†™ãƒ»ã€ç«¶åˆå„ªä½æ€§ã€ãªã©ã¯ç¦æ­¢ã€‚
    """
    sys = (
        "ã‚ãªãŸã¯æ—¥æœ¬èªã®ECã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚å‡ºåŠ›ã¯å¿…ãšJSONã®ã¿ã€‚\n"
        "ç¦å‰‡ï¼šç”»åƒãƒ»å†™çœŸãªã©ã®æå†™èªã€ç«¶åˆå„ªä½æ€§ã¨ã„ã†èªã€æ©Ÿå¯†ãƒ»æ³¨æ„å–šèµ·è¡¨ç¾ã¯ä½¿ã‚ãªã„ã€‚\n"
        "æ§‹æˆãƒ’ãƒ³ãƒˆï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬åŒ–ã¯ã—ãªã„ï¼‰ï¼šå•†å“ã‚¹ãƒšãƒƒã‚¯â†’ã‚³ã‚¢ã‚³ãƒ³ãƒ”ã‚¿ãƒ³ã‚¹â†’ã©ã‚“ãªäººâ†’ã©ã‚“ãªã‚·ãƒ¼ãƒ³â†’ä½¿ã†ã¨â†’èª²é¡Œè§£æ±º/ä¾¿åˆ©ã•ã€‚\n"
        "Yahooã‚³ãƒ”ãƒ¼ã¯è‡ªç„¶ãªå®Œçµæ–‡ã‚’**25ã€œ30å…¨è§’**ã«å³å®ˆã€‚\n"
        "ALTã¯ç”»åƒæå†™ç¦æ­¢ã§**100ã€œ130å…¨è§’**ã€æ„å‘³ãŒå®Œçµã—ãŸæ–‡ã§ã€‚"
    )
    usr = (
        f"å•†å“å: {product_name}\n\n"
        f"ãƒ­ãƒ¼ã‚«ãƒ«çŸ¥è¦‹:\n{knowledge_text}\n\n"
        "JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:\n"
        "{\n"
        '  "yahoo_copy": "25ã€œ30å…¨è§’ã®å®Œçµã‚³ãƒ”ãƒ¼",\n'
        '  "alt": "100ã€œ130å…¨è§’ã®ALTï¼ˆç”»åƒæå†™ç¦æ­¢ï¼‰"\n'
        "}"
    )
    return [
        {"role":"system","content":sys},
        {"role":"user","content":usr}
    ]

def generate_for_product(client, model, product_name, knowledge_text, retries=3):
    """
    å˜ä¸€å•†å“ã«ã¤ã„ã¦ Yahoo & ALT ã‚’ç”Ÿæˆã€‚
    å†è©¦è¡Œã§ç©ºå¿œç­”ã‚„APIã‚¨ãƒ©ãƒ¼ã‚’ç·©å’Œã€‚
    """
    last_err = None
    for attempt in range(1, retries+1):
        try:
            js = call_openai_json(client, model, prompt_messages(product_name, knowledge_text))
            yc = sanitize(js.get("yahoo_copy","").strip())
            alt = sanitize(js.get("alt","").strip())

            # Yahooé•·ã•ï¼ˆ25ã€œ30ï¼‰ã‚’å³æ ¼åŒ–
            if not (25 <= len(yc) <= 30):
                # ã‚ºãƒ¬ãŸã‚‰å®‰å…¨ã«ä¸¸ã‚ or å†è©¦è¡Œ
                if len(yc) > 30:
                    yc = limit_length_ja(yc, 30)
                elif len(yc) < 25:
                    # å†è©¦è¡Œ
                    raise ValueError("Yahoo copy too short")

            # ALTã¯ 80ã€œ110 ã¸ãƒ­ãƒ¼ã‚«ãƒ«æ•´å½¢ï¼ˆã¾ãšã¯100ã€œ130ç”Ÿæˆæƒ³å®šï¼‰
            alt_final = alt_shorten_to_range(alt, 80, 110)
            if len(alt_final) < 80:
                # å†è©¦è¡Œ
                raise ValueError("ALT too short after shorten")

            return yc, alt_final
        except Exception as e:
            last_err = e
            time.sleep(1.0)
    # ã™ã¹ã¦å¤±æ•—æ™‚
    raise last_err if last_err else RuntimeError("unknown generation error")

# ---------- ãƒ¡ã‚¤ãƒ³ ----------

def main():
    print("ğŸŒ¸ writer_splitter_perfect_v3.2 å®Ÿè¡Œé–‹å§‹ï¼ˆYahoo/ALTå†ç”Ÿæˆï¼‹çŸ¥è¦‹è¦ç´„ï¼‹ç¦å‰‡/é•·ã•æ•´å½¢ï¼‰")

    # å…¥åŠ›èª­ã¿è¾¼ã¿
    rows = load_csv_shiftjis(INPUT_CSV)
    header, col_idx = find_header_and_column(rows, "å•†å“å")
    names = extract_product_names(rows, col_idx)
    print(f"âœ… å•†å“åæŠ½å‡º: {len(names)}ä»¶ï¼ˆé‡è¤‡é™¤å»æ¸ˆï¼‰")

    # çŸ¥è¦‹è¦ç´„
    knowledge_text, forbidden_local = summarize_knowledge()
    # ãƒ­ãƒ¼ã‚«ãƒ«ç¦å‰‡ã‚‚åŠ ãˆã‚‹
    if forbidden_local:
        for w in forbidden_local:
            if w: FORBIDDEN.add(str(w))

    # æ¥½å¤©ã‚³ãƒ”ãƒ¼ã¯æ—¢å­˜ã‚’æ¸©å­˜ï¼ˆç„¡ã‘ã‚Œã°ç©ºï¼‰
    rakuten_map = read_latest_rakuten_or_empty()

    # é€²æ—ãƒãƒ¼
    it = range(len(names))
    if tqdm:
        it = tqdm(it, desc="ğŸ§  å•†å“åˆ¥AIç”Ÿæˆä¸­", ncols=80)

    client = build_client()

    # å‡ºåŠ›ç”¨
    yahoo_rows = []
    alt_rows = []
    jsonl_path = AI_OUT / f"split_full_{nowstamp()}.jsonl"

    with open(jsonl_path, "w", encoding="utf-8") as fj:
        for i in it:
            nm = names[i]
            try:
                yc, alt = generate_for_product(client, MODEL, nm, knowledge_text, retries=3)
            except Exception as e:
                # å¤±æ•—æ™‚ã¯ç©ºæ¬„ã§è½ã¨ã•ãšç¶šè¡Œï¼ˆå¾Œå·¥ç¨‹ã§å€‹åˆ¥å†ç”Ÿæˆå¯èƒ½ï¼‰
                yc, alt = "", ""
                if tqdm is None:
                    print(f"âš ï¸ ç”Ÿæˆå¤±æ•—: {nm[:20]}... => {e}")

            # JSONLè¨˜éŒ²
            rec = {
                "product_name": nm,
                "yahoo_copy": yc,
                "alt": alt,
                "ts": nowstamp()
            }
            fj.write(json.dumps(rec, ensure_ascii=False) + "\n")

            yahoo_rows.append({"å•†å“å": nm, "Yahoo_Copy": yc})
            # ALTã¯æ¨ªæŒã¡20æœ¬ â†’ ä»Šå›ã¯ ALTå…±é€š1æ ã«æ ¼ç´ï¼ˆå…±é€šALT20ã‚’æƒ³å®šã™ã‚‹å ´åˆã¯20åˆ—ã«è¤‡å†™ã‚‚å¯ï¼‰
            # ä»•æ§˜é€šã‚Šã€Œå…±é€šALT20ã€ã‚’1åˆ—ã§ã¯ãªã â€œALT1..ALT20â€ ã«å±•é–‹ã™ã‚‹å ´åˆã¯ã“ã“ã§è¤‡å†™ã™ã‚‹ã€‚
            # ã“ã“ã§ã¯ 1å•†å“ã«ã¤ã1è¡Œã§ ALT1..ALT20 ã‚’åŒæ–‡ã§åŸ‹ã‚ã‚‹ï¼ˆè¦ä»¶ï¼šå…±é€šALT20ï¼‰
            alt_row = {"å•†å“å": nm}
            for k in range(1, 21):
                alt_row[f"ALT{k}"] = alt
            alt_rows.append(alt_row)

    # æ—¢å­˜æ¥½å¤©CSVã®æœ€æ–°ã‚’ã‚³ãƒ”ãƒ¼ or æ–°è¦ä½œæˆ
    # â†’ å‡ºåŠ›ã¯æœ€æ–°ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§å¿…ãšä½œã‚‹ï¼ˆä¸­èº«ã¯æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°è»¢è¨˜ã€ç„¡ã‘ã‚Œã°ç©ºï¼‰
    rk_rows = []
    for nm in names:
        rk_rows.append({"å•†å“å": nm, "Rakuten_Copy": rakuten_map.get(nm, "")})

    ts = nowstamp()
    yahoo_csv = AI_OUT / f"yahoo_copy_{ts}.csv"
    alt_csv   = AI_OUT / f"alt_text_{ts}.csv"
    rak_csv   = AI_OUT / f"rakuten_copy_{ts}.csv"

    # æ›¸ãå‡ºã—
    with open(yahoo_csv, "w", encoding="utf-8", newline="") as f:
        w = csv.DictWriter(f, fieldnames=["å•†å“å", "Yahoo_Copy"])
        w.writeheader()
        w.writerows(yahoo_rows)

    with open(alt_csv, "w", encoding="utf-8", newline="") as f:
        fields = ["å•†å“å"] + [f"ALT{k}" for k in range(1,21)]
        w = csv.DictWriter(f, fieldnames=fields)
        w.writeheader()
        w.writerows(alt_rows)

    with open(rak_csv, "w", encoding="utf-8", newline="") as f:
        w = csv.DictWriter(f, fieldnames=["å•†å“å", "Rakuten_Copy"])
        w.writeheader()
        w.writerows(rk_rows)

    print("âœ… å‡ºåŠ›å®Œäº†:")
    print(f"   - æ¥½å¤©: {rak_csv}")
    print(f"   - Yahoo: {yahoo_csv}")
    print(f"   - ALT20: {alt_csv}")
    print(f"   - JSONL: {jsonl_path}")
    print("âœ… å…±é€šALT20ã¯ã€alt_text_*.csvã€ã«ALT1ã€œALT20ã¸æ¨ªæŒã¡ã§è¤‡å†™ã—ã¾ã™ã€‚")

if __name__ == "__main__":
    main()
import atlas_autosave_core
