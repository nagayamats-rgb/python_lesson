# -*- coding: utf-8 -*-
"""
writer_splitter_perfect_v3.2_fix2.py
- å…¨ä»¶AIï¼‹çŸ¥è¦‹è¦ç´„ï¼‹3åˆ†å‰²ï¼ˆæ¥½å¤©ã¯æ¸©å­˜ã€Yahoo/ALTã®ã¿å†ç”Ÿæˆï¼‰
- å…¥åŠ›: ./input.csvï¼ˆShift-JIS, ãƒ˜ãƒƒãƒ€è¡Œã‚ã‚Š, ã€Œå•†å“åã€åˆ—ã‚’ç¸¦èµ°æŸ»ï¼‰
- å‡ºåŠ›: ./output/ai_writer/{rakuten_copy_*.csv, yahoo_copy_*.csv, alt_text_*.csv, split_full_*.jsonl}
"""

import os, re, csv, glob, json, time, datetime
from pathlib import Path
from openai import OpenAI, OpenAIError

try:
    from tqdm import tqdm
except ImportError:
    tqdm = None

# ---------- åŸºæœ¬è¨­å®š ----------
BASE = Path(".")
AI_OUT = BASE / "output" / "ai_writer"
AI_OUT.mkdir(parents=True, exist_ok=True)
SEM_OUT = BASE / "output" / "semantics"
INPUT_CSV = BASE / "input.csv"

MODEL = os.environ.get("OPENAI_MODEL", "gpt-4o-mini")

FORBIDDEN = set([
    "ç”»åƒ", "å†™çœŸ", "ãƒ•ã‚©ãƒˆ", "ã‚¤ãƒ¡ãƒ¼ã‚¸", "è¦‹ãŸç›®", "æ˜ ã£ã¦ã„ã‚‹", "æ˜ åƒ",
    "ç«¶åˆå„ªä½æ€§", "ä»–ç¤¾å„ªä½æ€§", "ç¤¾å¤–ç§˜", "æ³¨æ„å–šèµ·", "â€»ç”»åƒã¯ã‚¤ãƒ¡ãƒ¼ã‚¸ã§ã™",
    "ã‚¯ãƒªãƒƒã‚¯", "ä¸Šã®å†™çœŸ", "ä¸‹ã®ç”»åƒ",
])

SENT_END = "ã€‚ï¼ï¼!ï¼Ÿ?ï¼›;"

# ---------- å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ----------
def nowstamp():
    return datetime.datetime.now().strftime("%Y%m%d_%H%M")

def load_csv_shiftjis(path: Path):
    if not path.exists():
        raise FileNotFoundError(f"å…¥åŠ›ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path}")
    with open(path, "r", encoding="cp932", errors="ignore", newline="") as f:
        reader = csv.reader(f)
        rows = [r for r in reader]
    return rows

def find_header_and_column(rows, header_name="å•†å“å"):
    header = rows[0]
    if header_name not in header:
        raise KeyError(f"ãƒ˜ãƒƒãƒ€ã«ã€Œ{header_name}ã€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    return header, header.index(header_name)

def extract_product_names(rows, col_idx):
    names = []
    for r in rows[1:]:
        if col_idx < len(r):
            nm = (r[col_idx] or "").strip()
            if nm:
                names.append(nm)
    seen, uniq = set(), []
    for n in names:
        if n not in seen:
            seen.add(n)
            uniq.append(n)
    return uniq

def sanitize(text: str) -> str:
    if not text:
        return ""
    t = text
    for ng in FORBIDDEN:
        t = t.replace(ng, "")
    t = re.sub(r"\s+", " ", t).strip()
    t = re.sub(r"[\/\|\-ãƒ»\s]+$", "", t)
    return t

def limit_length_ja(s: str, max_chars: int) -> str:
    s = s.strip()
    return s if len(s) <= max_chars else s[:max_chars].rstrip()

def alt_shorten_to_range(s: str, min_len=80, max_len=110):
    s = sanitize(s)
    if not s:
        return s
    if len(s) > max_len:
        cut_idx = None
        for m in re.finditer(r"[ã€‚ï¼ï¼ï¼Ÿ!?]", s):
            pos = m.end()
            if min_len <= pos <= max_len:
                cut_idx = pos
        if cut_idx:
            s = s[:cut_idx]
        else:
            s = s[:max_len]
    return s.strip(" ã€€ã€ï¼Œ.")

def find_latest_file(pattern: str):
    files = sorted(glob.glob(str(AI_OUT / pattern)))
    return Path(files[-1]) if files else None

def read_latest_rakuten_or_empty():
    latest = find_latest_file("rakuten_copy_*.csv")
    if not latest:
        return {}
    out = {}
    with open(latest, "r", encoding="utf-8", newline="") as f:
        reader = csv.DictReader(f)
        for row in reader:
            nm = (row.get("å•†å“å") or "").strip()
            rk = (row.get("Rakuten_Copy") or row.get("æ¥½å¤©ã‚³ãƒ”ãƒ¼") or "").strip()
            if nm:
                out[nm] = rk
    return out

# ---------- çŸ¥è¦‹è¦ç´„ ----------
def summarize_knowledge():
    """ãƒ­ãƒ¼ã‚«ãƒ«çŸ¥è¦‹JSONç¾¤ã‚’èª­ã¿è¾¼ã¿ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ³¨å…¥ç”¨ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦ç´„ç”Ÿæˆ"""
    try:
        def latest(pattern):
            files = sorted(glob.glob(str(SEM_OUT / pattern)))
            return Path(files[-1]) if files else None

        def safe_load_json(p):
            if not p or not p.exists():
                return {}
            try:
                with open(p, "r", encoding="utf-8") as f:
                    return json.load(f)
            except Exception:
                return {}

        def cap_list(xs, key=None, cap=10):
            out = []
            if isinstance(xs, dict):
                xs = list(xs.values())
            if not isinstance(xs, list):
                xs = [xs]
            for x in xs[:cap]:
                if isinstance(x, dict) and key:
                    v = x.get(key, "")
                else:
                    v = str(x)
                v = (v or "").strip()
                if v:
                    out.append(v)
            return "ã€".join(out)

        lex = safe_load_json(latest("lexical_clusters_*.json"))
        sem = safe_load_json(latest("structured_semantics_*.json"))
        per = safe_load_json(latest("styled_persona_*.json"))
        mar = safe_load_json(latest("market_vocab_*.json"))
        nor = safe_load_json(latest("normalized_*.json"))

        trend = cap_list(mar, key="vocabulary", cap=12)

        clusters = []
        if isinstance(lex, dict) and "clusters" in lex:
            clusters = [c.get("name", "") for c in lex["clusters"] if isinstance(c, dict)]
        elif isinstance(lex, list):
            clusters = [x.get("name","") if isinstance(x, dict) else str(x) for x in lex]
        clusters_str = "ã€".join([x for x in clusters[:12] if x])

        concepts = []
        if isinstance(sem, dict):
            for k,v in sem.items():
                if isinstance(v, (str, int, float)):
                    concepts.append(f"{k}:{v}")
                elif isinstance(v, list):
                    concepts.append(f"{k}:{'|'.join(map(str,v[:3]))}")
                elif isinstance(v, dict):
                    concepts.append(f"{k}:{'|'.join(list(v.keys())[:3])}")
        concepts_str = "ã€".join(concepts[:12])

        tone = ""
        if isinstance(per, dict):
            t = per.get("tone", {})
            if isinstance(t, dict):
                tone = "ãƒ»".join([f"{k}:{v}" for k,v in t.items()])[:120]
            elif isinstance(t, list):
                tone = "ãƒ»".join(map(str, t))[:120]

        forbidden_local = []
        if isinstance(nor, dict):
            fw = nor.get("forbidden_words", [])
            if isinstance(fw, list):
                forbidden_local = [str(x) for x in fw]

        knowledge = []
        if trend:
            knowledge.append(f"å¸‚å ´èªå½™/ãƒˆãƒ¬ãƒ³ãƒ‰: {trend}")
        if clusters_str:
            knowledge.append(f"èªå½™ã‚¯ãƒ©ã‚¹ã‚¿: {clusters_str}")
        if concepts_str:
            knowledge.append(f"æ¦‚å¿µ/æ§‹é€ : {concepts_str}")
        if tone:
            knowledge.append(f"æ¨å¥¨ãƒˆãƒ¼ãƒ³: {tone}")

        knowledge_txt = "\n".join(knowledge)
        return knowledge_txt, set(forbidden_local)

    except Exception as e:
        print(f"âš ï¸ summarize_knowledge() å†…ã§ã‚¨ãƒ©ãƒ¼: {e}")
        return "", set()

# ---------- OpenAIå‘¼ã³å‡ºã— ----------
def build_client():
    return OpenAI()

def call_openai_json(client, model, messages, max_completion_tokens=800):
    res = client.chat.completions.create(
        model=model,
        messages=messages,
        response_format={"type": "json_object"},
        max_completion_tokens=max_completion_tokens,
    )
    txt = (res.choices[0].message.content or "").strip()
    if not txt:
        raise ValueError("Empty content")
    return json.loads(txt)

def prompt_messages(product_name: str, knowledge_text: str):
    sys = (
        "ã‚ãªãŸã¯æ—¥æœ¬èªã®ECã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚å‡ºåŠ›ã¯å¿…ãšJSONã®ã¿ã€‚\n"
        "ç¦å‰‡ï¼šç”»åƒãƒ»å†™çœŸãªã©ã®æå†™èªã€ç«¶åˆå„ªä½æ€§ã¨ã„ã†èªã€æ©Ÿå¯†ãƒ»æ³¨æ„å–šèµ·è¡¨ç¾ã¯ä½¿ã‚ãªã„ã€‚\n"
        "æ§‹æˆãƒ’ãƒ³ãƒˆï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬åŒ–ã¯ã—ãªã„ï¼‰ï¼šå•†å“ã‚¹ãƒšãƒƒã‚¯â†’ã‚³ã‚¢ã‚³ãƒ³ãƒ”ã‚¿ãƒ³ã‚¹â†’ã©ã‚“ãªäººâ†’ã©ã‚“ãªã‚·ãƒ¼ãƒ³â†’ä½¿ã†ã¨â†’èª²é¡Œè§£æ±º/ä¾¿åˆ©ã•ã€‚\n"
        "Yahooã‚³ãƒ”ãƒ¼ã¯è‡ªç„¶ãªå®Œçµæ–‡ã‚’25ã€œ30å…¨è§’ã«å³å®ˆã€‚\n"
        "ALTã¯ç”»åƒæå†™ç¦æ­¢ã§100ã€œ130å…¨è§’ã€æ„å‘³ãŒå®Œçµã—ãŸæ–‡ã§ã€‚"
    )
    usr = (
        f"å•†å“å: {product_name}\n\n"
        f"ãƒ­ãƒ¼ã‚«ãƒ«çŸ¥è¦‹:\n{knowledge_text}\n\n"
        "JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:\n"
        "{\n"
        '  \"yahoo_copy\": \"25ã€œ30å…¨è§’ã®å®Œçµã‚³ãƒ”ãƒ¼\",\n'
        '  \"alt\": \"100ã€œ130å…¨è§’ã®ALTï¼ˆç”»åƒæå†™ç¦æ­¢ï¼‰\"\n'
        "}"
    )
    return [
        {"role":"system","content":sys},
        {"role":"user","content":usr}
    ]

def generate_for_product(client, model, product_name, knowledge_text, retries=3):
    last_err = None
    for attempt in range(1, retries+1):
        try:
            js = call_openai_json(client, model, prompt_messages(product_name, knowledge_text))
            yc = sanitize(js.get("yahoo_copy","").strip())
            alt = sanitize(js.get("alt","").strip())
            if not (25 <= len(yc) <= 30):
                if len(yc) > 30:
                    yc = limit_length_ja(yc, 30)
                elif len(yc) < 25:
                    raise ValueError("Yahoo copy too short")
            alt_final = alt_shorten_to_range(alt, 80, 110)
            if len(alt_final) < 80:
                raise ValueError("ALT too short after shorten")
            return yc, alt_final
        except Exception as e:
            last_err = e
            time.sleep(1.0)
    raise last_err if last_err else RuntimeError("unknown generation error")

# ---------- ãƒ¡ã‚¤ãƒ³ ----------
def main():
    print("ğŸŒ¸ writer_splitter_perfect_v3.2_fix2 å®Ÿè¡Œé–‹å§‹ï¼ˆYahoo/ALTå†ç”Ÿæˆï¼‹çŸ¥è¦‹è¦ç´„ï¼‹ç¦å‰‡/é•·ã•æ•´å½¢ï¼‰")
    rows = load_csv_shiftjis(INPUT_CSV)
    header, col_idx = find_header_and_column(rows, "å•†å“å")
    names = extract_product_names(rows, col_idx)
    print(f"âœ… å•†å“åæŠ½å‡º: {len(names)}ä»¶ï¼ˆé‡è¤‡é™¤å»æ¸ˆï¼‰")

    knowledge_text, forbidden_local = summarize_knowledge()
    if forbidden_local:
        FORBIDDEN.update(forbidden_local)

    rakuten_map = read_latest_rakuten_or_empty()

    it = tqdm(range(len(names)), desc="ğŸ§  å•†å“åˆ¥AIç”Ÿæˆä¸­", ncols=80) if tqdm else range(len(names))
    client = build_client()

    yahoo_rows, alt_rows = [], []
    jsonl_path = AI_OUT / f"split_full_{nowstamp()}.jsonl"

    with open(jsonl_path, "w", encoding="utf-8") as fj:
        for i in it:
            nm = names[i]
            try:
                yc, alt = generate_for_product(client, MODEL, nm, knowledge_text)
            except Exception as e:
                yc, alt = "", ""
                if tqdm is None:
                    print(f"âš ï¸ ç”Ÿæˆå¤±æ•—: {nm[:20]}... => {e}")
            rec = {"product_name": nm, "yahoo_copy": yc, "alt": alt, "ts": nowstamp()}
            fj.write(json.dumps(rec, ensure_ascii=False) + "\n")
            yahoo_rows.append({"å•†å“å": nm, "Yahoo_Copy": yc})
            alt_row = {"å•†å“å": nm}
            for k in range(1, 21):
                alt_row[f"ALT{k}"] = alt
            alt_rows.append(alt_row)

    rk_rows = [{"å•†å“å": nm, "Rakuten_Copy": rakuten_map.get(nm, "")} for nm in names]

    ts = nowstamp()
    yahoo_csv = AI_OUT / f"yahoo_copy_{ts}.csv"
    alt_csv   = AI_OUT / f"alt_text_{ts}.csv"
    rak_csv   = AI_OUT / f"rakuten_copy_{ts}.csv"

    with open(yahoo_csv, "w", encoding="utf-8", newline="") as f:
        w = csv.DictWriter(f, fieldnames=["å•†å“å", "Yahoo_Copy"])
        w.writeheader()
        w.writerows(yahoo_rows)

    with open(alt_csv, "w", encoding="utf-8", newline="") as f:
        fields = ["å•†å“å"] + [f"ALT{k}" for k in range(1,21)]
        w = csv.DictWriter(f, fieldnames=fields)
        w.writeheader()
        w.writerows(alt_rows)

    with open(rak_csv, "w", encoding="utf-8", newline="") as f:
        w = csv.DictWriter(f, fieldnames=["å•†å“å", "Rakuten_Copy"])
        w.writeheader()
        w.writerows(rk_rows)

    print("âœ… å‡ºåŠ›å®Œäº†:")
    print(f"   - æ¥½å¤©: {rak_csv}")
    print(f"   - Yahoo: {yahoo_csv}")
    print(f"   - ALT20: {alt_csv}")
    print(f"   - JSONL: {jsonl_path}")
    print("âœ… å…±é€šALT20ã¯ã€alt_text_*.csvã€ã«ALT1ã€œALT20ã¸æ¨ªæŒã¡ã§è¤‡å†™ã—ã¾ã™ã€‚")

if __name__ == "__main__":
    main()
import atlas_autosave_core
