# ============================================
# ğŸŒ¸ writer_splitter_perfect_v3_1.py
# å®‰å®šç‰ˆï¼šç©ºå¿œç­”ãƒ»å£Šã‚ŒãŸJSONãƒ»å†è©¦è¡Œä¿è­·ä»˜ã
# ============================================

import json, csv, os, re, time
from datetime import datetime
from tqdm import tqdm
from openai import OpenAI

MODEL = "gpt-5"
MAX_TOKENS = 800
OUTPUT_DIR = "./output/ai_writer"
os.makedirs(OUTPUT_DIR, exist_ok=True)
INPUT_CSV = "./input.csv"
RETRY_WAIT = 5

SEED_JSONS = {
    "persona": "./output/semantics/styled_persona_20251031_0031.json",
    "lexical": "./output/semantics/lexical_clusters_20251030_223013.json",
    "market": "./output/semantics/market_vocab_20251030_201906.json",
    "semantic": "./output/semantics/structured_semantics_20251030_224846.json",
    "template": "./output/semantics/template_composer.json",
    "norm": "./output/semantics/normalized_20251031_0039.json"
}

client = OpenAI()

def load_jsons():
    cfg = {}
    for k, path in SEED_JSONS.items():
        if os.path.exists(path):
            with open(path, "r", encoding="utf-8") as f:
                try:
                    cfg[k] = json.load(f)
                except:
                    cfg[k] = {}
        else:
            cfg[k] = {}
    return cfg

def summarize_knowledge(cfg):
    persona = ", ".join([v.get("tone", "") for v in cfg["persona"] if isinstance(v, dict)])
    lexical = ", ".join([v.get("keyword", "") for v in cfg["lexical"] if isinstance(v, dict)])
    market = ", ".join([v.get("vocabulary", "") for v in cfg["market"] if isinstance(v, dict)])
    semantic = ", ".join([v.get("concept", "") for v in cfg["semantic"] if isinstance(v, dict)])
    template = ", ".join([v.get("pattern", "") for v in cfg["template"] if isinstance(v, dict)])
    norm = ", ".join([v.get("forbidden_words", "") for v in cfg["norm"] if isinstance(v, dict)])
    return f"""
ã€ãƒ­ãƒ¼ã‚«ãƒ«çŸ¥è¦‹è¦ç´„ã€‘
- ãƒˆãƒ¼ãƒ³: {persona}
- å¸‚å ´èªå½™: {market}
- æ¦‚å¿µç¾¤: {semantic}
- æ§‹æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³: {template}
- ç¦æ­¢èª: {norm}
- ä»£è¡¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {lexical}
"""

def call_openai_json(messages, retries=3):
    for attempt in range(retries):
        try:
            res = client.chat.completions.create(
                model=MODEL,
                max_completion_tokens=MAX_TOKENS,
                messages=messages
            )
            content = res.choices[0].message.content
            if not content or not content.strip():
                print(f"âš ï¸ ç©ºå¿œç­”ã‚’æ¤œå‡ºï¼ˆ{attempt+1}/{retries}ï¼‰â†’å†è©¦è¡Œä¸­â€¦")
                time.sleep(RETRY_WAIT)
                # knowledgeãŒé•·ã™ãã‚‹å ´åˆã¯çŸ­ç¸®ã—ã¦å†é€
                for msg in messages:
                    if msg["role"] == "user" and len(msg["content"]) > 1500:
                        msg["content"] = msg["content"][:1000] + "\nï¼ˆè¦ç´„çŸ­ç¸®ç‰ˆï¼‰"
                continue

            try:
                data = json.loads(content)
            except json.JSONDecodeError:
                if "{" in content and "}" in content:
                    data = json.loads(content[:content.rfind("}")+1])
                else:
                    print(f"âš ï¸ JSONå¤‰æ›å¤±æ•—ï¼ˆ{attempt+1}/{retries}ï¼‰â†’å†è©¦è¡Œ")
                    time.sleep(RETRY_WAIT)
                    continue
            return data
        except Exception as e:
            print(f"âš ï¸ OpenAIã‚¨ãƒ©ãƒ¼: {e}ï¼ˆ{attempt+1}/{retries}ï¼‰")
            time.sleep(RETRY_WAIT)
    print("âŒ å¿œç­”å–å¾—å¤±æ•—ï¼ˆã™ã¹ã¦ã®å†è©¦è¡ŒãŒå¤±æ•—ï¼‰")
    return None

def ai_generate(product_name, knowledge):
    system_prompt = f"""
ã‚ãªãŸã¯ECå‘ã‘ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚
ä»¥ä¸‹ã®çŸ¥è¦‹ã¨å•†å“åã‚’ã‚‚ã¨ã«ã€æ¥½å¤©ã¨Yahooå‘ã‘ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼ã€ãã—ã¦SEOã«æœ€é©åŒ–ã•ã‚ŒãŸALT20ä»¶ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ(JSON):
{{
  "rakuten_copy": "å…¨è§’60ã€œ80æ–‡å­—ã€ä¸Šé™87æ–‡å­—",
  "yahoo_copy": "å…¨è§’25ã€œ30æ–‡å­—ã€ä¸Šé™30æ–‡å­—",
  "alt_texts": ["80ã€œ110æ–‡å­—Ã—20"]
}}

æ§‹æ–‡ã‚¬ã‚¤ãƒ‰:
- å•†å“ã‚¹ãƒšãƒƒã‚¯(spec)
- ã‚³ã‚¢ã‚³ãƒ³ãƒ”ã‚¿ãƒ³ã‚¹(competence)
- ã©ã‚“ãªäººãŒ(user)
- ã©ã‚“ãªã‚·ãƒ¼ãƒ³ã§(scene)
- ä½¿ã†ã¨ã©ã†ä¾¿åˆ©ãƒ»å›°ã‚Šã”ã¨è§£æ±º(benefit)
ã“ã®æ§‹æˆè¦ç´ ã§è‡ªç„¶ã«æ–‡ç« ã‚’ä½œæˆã—ãªã•ã„ã€‚
"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"å•†å“å: {product_name}\n{knowledge}"}
    ]

    data = call_openai_json(messages)
    if not data:
        return "", "", [""] * 20

    rakuten = data.get("rakuten_copy", "")[:87]
    yahoo = data.get("yahoo_copy", "")[:30]
    alts = data.get("alt_texts", [])
    if not isinstance(alts, list):
        alts = [""] * 20
    elif len(alts) < 20:
        alts += [""] * (20 - len(alts))
    return rakuten, yahoo, alts[:20]

def extract_names():
    names = []
    with open(INPUT_CSV, "r", encoding="shift_jis", errors="ignore") as f:
        reader = csv.DictReader(f)
        for row in reader:
            name = row.get("å•†å“å", "").strip()
            if name:
                names.append(name)
    names = list(dict.fromkeys(names))
    return names

def main():
    print("ğŸŒ¸ writer_splitter_perfect_v3.1 å®Ÿè¡Œé–‹å§‹ï¼ˆå®‰å®šãƒ¢ãƒ¼ãƒ‰ï¼‹å†è©¦è¡Œä¿è­·ï¼‰")
    cfg = load_jsons()
    knowledge = summarize_knowledge(cfg)
    names = extract_names()
    print(f"âœ… å•†å“åæŠ½å‡º: {len(names)}ä»¶ï¼ˆé‡è¤‡é™¤å»æ¸ˆï¼‰")

    results = []
    for nm in tqdm(names, desc="ğŸ§  å•†å“åˆ¥AIç”Ÿæˆä¸­"):
        rak, yah, alts = ai_generate(nm, knowledge)
        results.append({
            "product_name": nm,
            "rakuten_copy": rak,
            "yahoo_copy": yah,
            "alt_texts": alts
        })

    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    base = os.path.join(OUTPUT_DIR, f"split_full_{timestamp}")
    jsonl_path = f"{base}.jsonl"
    csv_path_r = f"{OUTPUT_DIR}/rakuten_copy_{timestamp}.csv"
    csv_path_y = f"{OUTPUT_DIR}/yahoo_copy_{timestamp}.csv"
    csv_path_alt = f"{OUTPUT_DIR}/alt_text_{timestamp}.csv"

    # JSONLå‡ºåŠ›
    with open(jsonl_path, "w", encoding="utf-8") as jf:
        for item in results:
            jf.write(json.dumps(item, ensure_ascii=False) + "\n")

    # æ¥½å¤©ãƒ»Yahoo
    with open(csv_path_r, "w", encoding="utf-8-sig", newline="") as rf, \
         open(csv_path_y, "w", encoding="utf-8-sig", newline="") as yf, \
         open(csv_path_alt, "w", encoding="utf-8-sig", newline="") as af:

        writer_r = csv.writer(rf)
        writer_y = csv.writer(yf)
        writer_a = csv.writer(af)

        writer_r.writerow(["å•†å“å", "æ¥½å¤©ã‚³ãƒ”ãƒ¼"])
        writer_y.writerow(["å•†å“å", "Yahooã‚³ãƒ”ãƒ¼"])
        writer_a.writerow(["å•†å“å"] + [f"ALT{i}" for i in range(1, 21)])

        for item in results:
            writer_r.writerow([item["product_name"], item["rakuten_copy"]])
            writer_y.writerow([item["product_name"], item["yahoo_copy"]])
            writer_a.writerow([item["product_name"]] + item["alt_texts"])

    print(f"âœ… å‡ºåŠ›å®Œäº†:\n   - æ¥½å¤©: {csv_path_r}\n   - Yahoo: {csv_path_y}\n   - ALT20: {csv_path_alt}\n   - JSONL: {jsonl_path}")
    print("âœ… å…±é€šALT20ã¯ã€alt_text_*.csvã€ã«å…¨å•†å“ã¶ã‚“ã‚’æ¨ªæŒã¡ã§æ›¸ãå‡ºã—ã¾ã™ã€‚")

if __name__ == "__main__":
    main()
import atlas_autosave_core
