# -*- coding: utf-8 -*-
"""
writer_splitter_perfect_v3_5_unified.py
- å…¨ä»¶AIç”Ÿæˆï¼ˆæ¥½å¤©/Yahoo/ALTï¼‰ï¼‹ãƒ­ãƒ¼ã‚«ãƒ«çŸ¥è¦‹è¦ç´„ï¼‹ç¦å‰‡/é•·ã•æ•´å½¢
-# å…¥åŠ›CSVã‚’æ¥½å¤©ï¼Yahooã§åˆ¥æŒ‡å®š
RAKUTEN_INPUT = "/Users/tsuyoshi/Desktop/python_lesson/rakuten.csv"
YAHOO_INPUT   = "/Users/tsuyoshi/Desktop/python_lesson/yahoo.csv"
PRODUCT_NAME_COL = "å•†å“å"
- å‡ºåŠ›:
    ./output/ai_writer/rakuten_copy_YYYYMMDD_HHMM.csv
    ./output/ai_writer/yahoo_copy_YYYYMMDD_HHMM.csv
    ./output/ai_writer/alt_text_YYYYMMDD_HHMM.csv
    ./output/ai_writer/split_full_YYYYMMDD_HHMM.jsonl
"""

import os, sys, csv, json, time, re, unicodedata
from datetime import datetime
from collections import OrderedDict
from typing import List, Dict, Tuple, Any, Optional

import pandas as pd
from dotenv import load_dotenv
from tqdm import tqdm

# ====== OpenAI client (official SDK 1.x) ======
from openai import OpenAI
from openai import OpenAIError

# -------------------------------
# è¨­å®š
# -------------------------------
OUTPUT_DIR = "./output/ai_writer"
INPUT_CSV = "./input.csv"  # Shift-JIS, ãƒ˜ãƒƒãƒ€ã‚ã‚Š, ã€Œå•†å“åã€åˆ—
PRODUCT_NAME_COL = "å•†å“å"

# ç”»åƒæå†™ã‚’é¿ã‘ã‚‹ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æœ›ï¼‰
IMAGE_WORDS = [
    "ç”»åƒ", "å†™çœŸ", "è¦‹ãŸç›®", "æ˜ ãˆã‚‹", "ç”»é¢ä¸Š", "å†™çœŸã®ã‚ˆã†", "ã‚¤ãƒ¡ãƒ¼ã‚¸å›³", "ã‚µãƒ ãƒã‚¤ãƒ«", "ç”»åƒèª¬æ˜", "è¦‹ãŸæ„Ÿã˜",
    "è‰²åˆã„ã®å†™çœŸ", "å†™çœŸã¯ã‚¤ãƒ¡ãƒ¼ã‚¸", "ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«"
]

# çŸ¥è¦‹JSONã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¢ç´¢ãƒ‘ã‚¹
DEFAULT_PATHS = {
    "lexical": "./output/semantics/lexical_clusters_20251030_223013.json",
    "market": "./output/semantics/market_vocab_20251030_201906.json",
    "semantic": "./output/semantics/structured_semantics_20251030_224846.json",
    "persona": "./output/semantics/styled_persona_20251031_0031.json",
    "normalized": "./output/semantics/normalized_20251031_0039.json",
    "template": "./output/semantics/template_composer.json",
}

# æ–‡å­—æ•°ãƒ«ãƒ¼ãƒ«
RULES = {
    "rakuten_copy_min": 60, "rakuten_copy_max": 87,
    "rakuten_sp_min": 100, "rakuten_sp_max": 300,
    "yahoo_headline_min": 25, "yahoo_headline_max": 30,
    "yahoo_exp_min": 200, "yahoo_exp_max": 600,
    "yahoo_meta_min": 60, "yahoo_meta_max": 80,
    "alt_min": 80, "alt_max": 110,
    # ALTã¯ç”Ÿæˆæ™‚ã®é•·ã•ï¼ˆãƒ­ãƒ³ã‚°ï¼‰â†’ãƒˆãƒªãƒ 
    "alt_gen_min": 120, "alt_gen_max": 150,
}

RETRY = 3
RETRY_WAIT = 6  # seconds

# -------------------------------
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# -------------------------------

def ensure_output_dirs():
    os.makedirs(OUTPUT_DIR, exist_ok=True)

def now_tag():
    return datetime.now().strftime("%Y%m%d_%H%M")

def zenkaku_len(s: str) -> int:
    """å…¨è§’æ›ç®—æ–‡å­—æ•°ï¼ˆç°¡æ˜“ï¼‰ï¼šåŠè§’=1, å…¨è§’=2 ã‚’åˆç®—ã— 2ã§å‰²ã£ã¦å››æ¨äº”å…¥"""
    l = 0
    for ch in s:
        if unicodedata.east_asian_width(ch) in ("F", "W", "A"):
            l += 2
        else:
            l += 1
    # å…¨è§’åŸºæº–ã«ä¸¸ã‚è¾¼ã¿
    return int(round(l / 2.0))

def clamp_by_len(s: str, min_len: int, max_len: int) -> str:
    """å…¨è§’æ›ç®—ã§minã€œmaxã«åã‚ã‚‹ã€‚é•·ã„å ´åˆã¯æ–‡æœ«ã§è‡ªç„¶ã«åˆ‡ã‚‹ã€‚çŸ­ã„å ´åˆã¯ãã®ã¾ã¾ï¼ˆAIå´ã§å†è©¦è¡Œã™ã‚‹å‰æï¼‰ã€‚"""
    if not s:
        return s
    # ã„ã£ãŸã‚“ãƒˆãƒªãƒ 
    s = s.strip()
    # é•·éãã‚‹ã¨ãï¼šå¥ç‚¹ã‚„èª­ç‚¹ãƒ»ã€Œã€ã€‚!ã€ç­‰ã‚’ç›®å®‰ã«è‡ªç„¶åˆ‡ã‚Š
    def natural_cut(text: str, limit: int) -> str:
        if zenkaku_len(text) <= limit:
            return text
        # æ–‡æœ«å€™è£œã§åˆ‡ã‚‹
        marks = ["ã€‚", "ï¼", "?", "ï¼Ÿ", "ã€", "ï¼Œ", "ï¼", ".", "ï¼›", ";", "ï¼š", ":"]
        cut_idx = None
        current = ""
        for i, ch in enumerate(text):
            current += ch
            if zenkaku_len(current) > limit:
                break
            if ch in marks:
                cut_idx = i + 1
        if cut_idx is None:
            # ä»•æ–¹ãªãç”Ÿã‚«ãƒƒãƒˆ
            # å…¨è§’æ›ç®—ã§limitã«ç›¸å½“ã™ã‚‹å®Ÿé•·ã‚’æ¦‚ç®—
            acc = 0
            out = ""
            for ch in text:
                w = 2 if unicodedata.east_asian_width(ch) in ("F","W","A") else 1
                if int(round((acc + w)/2.0)) > limit:
                    break
                acc += w
                out += ch
            return out.rstrip("ã€ã€‚,.!ï¼ï¼Ÿã€€ ")
        return text[:cut_idx].rstrip("ã€ã€‚,.!ï¼ï¼Ÿã€€ ")

    if zenkaku_len(s) > max_len:
        s = natural_cut(s, max_len)

    # æœ€çµ‚ï¼šä½™è¨ˆãªç©ºç™½ãƒ»è¨˜å·ã‚’è½ã¨ã™
    return s.strip("ã€€ ").strip()

def remove_forbidden_words(s: str, forbidden: List[str]) -> str:
    if not s:
        return s
    t = s
    for w in forbidden + IMAGE_WORDS:
        if not w:
            continue
        t = t.replace(w, "")
    # ç½®æ›ã§å¤‰ãªç©ºç™½ãŒã§ããŸã‚‰æ•´ãˆã‚‹
    t = re.sub(r"[ ã€€]{2,}", " ", t).strip()
    return t

def is_list_like(x):
    return isinstance(x, (list, tuple))

def jload_safe(path: str) -> Any:
    if not os.path.exists(path):
        return None
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None

def extract_forbidden(normalized_cfg: Any) -> List[str]:
    # normalized ãŒ dict or list ã®ä¸¡å¯¾å¿œ
    if isinstance(normalized_cfg, dict):
        fw = normalized_cfg.get("forbidden_words") or []
        return fw if is_list_like(fw) else []
    if is_list_like(normalized_cfg):
        out = []
        for item in normalized_cfg:
            if isinstance(item, dict) and "forbidden_words" in item:
                ws = item["forbidden_words"]
                if is_list_like(ws):
                    out.extend(ws)
        return list(OrderedDict.fromkeys(out))
    return []

def cap_from_list(xs: Any, key: Optional[str], limit: int) -> List[str]:
    out = []
    if not xs:
        return out
    if isinstance(xs, dict):
        xs = xs.get("items") or xs.get("list") or []
    if not is_list_like(xs):
        return out
    for el in xs[:limit]:
        if isinstance(el, dict):
            if key and key in el and isinstance(el[key], str):
                out.append(el[key])
        elif isinstance(el, str):
            out.append(el)
    # é‡è¤‡æ’é™¤
    seen = OrderedDict()
    for w in out:
        if w and w not in seen:
            seen[w] = True
    return list(seen.keys())

def summarize_knowledge(cfgs: Dict[str, Any]) -> Tuple[str, List[str]]:
    """ãƒ­ãƒ¼ã‚«ãƒ«çŸ¥è¦‹JSONç¾¤ã‚’è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆåŒ–ï¼‹ç¦å‰‡èªæŠ½å‡º"""
    persona = cfgs.get("persona")
    lexical = cfgs.get("lexical")
    market  = cfgs.get("market")
    sem     = cfgs.get("semantic")
    template= cfgs.get("template")
    normalized = cfgs.get("normalized")

    # è»½é‡ã‚µãƒãƒªï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ã§ã¯ãªãâ€œæ§‹æ–‡ãƒ’ãƒ³ãƒˆâ€ã‚’è‡ªç„¶è¨€èªåŒ–ï¼‰
    tones = []
    if isinstance(persona, dict):
        t = persona.get("tone")
        if isinstance(t, dict):
            tones = [f"{k}:{v}" for k,v in t.items() if isinstance(v, str)]
        elif is_list_like(t):
            tones = [str(x) for x in t if isinstance(x, str)]
    persona_hint = "ï¼".join(tones[:5]) if tones else "è½ã¡ç€ã„ãŸè‡ªç„¶æ–‡ãƒ»èª‡å¼µãªã—ãƒ»èª å®Ÿ"

    cluster_terms = cap_from_list(lexical, key="term", limit=12)
    market_terms  = cap_from_list(market,  key="vocabulary", limit=12)
    concepts      = cap_from_list(sem,     key="concept", limit=12)

    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®æœ‰ç”¨ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    tmpl_snips = []
    if isinstance(template, dict):
        for k,v in template.items():
            if isinstance(v, str) and len(v) < 120:
                tmpl_snips.append(v)
            elif is_list_like(v):
                tmpl_snips.extend([x for x in v if isinstance(x, str) and len(x)<120])
    tmpl_hint = "ï½œ".join(tmpl_snips[:6])

    knowledge_text = (
        f"æ–‡ä½“ãƒ’ãƒ³ãƒˆ:{persona_hint}\n"
        f"é »å‡ºèªï¼ˆã‚¯ãƒ©ã‚¹ã‚¿ï¼‰:{'ã€'.join(cluster_terms)}\n"
        f"å¸‚å ´èªå½™:{'ã€'.join(market_terms)}\n"
        f"æ¦‚å¿µ/è¨´æ±‚:{'ã€'.join(concepts)}\n"
        f"è¡¨ç¾ãƒ’ãƒ³ãƒˆ:{tmpl_hint}"
    ).strip()

    forbidden = extract_forbidden(normalized)
    # ç”»åƒæå†™ç¦æ­¢ãƒ¯ãƒ¼ãƒ‰ã‚‚è¿½åŠ ï¼ˆæœ€å¾Œã§ã¾ã¨ã‚é™¤å»ï¼‰
    forbidden = list(OrderedDict.fromkeys(forbidden + IMAGE_WORDS))
    return knowledge_text, forbidden

def load_all_knowledge(paths: Dict[str, str]) -> Dict[str, Any]:
    return {
        "lexical": jload_safe(paths["lexical"]),
        "market": jload_safe(paths["market"]),
        "semantic": jload_safe(paths["semantic"]),
        "persona": jload_safe(paths["persona"]),
        "normalized": jload_safe(paths["normalized"]),
        "template": jload_safe(paths["template"]),
    }

def read_product_names(input_csv: str) -> List[str]:
    # Shift-JISèª­è¾¼ã€ãƒ˜ãƒƒãƒ€ã‚ã‚Šã€ã€Œå•†å“åã€åˆ—ã‚’æ¢ã™
    try_encs = ["cp932", "shift_jis", "utf-8-sig", "utf-8"]
    df = None
    last_err = None
    for enc in try_encs:
        try:
            df = pd.read_csv(input_csv, encoding=enc)
            break
        except Exception as e:
            last_err = e
    if df is None:
        print(f"âŒ CSVèª­è¾¼å¤±æ•—: {input_csv} / {last_err}")
        return []
    if PRODUCT_NAME_COL not in df.columns:
        print(f"âŒ ãƒ˜ãƒƒãƒ€ã«ã€{PRODUCT_NAME_COL}ã€ãŒã‚ã‚Šã¾ã›ã‚“ã€‚åˆ—ä¸€è¦§: {list(df.columns)[:10]} ...")
        return []

    names = [str(x).strip() for x in df[PRODUCT_NAME_COL].tolist() if isinstance(x, str) and x.strip()]
    # é‡è¤‡é™¤å»ï¼ˆé †åºä¿æŒï¼‰
    seen = OrderedDict()
    for n in names:
        if n not in seen:
            seen[n] = True
    return list(seen.keys())

# -------------------------------
# OpenAI ã‚³ãƒ¼ãƒ«
# -------------------------------

def call_openai_json(client: OpenAI, model: str, messages: List[Dict[str, str]], max_completion_tokens: int = 800) -> Optional[Dict[str, Any]]:
    """
    Chat Completions(JSON). æ¸©åº¦ã¯æœªæŒ‡å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ=1ï¼‰ã€‚max_completion_tokens ã‚’ä½¿ç”¨ã€‚
    """
    for _ in range(RETRY):
        try:
            res = client.chat.completions.create(
                model=model,
                messages=messages,
                response_format={"type": "json_object"},
                max_completion_tokens=max_completion_tokens,
            )
            content = (res.choices[0].message.content or "").strip()
            if not content:
                raise ValueError("Empty content")
            return json.loads(content)
        except Exception as e:
            print(f"âš ï¸ OpenAIã‚¨ãƒ©ãƒ¼: {e}")
            time.sleep(RETRY_WAIT)
    return None

# -------------------------------
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
# -------------------------------

def build_system_prompt(knowledge_text: str, forbidden: List[str]) -> str:
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼å“²å­¦ã«æ²¿ã£ã¦â€œãƒ†ãƒ³ãƒ—ãƒ¬ç¦æ­¢â€ã§ã¯ãªãâ€œæ§‹æˆãƒ’ãƒ³ãƒˆã‚’è‡ªç„¶ã«å«ã‚ã‚‹â€
    return (
        "ã‚ãªãŸã¯æ—¥æœ¬èªã®ECå•†å“ã‚³ãƒ”ãƒ¼ã®å°‚é–€ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚ä»¥ä¸‹ã‚’å³å®ˆã—ã¦è‡ªç„¶ãªæ–‡ã‚’ä½œæˆã—ã¾ã™ã€‚\n"
        "ãƒ»å„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯1ã€œ2æ–‡ã§è‡ªç„¶ãªæ—¥æœ¬èªã€‚\n"
        "ãƒ»æ§‹æˆãƒ’ãƒ³ãƒˆï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ã§ã¯ãªã„ï¼‰:ã€å•†å“ã‚¹ãƒšãƒƒã‚¯â†’ã‚³ã‚¢ã‚³ãƒ³ãƒ”ã‚¿ãƒ³ã‚¹â†’ã©ã‚“ãªäººâ†’ã‚·ãƒ¼ãƒ³â†’ãƒ™ãƒãƒ•ã‚£ãƒƒãƒˆã€ã‚’ç„¡ç†ãªãå«ã‚ã‚‹ã€‚\n"
        "ãƒ»çµµæ–‡å­—ãƒ»ç‰¹æ®Šè¨˜å·ãƒ»HTMLã‚¿ã‚°ã¯ç¦æ­¢ã€‚æ”¹è¡Œç¦æ­¢ã®æ¬„ã§ã¯æ”¹è¡Œã‚’å…¥ã‚Œãªã„ã€‚\n"
        "ãƒ»ç¦æ­¢èªã¯ä½¿ç”¨ã—ãªã„ã€‚ç«¶åˆæ¯”è¼ƒã‚„â€œç«¶åˆå„ªä½æ€§â€ã®ã‚ˆã†ãªãƒ¡ã‚¿è¡¨ç¾ã¯é¿ã‘ã‚‹ã€‚\n"
        "ãƒ»æ–‡å­—æ•°ã¯â€œå…¨è§’æ›ç®—â€ã§å„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®minã€œmaxã«å…¥ã‚‹ç¨‹åº¦ã«æ„è­˜ã€‚\n"
        "ãƒ»ALTã¯ç”»åƒã®æå†™ã‚’ã—ãªã„ï¼ˆè¢«å†™ä½“ã‚„æ§‹å›³ã®èª¬æ˜ã¯ã—ãªã„ï¼‰ã€‚\n"
        "ãƒ»è¿”ç­”ã¯ JSON ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼ˆå„ã‚­ãƒ¼ï¼šrakuten_copy, rakuten_sp, yahoo_headline, yahoo_explanation, "
        "yahoo_meta, alt_listï¼‰ã§è¿”ã™ã€‚\n"
        "ãƒ»alt_list ã¯ 20æœ¬ã®æ–‡ï¼ˆ120ã€œ150å­—ã‚’ç›®å®‰ï¼‰ã‚’ç”Ÿæˆã€‚\n"
        "â€•â€•â€•â€•â€•â€•â€•â€•\n"
        f"ã€çŸ¥è¦‹è¦ç´„ã€‘\n{knowledge_text}\n"
        "â€•â€•â€•â€•â€•â€•â€•â€•\n"
        f"ã€ç¦æ­¢èªï¼ˆå«ã‚€ç”»åƒæå†™NGèªï¼‰ã€‘\n{', '.join(forbidden)}\n"
    )

def build_user_prompt(product_name: str) -> str:
    return (
        f"å•†å“å: {product_name}\n"
        "å‡ºåŠ›è¦ä»¶ï¼ˆå…¨è§’æ›ç®—ï¼‰:\n"
        f"- æ¥½å¤©ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼: {RULES['rakuten_copy_min']}ã€œ{RULES['rakuten_copy_max']}æ–‡å­—\n"
        f"- æ¥½å¤©ã‚¹ãƒãƒ›èª¬æ˜: {RULES['rakuten_sp_min']}ã€œ{RULES['rakuten_sp_max']}æ–‡å­—\n"
        f"- Yahoo headline: {RULES['yahoo_headline_min']}ã€œ{RULES['yahoo_headline_max']}æ–‡å­—\n"
        f"- Yahoo explanation: {RULES['yahoo_exp_min']}ã€œ{RULES['yahoo_exp_max']}æ–‡å­—\n"
        f"- Yahoo meta-desc: {RULES['yahoo_meta_min']}ã€œ{RULES['yahoo_meta_max']}æ–‡å­—\n"
        f"- ALTÃ—20: {RULES['alt_gen_min']}ã€œ{RULES['alt_gen_max']}æ–‡å­—ã§ç”Ÿæˆï¼ˆå¾Œã§80ã€œ110ã«èª¿æ•´ï¼‰\n"
        "è¿”ç­”ã¯JSONï¼ˆä¾‹ï¼‰ï¼š\n"
        "{\n"
        '  "rakuten_copy": "...",\n'
        '  "rakuten_sp": "...",\n'
        '  "yahoo_headline": "...",\n'
        '  "yahoo_explanation": "...",\n'
        '  "yahoo_meta": "...",\n'
        '  "alt_list": ["...", "...", "...ï¼ˆ20æœ¬ï¼‰"]\n'
        "}\n"
    )

# -------------------------------
# æ•´å½¢ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ï¼‰
# -------------------------------

def refine_field(s: str, min_len: int, max_len: int, forbidden: List[str]) -> str:
    s = (s or "").replace("\n", "").strip()
    s = remove_forbidden_words(s, forbidden)
    s = clamp_by_len(s, min_len, max_len)
    return s

def refine_alt_list(alts: List[str], forbidden: List[str]) -> List[str]:
    # 120ã€œ150ã§ç”Ÿã¾ã‚ŒãŸALTã‚’ 80ã€œ110 ã«è‡ªç„¶ãƒˆãƒªãƒ 
    out = []
    for a in alts[:20]:
        t = remove_forbidden_words((a or "").replace("\n"," ").strip(), forbidden)
        t = clamp_by_len(t, RULES["alt_min"], RULES["alt_max"])
        if t:
            out.append(t)
    # 20æœ¬ã«æº€ãŸãªã„å ´åˆã¯ç©ºã‚’åŸ‹ã‚ãšã€çŸ­æ¬ è½ã®ã¾ã¾ï¼ˆæ¬ è½ç‡ã®å¯è¦–åŒ–ã®ãŸã‚ï¼‰
    return out

# -------------------------------
# ãƒ¡ã‚¤ãƒ³
# -------------------------------

def main():
    print("ğŸŒ¸ writer_splitter_perfect_v3_5_unified å®Ÿè¡Œé–‹å§‹ï¼ˆå…¨ä»¶AIï¼‹çŸ¥è¦‹è¦ç´„ï¼‹ç¦å‰‡/é•·ã•æ•´å½¢ï¼‰")

    # ç’°å¢ƒ
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âŒ OPENAI_API_KEY ãŒæœªè¨­å®šã§ã™ã€‚.envã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)
    model = os.getenv("OPENAI_API_MODEL", "gpt-4o-mini")

    ensure_output_dirs()
    tag = now_tag()

    # çŸ¥è¦‹ãƒ­ãƒ¼ãƒ‰ï¼‹è¦ç´„
    cfg = load_all_knowledge(DEFAULT_PATHS)
    knowledge_text, forbidden = summarize_knowledge(cfg)

    # å…¥åŠ›å•†å“å
    names = read_product_names(INPUT_CSV)
    print(f"âœ… å•†å“åæŠ½å‡º: {len(names)}ä»¶ï¼ˆé‡è¤‡é™¤å»æ¸ˆï¼‰")
    if not names:
        print("âŒ å•†å“åãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        sys.exit(1)

    client = OpenAI()

    # å‡ºåŠ›æº–å‚™
    path_rak = os.path.join(OUTPUT_DIR, f"rakuten_copy_{tag}.csv")
    path_yah = os.path.join(OUTPUT_DIR, f"yahoo_copy_{tag}.csv")
    path_alt = os.path.join(OUTPUT_DIR, f"alt_text_{tag}.csv")
    path_jsonl = os.path.join(OUTPUT_DIR, f"split_full_{tag}.jsonl")

    # CSV ãƒ˜ãƒƒãƒ€
    rak_cols = ["å•†å“å", "æ¥½å¤©_ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼", "æ¥½å¤©_ã‚¹ãƒãƒ›èª¬æ˜"]
    yah_cols = ["å•†å“å", "Yahoo_headline", "Yahoo_explanation", "Yahoo_meta_desc"]
    alt_cols = ["å•†å“å"] + [f"ALT{i}" for i in range(1, 21)]

    # åˆæœŸåŒ–
    with open(path_rak, "w", newline="", encoding="utf-8") as fr, \
         open(path_yah, "w", newline="", encoding="utf-8") as fy, \
         open(path_alt, "w", newline="", encoding="utf-8") as fa, \
         open(path_jsonl, "w", encoding="utf-8") as fj:

        wr_rak = csv.writer(fr)
        wr_yah = csv.writer(fy)
        wr_alt = csv.writer(fa)
        wr_rak.writerow(rak_cols)
        wr_yah.writerow(yah_cols)
        wr_alt.writerow(alt_cols)

        for nm in tqdm(names, desc="ğŸ§  å•†å“åˆ¥AIç”Ÿæˆä¸­", ncols=100):
            sys_msg = {"role": "system", "content": build_system_prompt(knowledge_text, forbidden)}
            user_msg = {"role": "user", "content": build_user_prompt(nm)}

            raw = call_openai_json(client, model, [sys_msg, user_msg],
                                   max_completion_tokens=900)  # ã‚„ã‚„ä½™è£•

            # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•
            rak_copy = rak_sp = yah_h = yah_e = yah_m = ""
            alts = []

            if isinstance(raw, dict):
                rak_copy = raw.get("rakuten_copy", "") or ""
                rak_sp   = raw.get("rakuten_sp", "") or ""
                yah_h    = raw.get("yahoo_headline", "") or ""
                yah_e    = raw.get("yahoo_explanation", "") or ""
                yah_m    = raw.get("yahoo_meta", "") or ""
                tmp_alts = raw.get("alt_list", []) or []
                if is_list_like(tmp_alts):
                    alts = [str(x) for x in tmp_alts if isinstance(x, str)]

            # ãƒ­ãƒ¼ã‚«ãƒ«æ•´å½¢ï¼ˆç¦å‰‡ãƒ»é•·ã•ï¼‰
            rak_copy = refine_field(rak_copy, RULES["rakuten_copy_min"], RULES["rakuten_copy_max"], forbidden)
            rak_sp   = refine_field(rak_sp,   RULES["rakuten_sp_min"],   RULES["rakuten_sp_max"],   forbidden)
            yah_h    = refine_field(yah_h,    RULES["yahoo_headline_min"], RULES["yahoo_headline_max"], forbidden)
            yah_e    = refine_field(yah_e,    RULES["yahoo_exp_min"],      RULES["yahoo_exp_max"],   forbidden)
            yah_m    = refine_field(yah_m,    RULES["yahoo_meta_min"],     RULES["yahoo_meta_max"],  forbidden)
            alts_ref = refine_alt_list(alts, forbidden)

            # CSVæ›¸ãå‡ºã—
            wr_rak.writerow([nm, rak_copy, rak_sp])
            wr_yah.writerow([nm, yah_h, yah_e, yah_m])

            row_alt = [nm] + alts_ref + [""] * (20 - len(alts_ref))
            wr_alt.writerow(row_alt)

            # JSONL ãƒ­ã‚°ï¼ˆç”Ÿ/æ•´å½¢å¾Œã®ä¸¡æ–¹ï¼‰
            log_item = {
                "product_name": nm,
                "raw": raw,
                "refined": {
                    "rakuten_copy": rak_copy,
                    "rakuten_sp": rak_sp,
                    "yahoo_headline": yah_h,
                    "yahoo_explanation": yah_e,
                    "yahoo_meta": yah_m,
                    "alt_list": alts_ref
                }
            }
            fj.write(json.dumps(log_item, ensure_ascii=False) + "\n")

    print("âœ… å‡ºåŠ›å®Œäº†:")
    print(f"   - æ¥½å¤©: {path_rak}")
    print(f"   - Yahoo: {path_yah}")
    print(f"   - ALT20: {path_alt}")
    print(f"   - JSONL: {path_jsonl}")
    print("âœ… å…±é€šALT20ã¯ã€alt_text_*.csvã€ã«ALT1ã€œALT20ã¨ã—ã¦æ¨ªæŒã¡ã—ã¾ã™ã€‚")

if __name__ == "__main__":
    main()
import atlas_autosave_core
